[["index.html", "Introduction to Statistical Learning Using R Book Club Welcome", " Introduction to Statistical Learning Using R Book Club The R4DS Online Learning Community 2022-06-06 Welcome This is a companion for the book Introduction to Statistical Learning Using R by Gareth James, Daniela Witten, Trevor Hastie, and Rob Tibshirani (Springer Science+Business Media, LLC, part of Springer Nature, copyright 2021, 978-1-0716-1418-1_1). This companion is available at r4ds.io/islr. This website is being developed by the R4DS Online Learning Community. Follow along, and join the community to participate. This companion follows the R4DS Online Learning Community Code of Conduct. "],["book-club-meetings.html", "Book club meetings", " Book club meetings Each week, a volunteer will present a chapter from the book (or part of a chapter). This is the best way to learn the material. Presentations will usually consist of a review of the material, a discussion, and/or a demonstration of the principles presented in that chapter. More information about how to present is available in the github repo. Presentations will be recorded, and will be available on the R4DS Online Learning Community YouTube Channel. "],["st-edition-vs-2nd-edition.html", "1st edition vs 2nd edition", " 1st edition vs 2nd edition This club is reading the digital version of the second edition of this book (2e). "],["pace.html", "Pace", " Pace This book is often used for two-semester-long courses. We’ll try to cover 1 chapter/week, but… …It’s ok to split chapters when they feel like too much. We will try to meet every week, but will likely take some breaks for holidays, etc. "],["introduction.html", "Chapter 1 Introduction", " Chapter 1 Introduction Learning objectives: Recognize various types of statistical learning. Understand why this book is useful for you. Be able to read mathematical notation used throughout this book. Describe the overall layout of this book. Be able to find data used in examples throughout the book. "],["what-is-statistical-learning.html", "1.1 What is statistical learning?", " 1.1 What is statistical learning? Statistical learning is the theoretical foundation for machine learning framework. It makes connections between the fields of statistics and functional analysis. In particular, the Statistical learning theory deals with the problem of finding a predictive function based on data and this is what is best known as supervised learning, in this book we will see more than just theory, as we will deal with unsupervised learning as well as making practical applications. Supervised: “Building a model to predict an output from inputs.” Predict wage from age, education, and year. Predict market direction from previous days' performance. Unsupervised: Inputs but no specific outputs, find relationships and structure. Identify clusters within cancer cell lines. "],["why-islr.html", "1.2 Why ISLR?", " 1.2 Why ISLR? “Facilitate the transition of statistical learning from an academic to a mainstream field.” Machine learning* is useful to everyone, let’s all learn enough to use it responsibly. R “labs” make this make sense for this community! "],["premises-of-islr.html", "1.3 Premises of ISLR", " 1.3 Premises of ISLR From Page 9 of the Introduction: “Many statistical learning methods are relevant and useful in a wide range of academic and non-academic disciplines, beyond just the statistical sciences.” “Statistical learning should not be viewed as a series of black boxes.” “While it is important to know what job is performed by each cog, it is not necessary to have the skills to construct the machine inside the box!” “We presume that the reader is interested in applying statistical learning methods to real-world problems.” "],["notation.html", "1.4 Notation", " 1.4 Notation n = number of observations (rows) p = number of features/variables (columns) We’ll come back here if we need to as we go! Some symbols they assume we know: \\(\\in\\) = “is an element of”, “in” \\({\\rm I\\!R}\\) = “real numbers” "],["what-have-we-gotten-ourselves-into.html", "1.5 What have we gotten ourselves into?", " 1.5 What have we gotten ourselves into? An Introduction to Statistical Learning (ISL by James, Witten, Hastie and Tibshiraniis), is a collection of modern statistical methods for modeling and making predictions from real-world data. It is a middle way between theoretical statistics and the practice of applying statistics to real-world problems. It can be considered as a user manual, with self-contained R labs, which lead you through the use of different methods for applying statistical analysis to different kinds of data. 2: Terminology &amp; main concepts 3-4: Classic linear methods 5: Resampling (so we can choose the best method) 6: Modern updates to linear methods 7+: Beyond Linearity (we can worry about details as we get there) "],["wheres-the-data.html", "1.6 Where’s the data?", " 1.6 Where’s the data? install.packages(&quot;ISLR2&quot;) Or “install” this book. install.packages(&quot;remotes&quot;) remotes::install_github(&quot;r4ds/bookclub-islr&quot;) remove.packages(&quot;bookclubislr&quot;) # This isn&#39;t really a package. We’ll look at this data in more detail below. "],["some-useful-resources.html", "1.7 Some useful resources:", " 1.7 Some useful resources: the book page: statlearning.com pdf of the book: ISLRv2_website course on edX: statistical-learning youtube channel: playlists exercise solutions: applied solutions book package ISLR2 Some more theoretical resources: The Elements of Statistical Learning (ESL, by Hastie, Tibshirani, and Friedman) ESLII "],["what-is-covered-in-the-book.html", "1.8 What is covered in the book?", " 1.8 What is covered in the book? The book provides a series of toolkits classified as supervised or unsupervised techniques for understanding data. The second edition of the book (2021) contains additions within the most updated statistical analysis. Figure 1.1: Editions "],["how-is-the-book-divided.html", "1.9 How is the book divided?", " 1.9 How is the book divided? The book is divided into 13 chapters covering: Introduction and Statistical Learning: Supervised Versus Unsupervised Learning Regression Versus Classification Problems Linear statistical learning Linear Regression: basic concepts introduction of K-nearest neighbor classifier Classification: logistic regression linear discriminant analysis Resampling Methods: cross-validation the bootstrap Linear Model Selection and Regularization: potential improvements over standard linear regression stepwise selection ridge regression principal components regression the lasso Non-linear statistical learning Moving Beyond Linearity: Polynomial Regression Regression Spline Smoothing Splines Local Regression Generalized Additive Models Tree-Based Methods: Decision Trees Bagging, Random Forests, Boosting, and Bayesian Additive Regression Trees Support Vector Machines (linear and non-linear classification) Deep Learning (non-linear regression and classification) Survival Analysis and Censored Data Unsupervised Learning: Principal components analysis K-means clustering Hierarchical clustering Multiple Testing Each chapter includes 1 self-contained R lab on the topic "],["some-examples-of-the-problems-addressed-with-statistical-analysis.html", "1.10 Some examples of the problems addressed with statistical analysis", " 1.10 Some examples of the problems addressed with statistical analysis Identify the risk factors for some type of cancers Predict whether someone will have a hearth attack on the basis of demographic, diet, and clinical measurements Email spam detection Classify a tissue sample into one of several cancer classes, based on a gene expression profile Establish the relationship between salary and demographic variables in population survey data (source) "],["datasets-provided-in-the-islr2-package.html", "1.11 Datasets provided in the ISLR2 package", " 1.11 Datasets provided in the ISLR2 package The book provides the {ISLR2} R package with all the datasets needed the analysis. # install.packages(&quot;ISLR2&quot;) # install.packages(&quot;remotes&quot;) # remotes::install_github(&quot;r4ds/bookclub-islr&quot;) # remove.packages(&quot;bookclubislr&quot;) library(ISLR2) Figure 1.2: Datasets in ISLR2 package 1.11.1 Example datasets As an example some of the data sets used are: Wage Data: predicting a continuous or quantitative output value (a regression problem) - Chapter3. ISLR2::Wage %&gt;% head() ## year age maritl race education region ## 231655 2006 18 1. Never Married 1. White 1. &lt; HS Grad 2. Middle Atlantic ## 86582 2004 24 1. Never Married 1. White 4. College Grad 2. Middle Atlantic ## 161300 2003 45 2. Married 1. White 3. Some College 2. Middle Atlantic ## 155159 2003 43 2. Married 3. Asian 4. College Grad 2. Middle Atlantic ## 11443 2005 50 4. Divorced 1. White 2. HS Grad 2. Middle Atlantic ## 376662 2008 54 2. Married 1. White 4. College Grad 2. Middle Atlantic ## jobclass health health_ins logwage wage ## 231655 1. Industrial 1. &lt;=Good 2. No 4.318063 75.04315 ## 86582 2. Information 2. &gt;=Very Good 2. No 4.255273 70.47602 ## 161300 1. Industrial 1. &lt;=Good 1. Yes 4.875061 130.98218 ## 155159 2. Information 2. &gt;=Very Good 1. Yes 5.041393 154.68529 ## 11443 2. Information 1. &lt;=Good 1. Yes 4.318063 75.04315 ## 376662 2. Information 2. &gt;=Very Good 1. Yes 4.845098 127.11574 p1 &lt;- Wage %&gt;% ggplot(aes(x = age, y = wage)) + geom_point(color = &quot;grey55&quot;) + geom_smooth() + theme_bw() p2&lt;-Wage %&gt;% ggplot(aes(x = year, y = wage)) + geom_point(color = &quot;grey55&quot;) + geom_smooth(method = &quot;lm&quot;) + theme_bw() p3&lt;-Wage %&gt;% ggplot(aes(x = education, y = wage)) + geom_boxplot(aes(fill = education), show.legend = FALSE) + theme_bw() + theme(axis.text.x = element_text(size = 5)) library(patchwork) p1|p2|p3 Stock Market Data: predicting a categorical or qualitative output (classification problem). Predict whether the index will increase or decrease on a given day, using the past 5 days’ percentage changes in the index - Chapter 4. ISLR2::Smarket %&gt;% head() ## Year Lag1 Lag2 Lag3 Lag4 Lag5 Volume Today Direction ## 1 2001 0.381 -0.192 -2.624 -1.055 5.010 1.1913 0.959 Up ## 2 2001 0.959 0.381 -0.192 -2.624 -1.055 1.2965 1.032 Up ## 3 2001 1.032 0.959 0.381 -0.192 -2.624 1.4112 -0.623 Down ## 4 2001 -0.623 1.032 0.959 0.381 -0.192 1.2760 0.614 Up ## 5 2001 0.614 -0.623 1.032 0.959 0.381 1.2057 0.213 Up ## 6 2001 0.213 0.614 -0.623 1.032 0.959 1.3491 1.392 Up Smarket %&gt;% pivot_longer( cols=c(&quot;Lag1&quot;,&quot;Lag2&quot;,&quot;Lag3&quot;), names_to=&quot;lags13&quot;, values_to=&quot;lags13_val&quot; ) %&gt;% ggplot(aes(x=Direction,y=lags13_val)) + geom_boxplot(aes(fill=Direction),show.legend = F) + facet_wrap(~lags13) + labs(x=&quot;Today&#39;s Direction&quot;,y=&quot;Percentage change in S&amp;P&quot;) + theme_bw() + theme(strip.background = element_blank()) Figure 1.3: fit a quadratic discriminant analysis model Gene Expression Data class(NCI60) ## [1] &quot;list&quot; ISLR2::NCI60 %&gt;% names() ## [1] &quot;data&quot; &quot;labs&quot; NCI60$labs ## [1] &quot;CNS&quot; &quot;CNS&quot; &quot;CNS&quot; &quot;RENAL&quot; &quot;BREAST&quot; ## [6] &quot;CNS&quot; &quot;CNS&quot; &quot;BREAST&quot; &quot;NSCLC&quot; &quot;NSCLC&quot; ## [11] &quot;RENAL&quot; &quot;RENAL&quot; &quot;RENAL&quot; &quot;RENAL&quot; &quot;RENAL&quot; ## [16] &quot;RENAL&quot; &quot;RENAL&quot; &quot;BREAST&quot; &quot;NSCLC&quot; &quot;RENAL&quot; ## [21] &quot;UNKNOWN&quot; &quot;OVARIAN&quot; &quot;MELANOMA&quot; &quot;PROSTATE&quot; &quot;OVARIAN&quot; ## [26] &quot;OVARIAN&quot; &quot;OVARIAN&quot; &quot;OVARIAN&quot; &quot;OVARIAN&quot; &quot;PROSTATE&quot; ## [31] &quot;NSCLC&quot; &quot;NSCLC&quot; &quot;NSCLC&quot; &quot;LEUKEMIA&quot; &quot;K562B-repro&quot; ## [36] &quot;K562A-repro&quot; &quot;LEUKEMIA&quot; &quot;LEUKEMIA&quot; &quot;LEUKEMIA&quot; &quot;LEUKEMIA&quot; ## [41] &quot;LEUKEMIA&quot; &quot;COLON&quot; &quot;COLON&quot; &quot;COLON&quot; &quot;COLON&quot; ## [46] &quot;COLON&quot; &quot;COLON&quot; &quot;COLON&quot; &quot;MCF7A-repro&quot; &quot;BREAST&quot; ## [51] &quot;MCF7D-repro&quot; &quot;BREAST&quot; &quot;NSCLC&quot; &quot;NSCLC&quot; &quot;NSCLC&quot; ## [56] &quot;MELANOMA&quot; &quot;BREAST&quot; &quot;BREAST&quot; &quot;MELANOMA&quot; &quot;MELANOMA&quot; ## [61] &quot;MELANOMA&quot; &quot;MELANOMA&quot; &quot;MELANOMA&quot; &quot;MELANOMA&quot; # View(NCI60) Figure 1.4: the first two principal components of the data "],["meeting-videos.html", "1.12 Meeting Videos", " 1.12 Meeting Videos 1.12.1 Cohort 1 Meeting chat log 00:07:44 Fran Barton: Hi!!! 00:08:00 Mei Ling Soh: Hello! 00:08:17 BEN: Hello! 00:08:53 Kim Martin: Hello :) 00:09:15 Stijn Van de Vondel: Hiya! 00:09:31 Ryan S: 18 participants before kick off time! Jon, hope you&#39;re expecting a large crowd. :-) 00:11:18 Fran Barton: Gesundheit 00:11:49 Jyoti Bhogal: That&#39;s Starry Nights in the background @Jon :) 00:11:59 Kim Martin: I&#39;m going to turn off my camera and mic for a little bit so no one is subjected to seeing me eat peanut butter from the &#39;emergency snack jar&#39; 😵 00:12:08 Fran Barton: :-D 00:12:31 Fran Barton: I&#39;m in a shared office with background noise so will stay on mute most of the time 00:12:43 Stijn Van de Vondel: Gotta say, that&#39;s one hell of a silent vacuum 00:13:05 Stijn Van de Vondel: If it&#39;s not the mic, you may be sitting on a vacuum goldmine 00:13:21 Kim Martin: 😂 00:16:42 Kim Martin: Shall we do tag-team presentations to spread the responsibility/opportunity around? 00:16:51 Kim Martin: (swopping presenter mid-session/chapter) 00:17:18 Raymond Balise: chapter 2 is a lot 00:20:02 Ryan S: love that idea -- discussion one week, exercises second week 00:20:15 Mei Ling Soh: I second that 00:20:27 Kim Martin: +1 00:20:40 Raymond Balise: thoughts on Tidymodels vs Base? 00:22:25 Kim Martin: Nothing against Tidyverse (recent convert), but it makes sense to me to start with base 00:22:36 Jyoti Bhogal: The link for today&#39;s meeting, I got through a friend. What do I need to do to subscribe for receiving the links for meetings? 00:22:41 Kim Martin: (the argument is overblown, imo) 00:23:16 Jyoti Bhogal: thank you 00:23:17 Raymond Balise: 👍🏼 00:23:17 Kim Martin: https://github.com/r4ds/bookclub-islr 00:23:23 David Severski: To be clear, we’re talking tidymodels vs base, not tidyverse v base. 00:23:31 Jon Harmon (jonthegeek): r4ds.io/islr 00:28:03 Fran Barton (he/him): do we know why they are called &quot;supervised&quot; and &quot;unsupervised&quot;? interesting terminology and I think it would help me to understad better if I could grasp what they have to do with whether you&#39;re looking for outputs or not 00:28:35 August: supervised has a target/output to be predicted. 00:29:02 Fran Barton (he/him): &quot;letting the machine go play&quot; is great - that helps thx 00:29:15 Kim Martin: All models that involve &#39;training&#39; are supervised, I&#39;m guessing? 00:30:24 Kim Martin: If the process of the unsupervised model &#39;deciding&#39; on how to group things into clusters is computationally expensive, then you&#39;d train it first... 00:30:52 Kim Martin: … then after it has &#39;decided&#39;, it&#39;ll be quick in the future to slot new observations into those clusters 00:30:58 Kim Martin: (guessing) 00:31:44 BEN: tidymodels nomenclature is always exciting :3 00:32:05 Arnab Dey (he/him): epsilon? 00:32:12 Kim Martin: &#39;Exciting&#39;? (novice - curious) 00:32:35 BEN: both 00:32:36 Keuntae Kim: that stands for &quot;element of&quot; 00:32:50 SriRam: In set theory, it is called “belongs to” 00:32:51 Arnab Dey (he/him): ah! 00:32:52 Kaustav Sen: belongs to? https://www.mathdoubts.com/belongs-to/#:~:text=In%20the%20set%20theory%2C%20the,element%20belongs%20to%20the%20set.&amp;text=It%20helps%20us%20to%20express,its%20set%20in%20mathematical%20form. 00:32:53 August: Isn&#39;t E also euler? 00:32:57 Keuntae Kim: correct. already in the slide... hahah 00:33:08 Raymond Balise: add the link to that book later plz 00:33:14 Fran Barton (he/him): wikipedia: &quot;The lunate epsilon, ϵ, is not to be confused with the set membership symbol ∈&quot; 00:33:33 Kim Martin: Math notation as bird spotting hobby alternative... 00:33:42 August: sorry you&#39;re right euler is more like e 00:34:05 Ryan Metcalf: Latex: \\in 00:34:18 Jon Harmon (jonthegeek): r4ds.io/islr 00:34:21 Jon Harmon (jonthegeek): statlearning.com 00:34:23 Raymond Balise: the notation book reference plz 00:34:29 Kim Martin: https://www.amazon.com/Mathematical-Notation-Guide-Engineers-Scientists/dp/1466230525 00:34:37 Raymond Balise: thx kim 00:34:41 Mei Ling Soh: https://www.rapidtables.com/math/symbols/Basic_Math_Symbols.html 00:35:11 Fran Barton (he/him): ^ useful! thx 00:38:08 Fran Barton (he/him): I think `install.github(&quot;ISLR2&quot;)` is maybe missing the repo owner name &quot;CRAN/ISLR2&quot; ?? 00:38:37 Ryan S: let&#39;s say it&#39;s been many years since my last &quot;traditional / undergrad&quot; stats class. Will I survive ISLR or should I brush up a bit? 00:39:06 Fran Barton (he/him): ah of course - let&#39;s fix on github ;-) 00:39:56 SriRam: This is also my stepping stone into this world 00:40:25 Ryan S: good to know. Otherwise, I have the hours of 2:00 AM to 3:00 AM on Saturdays I can use to review Stats 101. 00:40:30 Ryan S: 😆 00:43:16 Ryan S: thx! 00:43:19 Wayne Defreitas: thx 00:43:19 Stijn Van de Vondel: Thanks for hosting, can&#39;t wait for the next parts! 00:43:20 A. S. Ghatpande: thanks 00:43:23 Jiwan Heo: thank you! 1.12.2 Cohort 2 Meeting chat log 00:32:05 Ricardo Serrano: https://github.com/r4ds/bookclub-islr 00:32:07 Michael Haugen: https://github.com/r4ds/bookclub-islr 00:50:59 Federica Gazzelloni: https://emilhvitfeldt.github.io/ISLR-tidymodels-labs/index.html 00:51:11 Federica Gazzelloni: https://www.dataschool.io/15-hours-of-expert-machine-learning-videos/ 1.12.3 Cohort 3 Meeting chat log 00:08:45 Fariborz Soroush: Lighting in my room is terrible. I cut my video :D 00:10:23 Mei Ling Soh: That’s fine! 00:15:24 Fariborz Soroush: Go CHoPpers I guess?:lol: 00:15:36 Rose Hartman (she/her): Woot 🙂 00:19:30 Amber: My wifi is a bit rocky so I don’t want to put on my audio, but hi everyone! I’m Amber, I’m a student at CUNY Grad Center in NY where I’m studying Quantitative Methods in the Social Sciences, and have struggled with r during the entire time of my studies 😂 so I’m really looking forward to this book club to finally get comfortable with r 00:19:59 Rose Hartman (she/her): Hi Amber! 🙂 00:20:36 Rose Franzen (she/her): Hi Amber! 00:20:38 Nilay Yönet: 👋☺️ 00:20:40 Fariborz Soroush: Hi Amber :) 00:21:46 Mounika [Monika]: Hi Amber :) 00:25:15 Mounika [Monika]: Mei, could you please post also the link to the R4DS GitHub resource? 00:25:24 Rose Franzen (she/her): https://r4ds.github.io/bookclub-islr/book-club-meetings.html 00:25:34 Mounika [Monika]: Thank you ! 00:36:28 Rose Hartman (she/her): Did you get the capitals? install.packages(“ISLR2”)? 00:36:31 Rose Hartman (she/her): It worked fine for me 00:36:38 Rahul: I don’t know much R but looking online https://cran.rstudio.com/web/packages/ISLR2/ seems that it Depends: R (≥ 3.5.0) 00:37:38 Nilay Yönet: yes my R version is 00:37:40 Nilay Yönet: R version 4.1.0 (2021-05-18) 00:38:12 Rahul: OK, then that might not be the reason 00:38:43 Nilay Yönet: Idk :/ I will try cloning :) 00:38:45 Rose Hartman (she/her): Nilay, usually when I get that error message, I find it’s because I had a typo in the package name ;) Double check that you typed it in correctly? 00:39:03 Nilay Yönet: oh ok I will check that as well :) 00:49:10 Fariborz Soroush: Suggestions for API? VSCode? Jupyter? I remember R had its own interpreter but I have MacOS not sure if it works there. 00:50:10 Rose Hartman (she/her): I really like RStudio for R: https://www.rstudio.com/products/rstudio/ 00:50:24 Mounika [Monika]: RStudio is the OG for R!! 00:50:38 Fariborz Soroush: Ok, RStudio it is :) :D 00:54:31 Rahul: 13th march 00:54:36 Rose Hartman (she/her): Quick poll: Who is interested in doing tidymodels? I am, but happy to skip it if everyone else would prefer to stay in base R. 00:55:35 Mounika [Monika]: I think the plan of covering base as well as tidyR sounds good.. I am ok sticking to baseR if both is too much 00:57:24 Amber: ^^ same 00:57:44 Rose Franzen (she/her): I am also interested in tidy models 00:57:47 Amber: Bye everyone! 1.12.4 Cohort 4 Meeting chat log 00:20:25 Kevin Kent: https://github.com/r4ds/bookclub-islr 00:34:27 Kevin Kent: https://forms.gle/r4vfsArqSz3cBe9v6 00:36:18 Kevin Kent: https://forms.gle/r4vfsArqSz3cBe9v6 01:02:59 Ronald Legere: https://otexts.com/fpp2/ 01:03:17 Ronald Legere: https://otexts.com/fpp3/ 01:06:47 Lydia Gibson: Spam always says free or has an exclamation point lol "],["learning.html", "Chapter 2 Statistical Learning", " Chapter 2 Statistical Learning Learning objectives: Understand Vocabulary for prediction Understand “Error”/Accuracy Understand Parametric vs Nonparametric Models Describe the trade-off between more accurate models and more interpretable models. Compare and contrast supervised and unsupervised learning. Compare and contrast regression and classification problems. Measure the accuracy/goodness of regression model fits. Measure the accuracy/goodness of classification model fits. Describe how bias and variance contribute to the model error. Understand overfitting. Recognize KNN. Understand the role of tuning in ML models. "],["what-is-statistical-learning-1.html", "2.1 What is Statistical Learning?", " 2.1 What is Statistical Learning? The chapter opens with a discussion of what models are good for, how we use them, and some of the issues that are involved. Can we predict Sales using ad media spending? read_csv( &quot;https://www.statlearning.com/s/Advertising.csv&quot;, col_types = &quot;-dddd&quot;, skip = 1, col_names = c(&quot;row&quot;, &quot;TV&quot;, &quot;radio&quot;, &quot;newspaper&quot;, &quot;sales&quot;) ) %&gt;% pivot_longer(cols = -sales) %&gt;% ggplot(aes(value, sales)) + geom_point(shape = 21, color = &quot;red&quot;) + geom_smooth(method = &quot;lm&quot;, color = &quot;blue&quot;, se = FALSE, formula = &quot;y ~ x&quot;) + facet_wrap(vars(name), scales = &quot;free_x&quot;, strip.position = &quot;bottom&quot;) + theme_bw() + theme(strip.placement = &quot;outside&quot;, panel.grid = element_blank(), strip.background = element_blank()) + labs(x = NULL, caption = &quot;Source: https://www.statlearning.com/s/Advertising.csv | A simple least squares fit shown in blue&quot;) You can make out a trend, at least in radio and TV. We typically want to be able to characterize the Sales potential as a function of all three media inputs. How do they operate together? Notation In this setting, the advertising budgets are input variables while sales is an output variable. The input variables are typically denoted using the symbol \\(X\\), with a subscript to distinguish them. So \\(X_1\\) might be the TV budget, \\(X_2\\) the radio budget, and \\(X_3\\) the newspaper budget. The inputs go by different names, such as predictors, independent variables, features, or sometimes just variables. The output variable in this case, sales is often called the response or dependent variable, and is typically denoted using the symbol \\(Y\\). We assume that there is some relationship between \\(Y\\) and \\(X = (X_1, X_2,\\dots,X_p)\\), which can be written in the very general form \\[\\begin{equation} y = f(X) + \\epsilon \\end{equation}\\] Here \\(f\\) is some fixed but unknown function of \\(X_1, \\dots, X_p\\), and \\(\\epsilon\\) is a random error term, which is independent of \\(X\\) and has a mean of zero. In this formulation, \\(f\\) represents the systematic information that \\(X\\) provides about \\(Y\\). What is the \\(f(X)\\) good for? With a good \\(f\\) we can make predictions of \\(Y\\) at new points. We can understand which components are important in explaining \\(Y\\), and which components are irrelevant. Depending on the complexity of \\(f\\), we may be able to understand how each component of \\(X_j\\) of \\(X\\) affects \\(Y\\). read_csv(&quot;https://www.statlearning.com/s/Income1.csv&quot;, col_types = &quot;-dd&quot;, skip = 1, col_names = c(&quot;row&quot;, &quot;Education&quot;,&quot;Income&quot;)) %&gt;% mutate(res = residuals(loess(Income ~ Education))) %&gt;% ggplot(aes(Education, Income)) + geom_point(shape = 20, size = 4, color = &quot;red&quot;) + geom_segment(aes(xend = Education, yend = Income - res)) + geom_smooth(method = &quot;loess&quot;, se = FALSE, color = &quot;blue&quot;, formula = &quot;y ~ x&quot;) + scale_x_continuous(breaks = seq(10,22,2)) + theme_bw() + theme(panel.grid = element_blank()) + labs(x = &quot;Years of Education&quot;, caption = &quot;Source: https://www.statlearning.com/s/Income1.csv | A loess fit shown in blue&quot;) The vertical lines represent the error terms \\(\\epsilon\\). That is, the difference between the model estimate and the observed value. 2.1.1 Why Estimate \\(f\\)? There are two main reasons that we may wish to estimate \\(f\\): prediction and inference. Prediction In many situations, a set of inputs \\(X\\) are readily available, but the output \\(Y\\) cannot be easily obtained. We can predict \\(Y\\) using \\[\\begin{equation} \\hat{Y} = \\hat{f}(X) \\end{equation}\\] \\(\\hat{f}\\) is often treated as a black box, One is not typically concerned with the exact form of \\(\\hat{f}\\), provided that it yields accurate predictions for Y. The accuracy of \\(\\hat{Y}\\) as a prediction for \\(Y\\) depends on two quantities, called the reducible error and the irreducible error. Reducible Error We can reduce the gap between our estimate and the true function by applying improved methods. Irreducible Error The quantity \\(\\epsilon\\) may contain unmeasured variables that are useful in predicting \\(Y\\) : since we don’t measure them, \\(f\\) cannot use them for its prediction. The quantity \\(\\epsilon\\) will also contain unmeasurable variation. The focus of the book is on techniques for estimating \\(f\\) with the aim of minimizing the reducible error. Inference We are often interested in understanding the association between \\(Y\\) and \\(X = (X_1, X_2,\\dots,X_p)\\). In this situation we wish to estimate \\(f\\), but our goal is not necessarily to make predictions for \\(Y\\). Now \\(f\\) cannot be treated as a black box, because we need to know its exact form. In this setting, one may be interested in answering the following questions: Which predictors are associated with the response? What is the relationship between the response and each predictor? Can the relationship between Y and each predictor be adequately summarized using a linear equation, or is the relationship more complicated? Consider the Advertising data. One may be interested in answering questions such as: – Which media are associated with sales? – Which media generate the biggest boost in sales? or – How large of an increase in sales is associated with a given increase in TV advertising? 2.1.2 How do we estimate \\(f\\)? We want to find a function \\(f\\) such that \\(Y ≈ f(X)\\) for any observation. Broadly speaking, most statistical learning methods for this task can be characterized as either parametric or non-parametric. Parametric Methods Parametric methods involve a two-step model-based approach. First we make an assumption about the functional form of \\(f\\) After a model is selected, we apply a procedure to fit or train the model. The potential disadvantage of a parametric approach is that the model we choose will usually not match the true unknown form of \\(f\\). If the chosen model is too far from the true \\(f\\), then our estimate will be poor. We can try to address this problem by choosing more flexible models that can fit many different possible functional forms for \\(f\\). These more complex models can lead to a phenomenon known as overfitting the data, which essentially means they follow the errors, or noise, too closely. ISLR2::Portfolio %&gt;% ggplot(aes(X, Y)) + geom_point(shape = 21, color = &quot;gray50&quot;) + geom_smooth(method = &quot;lm&quot;, color = &quot;red&quot;, se = FALSE, formula = &quot;y ~ x&quot;) + geom_point(x = 1, y = 0.5, shape = 20, color = &quot;red&quot;, size = 8) + geom_vline(xintercept = 1, color = &quot;red&quot;) + theme_bw() + theme(panel.grid = element_blank()) + labs(caption = &quot;Source: ISLR2::Portfolio | A linear fit shown in red&quot;) In a given dataset, there may be many observed values of \\(Y\\) for each \\(X\\). In this regression example, the expected value of Y given X = 1 is 0.5. The value of 0.5 is an average of sorts of the slice taken at x = 1. Although it is almost never correct, a linear model often serves as a good and interpretable approximation. Non-Parametric Methods Non-parametric methods don’t make explicit assumptions about the functional form of \\(f\\) Instead they seek an estimate of \\(f\\) that gets as close to the data points as possible without being too rough or wiggly. Any parametric approach brings with it the possibility that the functional form used to estimate \\(f\\) on training data is very different from the true \\(f\\), in which case the resulting model will not fit unseen, new data well. Non-parametric approaches do suffer from a major disadvantage: A very large number of observations (far more a than is typically needed for a parametric approach) is required in order to obtain an accurate estimate for \\(f\\). 2.1.3 Prediction Accuracy vs Model Interpretability Why would we ever choose to use a more restrictive method instead of a very flexible approach? If we are mainly interested in inference, then restrictive models are often more interpretable. Flexible approaches can lead to such complicated estimates of \\(f\\) that it is difficult to understand how any individual predictor is associated with the response. 2.1.4 Supervised Versus Unsupervised Learning Supervised problems have labeled data with a response measurement, also called a dependent variable. Unsupervised problems have no labels. The challenge is to derive clusters, or patterns, to better understand what is happening. 2.1.5 Regression Versus Classification Problems Variables can be characterized as either quantitative or qualitative (categorical). Quantitative variables take on numerical values. In contrast, qualitative variables take on values in classes, or categories. "],["assessing-model-accuracy.html", "2.2 Assessing Model Accuracy", " 2.2 Assessing Model Accuracy There is no free lunch in statistics: no one method dominates all others over all possible problems. Selecting the best approach can be challenging in practice. 2.2.1 Measuring Quality of Fit There is no free lunch in statistics: no one method dominates all others over all possible data sets. On a particular data set, one specific method may work best, but some other method may work better on a similar but different data set. MSE We always need some way to measure how well a model’s predictions actually match the observed data. In the regression setting, the most commonly-used measure is the mean squared error (MSE), given by \\[ MSE = \\frac{1}{n}\\sum_{i=1}^n(y_i-\\hat{f}(x_i))^2,\\] The MSE will be small if the predicted responses are very close to the true responses,and will be large if for some of the observations, the predicted and true responses differ substantially. Training vs. Test The MSE in the above equation is computed using the training data that was used to fit the model, and so should more accurately be referred to as the training MSE. In general, we do not really care how well the method works on the training data. We are interested in the accuracy of the predictions that we obtain when we apply our method to previously unseen test data. \\[\\mathrm{Ave}(y_0 - \\hat{f}(x_0))^2 ,\\] We’d like to select the model for which this quantity is as small as possible on unseen, future samples. The degrees of freedom is a quantity that summarizes the flexibility of a curve. The training MSE declines monotonically as flexibility increases. Overfitting As model flexibility increases, training MSE will decrease, but the test MSE may not. When a given method yields a small training MSE but a large test MSE, we are said to be overfitting the data. Overfitting refers specifically to the case in which a less flexible model would have yielded a smaller test MSE. MSE, for a given value, can always be decomposed into the sum of three fundamental quantities: the variance of \\(\\hat{f}(x_0)\\), the squared bias of \\(\\hat{f}(x_0)\\) and the variance of the error terms \\(\\epsilon\\). That is, \\[E\\big(y_0 - \\hat{f}(x_0)\\big)^2 = \\mathrm{Var}\\big(\\hat{f}(x_0)\\big) +[\\mathrm{Bias}\\big(\\hat{f}(x_0)\\big)]^2 + \\mathrm{Var}(\\epsilon)\\] Here the notation \\(E\\big(y_0 - \\hat{f}(x_0)\\big)^2\\) defines the expected test MSE at \\(x_0\\) and refers to the average test MSE that we would obtain if we repeatedly estimated \\(f\\) using a large number of training sets, and tested each at \\(x_0\\). The overall expected test MSE can be computed by averaging \\(E \\big(y_0 - \\hat{f}x(x_0)\\big)^2\\) over all possible values of \\(x_0\\) in the test set FIGURE 2.9. Left: Data simulated from f, shown in black. Three estimates of f are shown: the linear regression line (orange curve), and two smoothing spline fits (blue and green curves). Right: Training MSE (grey curve), test MSE (red curve), and minimum possible test MSE over all methods (dashed line). Squares represent the training and test MSEs for the three fits shown in the left-hand panel. The orange, blue and green squares indicate the MSEs associated with the corresponding curves in the left hand panel. A more restricted and hence smoother curve has fewer degrees of freedom than a wiggly curve—note that in Figure 2.9, linear regression is at the most restrictive end, with two degrees of freedom. The training MSE declines monotonically as flexibility increases. As the flexibility of the statistical learning method increases, we observe a monotone decrease in the training MSE and a U-shape in the test MSE. This is a fundamental property of statistical learning that holds regardless of the particular data set at hand and regardless of the statistical method being used. As model flexibility increases, training MSE will decrease, but the test MSE may not. When a given method yields a small training MSE but a large test MSE, we are said to be overfitting the data. FIGURE 2.10. Details are as in Figure 2.9, using a different true f that is much closer to linear. In this setting, linear regression provides a very good fit to the data. Another example in which the true \\(f\\) is approximately linear. However, because the truth is close to linear, the test MSE only decreases slightly before increasing again, so that the orange least squares fit is substantially better than the highly flexible green curve. Figure 2.11 displays an example in which f is highly non-linear. The training and test MSE curves still exhibit the same general patterns, but now there is a rapid decrease in both curves before the test MSE starts to increase slowly. FIGURE 2.11. Details are as in Figure 2.9, using a different f that is far from linear. In this setting, linear regression provides a very poor fit to the data. We need to select a statistical learning method that simultaneously achieves low variance and low bias. 2.2.2 The Bias-Variance Trade-Off As we use more flexible methods, the variance will increase and the bias will decrease. As we increase the flexibility of a class of methods, the bias tends to initially decrease faster than the variance increases. However, at some point increasing flexibility has little impact on the bias but starts to significantly increase the variance. When this happens the test MSE increases. FIGURE 2.12. Squared bias (blue curve), variance (orange curve), Var(ϵ) (dashed line), and test MSE (red curve) for the three data sets in Figures 2.9–2.11. The vertical dotted line indicates the flexibility level corresponding to the smallest test MSE. In all three cases, the variance increases and the bias decreases as the method’s flexibility increases. The relationship between bias, variance, and test set MSE given is referred to as the bias-variance trade-off. The challenge lies in finding a method for which both the variance and the squared bias are low. This trade-off is one of the most important recurring themes in this book. 2.2.3 The Classification Setting The most common approach for quantifying the accuracy of our estimate \\(\\hat{f}\\) is the training error rate, the proportion of mistakes that are made if we apply our estimate \\(\\hat{f}\\) to the training observations: \\[\\frac{1}{n}\\sum_{i=1}^{n}I(y_i \\ne \\hat{y}_i).\\] The above equation computes the fraction of incorrect classifications. The equation is referred to as the training error rate because it is computed based on the data that was used to train our classifier. Again, we are most interested in the error rates that result from applying our classifier to test observations that were not used in training. Test Error The test error rate associated with a set of test observations of the form \\((x_0, y_0)\\) is given by \\[\\mathrm{Ave}\\big(I(y_i \\ne \\hat{y}_i)\\big).\\] Where \\(\\hat{y}_0\\) is the predicted class label that results from applying the classifier to the test observation with predictor \\(x_0\\). A good classifier is one for which the test error is smallest. The Bayes Classifier Hypothetical – cannot be done in practice The test error rate is minimized, on average, by a very simple classifier that assigns each observation to the most likely class, given its predictor values. In other words, we should simply assign a test observation with predictor vector \\(x_0\\) to the class \\(j\\) for which \\[\\mathrm{Pr}(Y=j|X=x_0).\\] Note that is a conditional probability: it is the probability that \\(Y = j\\), given the observed predictor vector \\(X_0\\). This very simple classifier is called the Bayes classifier. Bayes Classifier Decision Boundary Figure 2.13 provides an example using a simulated data set in a two dimensional space consisting of predictors X1 and X2. The orange and blue circles correspond to training observations that belong to two different classes. For each value of X1 and X2, there is a different probability of the response being orange or blue. FIGURE 2.13. A simulated data set consisting of 100 observations in each of two groups, indicated in blue and in orange. The purple dashed line represents the Bayes decision boundary. The orange background grid indicates the region in which a test observation will be assigned to the orange class, and the blue background grid indicates the region in which a test observation will be assigned to the blue class. The purple dashed line represents the points where the probability is exactly 50%. This is called the Bayes decision boundary. An observation that falls on the orange side of the boundary will be assigned to the orange class, and similarly an observation on the blue side of the boundary will be assigned to the blue class. The Bayes classifier produces the lowest possible test error rate, called the Bayes error rate. The overall Bayes error rate is given by \\[1- E\\big(\\mathop{\\mathrm{max}}_{j}(Y=j|X)\\big),\\] where the expectation averages the probability over all possible values of X. The Bayes error rate is analogous to the irreducible error, discussed earlier. K-Nearest Neighbors For real data, we do not know the conditional distribution of \\(Y\\) given \\(X\\), and so computing the Bayes classifier is impossible. Many approaches attempt to estimate the conditional distribution of \\(Y\\) given \\(X\\), and then classify a given observation to the class with highest estimated probability. One such method is the K-nearest neighbors (KNN) classifier. Given a positive integer K and a test observation \\(t_{0}\\), the KNN classifier first identifies the K points in the training data that are closest to \\(x_0\\) \\[\\mathrm{Pr}(Y=j|X=x_0) = \\frac{1}{K}\\sum_{i \\in \\mathcal{N}_0} I (y_i = j)\\] Finally, KNN classifies the test observation \\(x_0\\) to the class with the largest probability. Figure 2-143 The KNN approach with \\(K\\) = 3 at all of the possible values for \\(X_1\\) and \\(X_2\\), and have drawn in the corresponding KNN decision boundary. Despite the fact that it is a very simple approach, KNN can produce classifiers that are surprisingly close to the optimal Bayes classifier. KNN with Different K FIGURE 2.15. The black curve indicates the KNN decision boundary on the data from Figure 2.13, using K = 10. The Bayes decision boundary is shown as a purple dashed line. The KNN and Bayes decision boundaries are very similar. FIGURE 2.16. A comparison of the KNN decision boundaries obtained using K = 1 and K = 100 on the data from Figure 2.13. With K = 1, the decision boundary is overly flexible, while with K = 100 it is not sufficiently flexible. The Bayes decision boundary is shown as a purple dashed line. The choice of K has a drastic effect on the KNN classifier obtained. Nearest neighbor methods can be lousy when \\(p\\) is large. Reason: the curse of dimensionality. Nearest neighbors tend to be far away in high dimensions. KNN Tuning FIGURE 2.17. The KNN training error rate (blue, 200 observations) and test error rate (orange, 5,000 observations) on the data from Figure 2.13, as the level of flexibility (assessed using 1/K on the log scale) increases, or equivalently as the number of neighbors K decreases. The black dashed line indicates the Bayes error rate. The jumpiness of the curves is due to the small size of the training data set. As we use more flexible classification methods, the training error rate will decline but the test error rate may not. As \\(1/K\\) increases, the method becomes more flexible. As in the regression setting, the training error rate consistently declines as the flexibility increases. However, the test error exhibits a characteristic U-shape, declining at first (with a minimum at approximately \\(K\\) = 10) before increasing again when the method becomes excessively flexible and overfits. "],["exercises.html", "2.3 Exercises", " 2.3 Exercises Conceptual 1. For each of parts (a) through (d), indicate whether we would generally expect the performance of a flexible statistical learning method to be better or worse than an inflexible method. Justify your answer. (a) The sample size n is extremely large, and the number of predictors p is small. A flexible model can take advantage of the large number of observations to make a detailed model. (b) The number of predictors p is extremely large, and the number of observations n is small. I’d apply Principal Component Analysis to drive p well under n, and then an inflexible, parametric method like Ordinary Least Squares could provide a model with a reasonable test set performance. (c) The relationship between the predictors and response is highly non-linear. A flexible model could better capture the non-linearity. (d) The variance of the error terms, i.e. σ2 = Var(ϵ), is extremely high. An inflexible, parametric method would provide a standardized, average slice across the domain. 2. Explain whether each scenario is a classification or regression problem, and indicate whether we are most interested in inference or prediction. Finally, provide n and p. (a) We collect a set of data on the top 500 firms in the US. For each firm we record profit, number of employees, industry and the CEO salary. We are interested in understanding which factors affect CEO salary. The problem is a regression, and we are interested in inferring the component factors. N is 500, and p is 3. (b) We are considering launching a new product and wish to know whether it will be a success or a failure. We collect data on 20 similar products that were previously launched. For each product we have recorded whether it was a success or failure, price charged for the product, marketing budget, competition price, and ten other variables. The problem is a binary classification prediction. The N is 20, p is 13. (c) We are interested in predicting the % change in the USD/Euro exchange rate in relation to the weekly changes in the world stock markets. Hence we collect weekly data for all of 2012. For each week we record the % change in the USD/Euro, the % change in the US market, the % change in the British market, and the % change in the German market. The % change problem is a regression prediction with a p of 4 and an N of 52. 3. We now revisit the bias-variance decomposition. (a) Provide a sketch of typical (squared) bias, variance, training error, test error, and Bayes (or irreducible) error curves, on a single plot, as we go from less flexible statistical learning methods towards more flexible approaches. The x-axis should represent the amount of flexibility in the method, and the y-axis should represent the values for each curve. There should be five curves. Make sure to label each one. X-axis: increasing flexibility and model complexity Y-axis: increasing Error, Variance, and Bias Blue: Bias Brown: Variance Yellow: Training set MSE Green: Testing set MSE Black: Irreducibile error (Bayes) (b) Explain why each of the five curves has the shape displayed in part (a). Yellow: As models become more flexible and complex, the training error is reduced, but with diminishing benefit. Green: We often observe a U-shaped error in the holdout training error. This is a combination of the Bias error above the least flexible model and the Variance error from over-fitting the most flexible models and the irreducible error component. Blue: The bias error component, as observed in the test result Brown: The variance error component, as observed in the test result Black: The irreducible error component, as observed in the test result 4. You will now think of some real-life applications for statistical learning. (a) Describe three real-life applications in which classification might be useful. Describe the response, as well as the predictors. Is the goal of each application inference or prediction? Explain your answer. Infer the causes of expedited freight for a component part. The independent predictors would include country of origin, shipping method, promised lead time, and schedule changes for a year of deliveries. Predict customer churn for a learning labs subscription education program. The independent predictors would include monthly payment history, course topics, and web site visits. Predict the alphabetic letter of samples of handwriting. The independent predictors would be the pixels of many labeled images. (b) Describe three real-life applications in which regression might be useful. Describe the response, as well as the predictors. Is the goal of each application inference or prediction? Explain your answer. Predict the survival rate of a vehicle component part in use by customers. The response is the proportion of parts still in service, censored. The independent predictors would include engine hours accumulated, region, machine chassis, and type of farm. Infer the characteristics of bottles of wine that influence price. The independent predictors would include the vintage year, the type of grape, the region, the color of the bottle, and the length of the story on the label. Predict the number of sales conversions per visitor for a web portal store front, given the catalog of products available, the layout parameters of the web site, the colors, and dimensions of the shopping experience. (c) Describe three real-life applications in which cluster analysis might be useful. Given demographic data on millions of customers and what they buy, build five persona’s for describing the current customer base to a company investor. Given IoT operating data from agricultural equipment, build three persona’s for characterizing duty cycles of a fleet. Given employee communications transactional data, build three persona’s for better tailoring targeted leadership training. 5. What are the advantages and disadvantages of a very flexible (versus a less flexible) approach for regression or classification? Under what circumstances might a more flexible approach be preferred to a less flexible approach? When might a less flexible approach be preferred? The very flexible GBMs and Neural Nets require more data, and are often both less interpretable and explainable. Banks and highly regulated entities prefer simple linear models and decision trees so they can explain their management policies in simple terms. 6. Describe the differences between a parametric and a non-parametric statistical learning approach. What are the advantages of a parametric approach to regression or classification (as opposed to a nonparametric approach)? What are its disadvantages? In any parametric approach we start with an assumption about the functional form and then work towards fitting the data to the closest version of that functional form. On advantage is in explainability, and another is that less data is required to build a useful model. A disadvantage is that the model might never achieve the lowest error rate. 7. The table below provides a training data set containing six observations, three predictors, and one qualitative response variable. Obs. X1 X2 X3 Y 1 0 3 0 Red 2 2 0 0 Red 3 0 1 3 Red 4 0 1 2 Green 5 −1 0 1 Green 6 1 1 1 Red Suppose we wish to use this data set to make a prediction for Y when X1 = X2 = X3 = 0 using K-nearest neighbors. (a) Compute the Euclidean distance between each observation and the test point, X1 = X2 = X3 = 0. Red: sqrt((0-0)^2 + (0-3)^2 + (0-0)^2 )= sqrt(9) = 3 Red: sqrt((0-2)^2 + (0-0)^2 + (0-0)^2 )= sqrt(4) = 2 Red: sqrt((0-0)^2 + (0-1)^2 + (0-3)^2 )= sqrt(10) = 3.162278 Green: sqrt((0-0)^2 + (0-1)^2 + (0-2)^2 )= sqrt(5)= 2.236068 Green: sqrt((0+1)^2 + (0-0)^2 + (0-1)^2 )= sqrt(2) =1.414214 Red: sqrt((0-1)^2 + (0-1)^2 + (0-1)^2 )= sqrt(3)=1.732051 (b) What is our prediction with K = 1? Why? For a test set X1=X2=X3=0, this is closest to Green which is at a distance sqrt(2). Therefore the prediction is Green. (c) What is our prediction with K = 3? Why? For a test set X1=X2=X3=0, this is closest to Red (Obs 2), Green(Obs 5) and Red(Obs 6). Thus the prediction will be Red. (d) If the Bayes decision boundary in this problem is highly nonlinear, then would we expect the best value for K to be large or small? Why? Small k values yield a model with lots of detailed curves in the boundary, and likely the lowest irreducible error. Applied 8. This exercise relates to the College data set, which can be found in the file College.csv on the book website. (a) Use the read.csv() function to read the data into R. Call the loaded data college. Make sure that you have the directory set to the correct location for the data. (b) Look at the data using the View() function. You should notice that the first column is just the name of each university. We don’t really want R to treat this as data. However, it may be handy to have these names for later. Try the following commands: (c) i. Use the summary() function to produce a numerical summary of the variables in the data set. college &lt;- read_csv(&quot;https://www.statlearning.com/s/College.csv&quot;, show_col_types = FALSE) %&gt;% rename(college = `...1`) ## New names: ## • `` -&gt; `...1` summary(college) ## college Private Apps Accept ## Length:777 Length:777 Min. : 81 Min. : 72 ## Class :character Class :character 1st Qu.: 776 1st Qu.: 604 ## Mode :character Mode :character Median : 1558 Median : 1110 ## Mean : 3002 Mean : 2019 ## 3rd Qu.: 3624 3rd Qu.: 2424 ## Max. :48094 Max. :26330 ## Enroll Top10perc Top25perc F.Undergrad ## Min. : 35 Min. : 1.00 Min. : 9.0 Min. : 139 ## 1st Qu.: 242 1st Qu.:15.00 1st Qu.: 41.0 1st Qu.: 992 ## Median : 434 Median :23.00 Median : 54.0 Median : 1707 ## Mean : 780 Mean :27.56 Mean : 55.8 Mean : 3700 ## 3rd Qu.: 902 3rd Qu.:35.00 3rd Qu.: 69.0 3rd Qu.: 4005 ## Max. :6392 Max. :96.00 Max. :100.0 Max. :31643 ## P.Undergrad Outstate Room.Board Books ## Min. : 1.0 Min. : 2340 Min. :1780 Min. : 96.0 ## 1st Qu.: 95.0 1st Qu.: 7320 1st Qu.:3597 1st Qu.: 470.0 ## Median : 353.0 Median : 9990 Median :4200 Median : 500.0 ## Mean : 855.3 Mean :10441 Mean :4358 Mean : 549.4 ## 3rd Qu.: 967.0 3rd Qu.:12925 3rd Qu.:5050 3rd Qu.: 600.0 ## Max. :21836.0 Max. :21700 Max. :8124 Max. :2340.0 ## Personal PhD Terminal S.F.Ratio ## Min. : 250 Min. : 8.00 Min. : 24.0 Min. : 2.50 ## 1st Qu.: 850 1st Qu.: 62.00 1st Qu.: 71.0 1st Qu.:11.50 ## Median :1200 Median : 75.00 Median : 82.0 Median :13.60 ## Mean :1341 Mean : 72.66 Mean : 79.7 Mean :14.09 ## 3rd Qu.:1700 3rd Qu.: 85.00 3rd Qu.: 92.0 3rd Qu.:16.50 ## Max. :6800 Max. :103.00 Max. :100.0 Max. :39.80 ## perc.alumni Expend Grad.Rate ## Min. : 0.00 Min. : 3186 Min. : 10.00 ## 1st Qu.:13.00 1st Qu.: 6751 1st Qu.: 53.00 ## Median :21.00 Median : 8377 Median : 65.00 ## Mean :22.74 Mean : 9660 Mean : 65.46 ## 3rd Qu.:31.00 3rd Qu.:10830 3rd Qu.: 78.00 ## Max. :64.00 Max. :56233 Max. :118.00 ii. Use the pairs() function to produce a scatterplot matrix of the first ten columns or variables of the data. Recall that you can reference the first ten columns of a matrix A using A[,1:10]. # pairs(college[,3:12]) GGally::ggpairs(college[,2:11], mapping = aes(color = Private), progress = FALSE, lower = list(combo = GGally::wrap(&quot;facethist&quot;, bins = 40))) + theme_bw() + theme(panel.grid = element_blank()) + labs(caption = &quot;Source: ISLR2::College | Ten numeric features&quot;) ## Registered S3 method overwritten by &#39;GGally&#39;: ## method from ## +.gg ggplot2 iii. Use the plot() function to produce side-by-side boxplots of Outstate versus Private. # plot( college$Private, college$Outstate ) college %&gt;% ggplot(aes(Private, Outstate)) + geom_boxplot() + theme_bw() + theme(panel.grid = element_blank()) + labs(caption = &quot;Source: ISLR2::College&quot;) iv. Create a new qualitative variable, called Elite, by binning the Top10perc variable. We are going to divide universities into two groups based on whether or not the proportion of students coming from the top 10 % of their high school classes exceeds 50 %. Use the summary() function to see how many elite universities there are. # Elite &lt;- rep(&quot;No&quot;, nrow(college)) # Elite[college$Top10perc &gt; 50] &lt;- &quot; Yes &quot; # Elite &lt;- as.factor(Elite) # college &lt;- data.frame(college , Elite) college &lt;- college %&gt;% mutate(Elite = as_factor(if_else(Top10perc &gt; 50, &quot;Yes&quot;, &quot;No&quot;))) summary(college$Elite) ## No Yes ## 699 78 Now use the plot() function to produce side-by-side boxplots of Outstate versus Elite. #plot( college$Elite, college$Outstate ) college %&gt;% ggplot(aes(Elite, Outstate)) + geom_boxplot() + theme_bw() + theme(panel.grid = element_blank()) + labs(caption = &quot;Source: ISLR2::College&quot;) v. Use the hist() function to produce some histograms with differing numbers of bins for a few of the quantitative variables. You may find the command par(mfrow = c(2, 2)) useful: it will divide the print window into four regions so that four plots can be made simultaneously. Modifying the arguments to this function will divide the screen in other ways. college %&gt;% ggplot(aes(Enroll, fill = Private)) + geom_histogram(bins = 40) + theme_bw() + theme(panel.grid = element_blank()) + labs(caption = &quot;Source: ISLR2::College&quot;) vi. Continue exploring the data, and provide a brief summary of what you discover. GGally::ggpairs(bind_cols(college[,2],college[,12:18]), mapping = aes(color = Private), progress = FALSE, lower = list(combo = GGally::wrap(&quot;facethist&quot;, bins = 40))) + theme_bw() + theme(panel.grid = element_blank()) + labs(caption = &quot;Source: ISLR2::College | Ten numeric features&quot;) The public institutions in this dataset enroll more undergraduates, while the private ones have more pursuing terminal degrees in their fields. 9. This exercise involves the Auto data set studied in the lab. Make sure that the missing values have been removed from the data. (a) Which of the predictors are quantitative, and which are qualitative? Auto &lt;- ISLR2::Auto %&gt;% drop_na %&gt;% tibble() # summary(Auto) skimr::skim(Auto) Table 2.1: Data summary Name Auto Number of rows 392 Number of columns 9 _______________________ Column type frequency: factor 1 numeric 8 ________________________ Group variables None Variable type: factor skim_variable n_missing complete_rate ordered n_unique top_counts name 0 1 FALSE 301 amc: 5, for: 5, toy: 5, amc: 4 Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist mpg 0 1 23.45 7.81 9 17.00 22.75 29.00 46.6 ▆▇▆▃▁ cylinders 0 1 5.47 1.71 3 4.00 4.00 8.00 8.0 ▇▁▃▁▅ displacement 0 1 194.41 104.64 68 105.00 151.00 275.75 455.0 ▇▂▂▃▁ horsepower 0 1 104.47 38.49 46 75.00 93.50 126.00 230.0 ▆▇▃▁▁ weight 0 1 2977.58 849.40 1613 2225.25 2803.50 3614.75 5140.0 ▇▇▅▅▂ acceleration 0 1 15.54 2.76 8 13.78 15.50 17.02 24.8 ▁▆▇▂▁ year 0 1 75.98 3.68 70 73.00 76.00 79.00 82.0 ▇▆▇▆▇ origin 0 1 1.58 0.81 1 1.00 1.00 2.00 3.0 ▇▁▂▁▂ Quantitative: mpg, cylinders, displacement, horsepower, weight, acceleration, year, origin Qualitative: name (b) What is the range of each quantitative predictor? You can answer this using the range() function. range() Auto %&gt;% select(where(is.numeric)) %&gt;% summarize(across(everything(), range)) %&gt;% mutate(level = c(&quot;min&quot;, &quot;max&quot;)) %&gt;% pivot_longer(cols = -level, names_to = &quot;metric&quot;, values_to = &quot;value&quot;) %&gt;% pivot_wider(names_from = level, values_from = value) ## # A tibble: 8 × 3 ## metric min max ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 mpg 9 46.6 ## 2 cylinders 3 8 ## 3 displacement 68 455 ## 4 horsepower 46 230 ## 5 weight 1613 5140 ## 6 acceleration 8 24.8 ## 7 year 70 82 ## 8 origin 1 3 (c) What is the mean and standard deviation of each quantitative predictor? Auto %&gt;% select(where(is.numeric)) %&gt;% summarize(across(everything(), mean, na.rm = TRUE)) %&gt;% pivot_longer(everything(), values_to = &quot;mean&quot;) ## # A tibble: 8 × 2 ## name mean ## &lt;chr&gt; &lt;dbl&gt; ## 1 mpg 23.4 ## 2 cylinders 5.47 ## 3 displacement 194. ## 4 horsepower 104. ## 5 weight 2978. ## 6 acceleration 15.5 ## 7 year 76.0 ## 8 origin 1.58 Auto %&gt;% select(where(is.numeric)) %&gt;% summarize(across(everything(), sd, na.rm = TRUE)) %&gt;% pivot_longer(everything(), values_to = &quot;sd&quot;) ## # A tibble: 8 × 2 ## name sd ## &lt;chr&gt; &lt;dbl&gt; ## 1 mpg 7.81 ## 2 cylinders 1.71 ## 3 displacement 105. ## 4 horsepower 38.5 ## 5 weight 849. ## 6 acceleration 2.76 ## 7 year 3.68 ## 8 origin 0.806 (d) Now remove the 10th through 85th observations. What is the range, mean, and standard deviation of each predictor in the subset of the data that remains? Auto[-c(10:85), ] %&gt;% select(where(is.numeric)) %&gt;% summarize(across(everything(), range, na.rm = TRUE)) %&gt;% mutate(level = c(&quot;min&quot;, &quot;max&quot;)) %&gt;% pivot_longer(cols = -level, names_to = &quot;metric&quot;, values_to = &quot;value&quot;) %&gt;% pivot_wider(names_from = level, values_from = value) ## # A tibble: 8 × 3 ## metric min max ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 mpg 11 46.6 ## 2 cylinders 3 8 ## 3 displacement 68 455 ## 4 horsepower 46 230 ## 5 weight 1649 4997 ## 6 acceleration 8.5 24.8 ## 7 year 70 82 ## 8 origin 1 3 Auto[-c(10:85), ] %&gt;% select(where(is.numeric)) %&gt;% summarize(across(everything(), mean, na.rm = TRUE)) %&gt;% pivot_longer(everything(), values_to = &quot;mean&quot;) ## # A tibble: 8 × 2 ## name mean ## &lt;chr&gt; &lt;dbl&gt; ## 1 mpg 24.4 ## 2 cylinders 5.37 ## 3 displacement 187. ## 4 horsepower 101. ## 5 weight 2936. ## 6 acceleration 15.7 ## 7 year 77.1 ## 8 origin 1.60 Auto[-c(10:85), ] %&gt;% select(where(is.numeric)) %&gt;% summarize(across(everything(), sd, na.rm = TRUE)) %&gt;% pivot_longer(everything(), values_to = &quot;sd&quot;) ## # A tibble: 8 × 2 ## name sd ## &lt;chr&gt; &lt;dbl&gt; ## 1 mpg 7.87 ## 2 cylinders 1.65 ## 3 displacement 99.7 ## 4 horsepower 35.7 ## 5 weight 811. ## 6 acceleration 2.69 ## 7 year 3.11 ## 8 origin 0.820 (e) Using the full data set, investigate the predictors graphically, using scatterplots or other tools of your choice. Create some plots highlighting the relationships among the predictors. Comment on your findings. GGally::ggpairs(select(Auto, -name), progress = FALSE, lower = list(combo = GGally::wrap(&quot;facethist&quot;, bins = 40))) displacement appears to have a positive relationship with horsepower mpg has a negative, nonlinear relationship with displacement (f) Suppose that we wish to predict gas mileage (mpg) on the basis of the other variables. Do your plots suggest that any of the other variables might be useful in predicting mpg? Justify your answer. Yes, a predictive model is certainly possible with this data set. 2.4 Exercises 57 10. This exercise involves the Boston housing data set. (a) To begin, load in the Boston data set. The Boston data set is part of the ISLR2 library. How many rows are in this data set? nrow(ISLR2::Boston) ## [1] 506 How many columns? ncol(ISLR2::Boston) ## [1] 13 What do the rows and columns represent? These are housing sale values medv and the characteristics of 506 suburbs of Boston. (b) Make some pairwise scatterplots of the predictors (columns) in this data set. Describe your findings. GGally::ggpairs(ISLR2::Boston, progress = FALSE, lower = list(combo = GGally::wrap(&quot;facethist&quot;, bins = 40))) medv is reduced with higher lstat medv increases with rm (c) Are any of the predictors associated with per capita crime rate? If so, explain the relationship. Yes, rad highway access moderately at 0.626. Crime is highest at low rad, drops quickly, and then increase somewhat at high rad. (d) Do any of the census tracts of Boston appear to have particularly high crime rates? Tax rates? Pupil-teacher ratios? Comment on the range of each predictor ISLR2::Boston %&gt;% slice_max(crim, n = 3) ## crim zn indus chas nox rm age dis rad tax ptratio lstat medv ## 1 88.9762 0 18.1 0 0.671 6.968 91.9 1.4165 24 666 20.2 17.21 10.4 ## 2 73.5341 0 18.1 0 0.679 5.957 100.0 1.8026 24 666 20.2 20.62 8.8 ## 3 67.9208 0 18.1 0 0.693 5.683 100.0 1.4254 24 666 20.2 22.98 5.0 ISLR2::Boston %&gt;% ggplot(aes(crim)) + geom_histogram(bins = 30) Yes, the median crime rate crim is very low, at only 0.257. This feature is heavily skewed right. The range runs from 0.00637 to 89.0. ISLR2::Boston %&gt;% slice_max(tax, n = 3) ## crim zn indus chas nox rm age dis rad tax ptratio lstat medv ## 1 0.15086 0 27.74 0 0.609 5.454 92.7 1.8209 4 711 20.1 18.06 15.2 ## 2 0.18337 0 27.74 0 0.609 5.414 98.3 1.7554 4 711 20.1 23.97 7.0 ## 3 0.20746 0 27.74 0 0.609 5.093 98.0 1.8226 4 711 20.1 29.68 8.1 ## 4 0.10574 0 27.74 0 0.609 5.983 98.8 1.8681 4 711 20.1 18.07 13.6 ## 5 0.11132 0 27.74 0 0.609 5.983 83.5 2.1099 4 711 20.1 13.35 20.1 ISLR2::Boston %&gt;% ggplot(aes(tax)) + geom_histogram(bins = 30) Yes, median full value tax rates range from 169k to 711k. The distribution across suburbs is also not even. ISLR2::Boston %&gt;% slice_min(ptratio , n = 3) ## crim zn indus chas nox rm age dis rad tax ptratio lstat medv ## 1 0.04011 80 1.52 0 0.404 7.287 34.1 7.309 2 329 12.6 4.08 33.3 ## 2 0.04666 80 1.52 0 0.404 7.107 36.6 7.309 2 329 12.6 8.61 30.3 ## 3 0.03768 80 1.52 0 0.404 7.274 38.3 7.309 2 329 12.6 6.62 34.6 ISLR2::Boston %&gt;% ggplot(aes(ptratio)) + geom_histogram(bins = 30) The ptratio is skewed left, with the bulk of the suburbs at the high end of the range from 2.16 to 38. (e) How many of the census tracts in this data set bound the Charles river? ISLR2::Boston %&gt;% count(chas == 1) ## chas == 1 n ## 1 FALSE 471 ## 2 TRUE 35 35 suburbs bound the Charles River (f) What is the median pupil-teacher ratio among the towns in this data set? ISLR2::Boston %&gt;% summarise(ptratio_median = median(ptratio)) ## ptratio_median ## 1 19.05 (g) Which census tract of Boston has lowest median value of owner occupied homes? ISLR2::Boston %&gt;% add_rowindex() %&gt;% select(.row, medv) %&gt;% filter(medv == min(medv)) ## .row medv ## 1 399 5 ## 2 406 5 Rows 399 and 406. What are the values of the other predictors for that census tract, and how do those values compare to the overall ranges for those predictors? Comment on your findings. ISLR2::Boston %&gt;% mutate(bottom_tracts = if_else(medv == min(medv), &quot;Bottom&quot;, NA_character_)) %&gt;% pivot_longer(cols = -c(bottom_tracts, medv)) %&gt;% ggplot(aes(value, medv, color = bottom_tracts)) + geom_point() + facet_wrap(vars(name), scales = &quot;free_x&quot;) + theme_bw() + theme(strip.placement = &quot;outside&quot;, panel.grid = element_blank(), strip.background = element_blank()) + labs(caption = &quot;Source: ISLR2::Boston | Lowest Median Home Value Suburbs&quot;, color = NULL) The lowest median value suburbs of Boston are the oldest, they have higher crime than most, they are close to empoyment centers, they are related to industry, they have high taxes, and a low zn proportion of large lots zoned. (h) In this data set, how many of the census tracts average more than seven rooms per dwelling? More than eight rooms per dwelling? Comment on the census tracts that average more than eight rooms per dwelling. ISLR2::Boston %&gt;% mutate(room_state = case_when( rm &gt; 8 ~ &quot;eight_plus&quot;, rm &gt; 7 ~ &quot;seven_to_eight&quot;, TRUE ~ NA_character_ )) %&gt;% count(room_state) ## room_state n ## 1 eight_plus 13 ## 2 seven_to_eight 51 ## 3 &lt;NA&gt; 442 ISLR2::Boston %&gt;% mutate(room_state = case_when( rm &gt; 8 ~ &quot;eight_plus&quot;, rm &gt; 7 ~ &quot;seven_to_eight&quot;, TRUE ~ NA_character_ )) %&gt;% pivot_longer(cols = -c(room_state, medv)) %&gt;% ggplot(aes(value, medv, color = room_state)) + geom_point(shape = 21, alpha = 0.7) + facet_wrap(vars(name), scales = &quot;free_x&quot;) + theme_bw() + theme(strip.placement = &quot;outside&quot;, panel.grid = element_blank(), strip.background = element_blank()) + labs(caption = &quot;Source: ISLR2::Boston | More than 8 rooms per dwelling&quot;, color = NULL) Again, the oldest homes have more than 8 rooms per dwelling. They have the lowest tax assessments and a low proportion of non-retail business acres per town. "],["meeting-videos-1.html", "2.4 Meeting Videos", " 2.4 Meeting Videos 2.4.1 Cohort 1 Meeting chat log 1 00:08:32 Fran Barton: A very rainy afternoon here in this corner of England 00:09:17 Kim Martin: Good afternoon everyone :) 00:09:31 Stijn: Hiya! 00:09:59 Jon Harmon (jonthegeek): Good {time wherever you are}, everyone! 00:11:49 Stijn: You type like a programmer would, Jon 00:13:10 Kim Martin: @Stijn How does a programmer type? 00:13:24 Kim Martin: The curly braces? 00:13:51 Stijn: Hahaha, just making lame jokes over here. Let&#39;s not read into it too much :D 00:14:04 Kim Martin: (overthinking-R-us) 00:14:09 shamsuddeen: Increase the size pls 00:14:49 Wayne Defreitas: if you go to view options in zoom, you can also zoom screen size 00:15:38 Fran Barton: Would it be possible to F11 to make the browser full screen? 00:15:48 Kim Martin: These are your _notes_?! 00:15:49 August: That is awesome, thank you! 00:15:54 shamsuddeen: Thanks 00:15:55 Stijn: That is REALLY cool, thanks Ray! 00:16:00 Kim Martin: Link please! 00:16:09 Mei Ling Soh: Loved the appendix section 00:16:10 Laura Rose: very cool; thanks! 00:16:11 Keuntae Kim: awesome!!! I need this!!! 00:16:22 Ryan Metcalf: This is great. Thank you! 00:16:30 Kim Martin: Ah - found it: https://r4ds.github.io/bookclub-islr/statistical-learning.html 00:16:43 Jon Harmon (jonthegeek): r4ds.io/islr is the shortcut 00:17:28 Stijn: dependent and independent variables is what I&#39;m used to 00:17:44 Fran Barton: I&#39;m pretty new to all of this so I&#39;ll just take whatever terminology I&#39;m given for now 00:17:52 Jon Harmon (jonthegeek): Note to self: I should increase the default font size in this notes-book. 00:18:23 Brett Longworth: Features is newish to me. Not part of the ML club. =) 00:20:19 Jon Harmon (jonthegeek): &quot;Features&quot; is more used in the ML world, &quot;independent variable&quot; in actual science, in my experience. 00:21:12 Jyoti: the estimates of the regression coefficients are computed such that on an average the estimate of error becomes zero 00:21:30 Jon Harmon (jonthegeek): There&#39;s always a (somewhat) relevant xkcd: https://xkcd.com/2048/ 00:22:18 Till: from a probabilistic point, given that you model is correct, the error distribution is assumed to have a zero mean and any deviations from a prediction is completely random 00:23:31 Fran Barton: &quot;the whole point of the book is to present different ways to come up with surfaces like this&quot; (or something like that - Ray) - thank you, I like that kind of context, helps me get the bigger picture. 00:23:37 shamsuddeen: Y is also a function of ϵ, which, by definition, cannot be predicted using X. What this means from the book? 00:24:18 Kim Martin: Y = f(X) if everything about Y can be (perfectly) predicted by X 00:24:44 Jon Harmon (jonthegeek): Y depends on both X and epsilon, and you can&#39;t predict epsilon from X; if you could, it&#39;d be part of f(X). 00:25:04 Kim Martin: … but since Y _cannot_ be perfectly predicted just with X (there are other important variables that are not included in X... or some unpredictable randomness) this is acknowledged by adding the error term... hence Y = f(X) + e 00:26:43 shamsuddeen: It is important to keep in mind that the irreducible error will always provide an upper bound on the accuracy of our prediction for Y. This bound is almost always unknown in practice. 00:28:20 shamsuddeen: Thank you 00:29:52 August: this is basically image 2.9 00:30:52 Kim Martin: Every point in the _training_ set... but what about the _test_ set? 00:31:20 Kim Martin: You can fit your function perfect... with no errors... to your training set... but what happens when you get new data? 00:31:35 Rahul T: I think here it’s on the whole population but in most cases we only have sample and we can fit that sample well but doesn’t mean it will generalize well to the population 00:31:36 shamsuddeen: Uhmmm 00:31:39 shamsuddeen: Thanks 00:33:25 SriRam: In complex theory, you may call this rationality vs bounded rationality , there is always a boundary stopping you from knowing everything 00:33:28 shamsuddeen: I got it. Rahu, your point also make sense. 00:33:36 Brett Longworth: We can fit a function that fits all points, but that removes irreducible error, which I think is the definition of overfitting? 00:33:54 shamsuddeen: Yes, Bret I guess so 00:34:47 Brett Longworth: Best argument for a training and test set I&#39;ve heard. Overfitting should show up in the fit for the test set. 00:35:02 Kim Martin: Some domains have smaller error than others though... e.g. simple physics experiments will have a smaller error than (eg) experiments involving complex systems (including human behaviour) 00:35:16 Keuntae Kim: Also, the perfect prediction model for the sample does not necessarily guarantee prefect predicted outcomes when a new dataset comes into the model. 00:36:06 Fran Barton: I wonder if there is some confusion among the group as to the meaning of the word &quot;error&quot;. Here it doesn&#39;t mean that you as a researcher have done something wrong. It doesn&#39;t mean the model is unusable. It just means any useful model will inevitably only approximate to the phenomenon itself. 00:36:20 Sangeeta Bhatia: yes that makes sense. if we knew the truth, we won’t need “f”. 00:36:47 Brett Longworth: So for the physics example, running a jagged line through all points of a distance vs time curve would immediately be shown to be an overfit when predicting with new data. 00:37:50 Kim Martin: That&#39;s whey section 2.2.1 goes to pains to emphasize that the model should be assessed (MSE etc) based on the test data, not the training set (which will most likely be far lower) 00:38:16 August: I think we are getting overly bogged down here, this is revisited several times as the book progresses 00:38:28 shamsuddeen: Yes, lets progress 00:45:06 shamsuddeen: How to make this kind of plot in ggplot? 00:45:52 Keuntae Kim: geom_contour() &lt;- I guess 00:46:02 Stijn: https://www.r-graph-gallery.com/3d-surface-plot.html 00:46:10 Stijn: Not ggplot though 00:46:22 Keuntae Kim: https://ggplot2.tidyverse.org/reference/geom_contour.html#:~:text=ggplot2%20can%20not%20draw%20true,can%20appear%20at%20most%20once. 00:46:25 August: https://stackoverflow.com/questions/38331198/add-regression-plane-to-3d-scatter-plot-in-plotly 00:46:32 Keuntae Kim: Okay.. 00:46:34 August: plotly for the win 00:46:36 Ryan Metcalf: Ive tried this 3D in Python. (But that doesn’t apply to R.) 00:47:10 Stijn: And let&#39;s give rayshader an honourable mention for 3D stuff! 00:48:09 Rahul T: I am not sure if this has the code - it’s listed on the book site as a resource https://web.stanford.edu/~hastie/ISLR2/Labs/ 00:49:06 Jon Harmon (jonthegeek): A long, unfinished thread about surface in R: https://rfordatascience.slack.com/archives/C8JRJSW4S/p1627573385008700 00:49:54 Jon Harmon (jonthegeek): @rahul That&#39;s just the labs (~1/2 of each chapter). 00:51:35 Rahul T: Got it, thanks! 00:51:38 Kim Martin: Super cynical, Raymond 😂 00:52:02 August: Hey that&#39;s my job your talking about! :P 00:52:10 Ryan S: I&#39;m going to try that approach with my boss -- &quot;I need more money because I can do better!&quot; 00:52:18 Kim Martin: Why MSE vs RMSE? 00:52:21 August: but I can! 00:52:45 Kim Martin: Only to get units in normal sense? 00:53:28 Rahul T: May be it’s easy to derive the bias variance tradeoff - just a guess 00:53:30 Kim Martin: Why does ISLR2 stop at MSE? Because it does the job (getting &#39;normal units&#39; is irrelevant)? 00:53:31 shamsuddeen: No Free Lunch Theorem for Machine Learning: https://machinelearningmastery.com/no-free-lunch-theorem-for-machine-learning/ 00:54:29 Jon Harmon (jonthegeek): Yeah, that&#39;d be my take. RMSE is usually more reportable but if you&#39;re just trying to calculate a number to compare your fit to another fit. I definitely prefer RMSE! 00:55:34 Stijn: I&#39;m not sure what the &#39;decrease-stagnate-increase&#39; implies :/ 00:57:37 Stijn: Whew, I think it&#39;s somewhat clicking... Will have to re-read 00:58:43 Keuntae Kim: In the figure at the top, a yellow linear line is too simple, so a lot of errors when a new dataset comes in, so weak predictive power. 00:59:16 Kim Martin: How are they quantifying &#39;flexibility&#39;? 00:59:43 Kim Martin: Number of parameters to estimate (2 for linear)? 00:59:51 Rahul T: Book says “degrees of freedom, for a number of smoothing splines.” 01:00:07 Rahul T: It will be discussed in ch7 01:00:10 Keuntae Kim: For the line passing through every point, no errors (almost zero variance), but extremely hard to predict or estimate values when a new dataset comes in. 01:00:22 Keuntae Kim: This is what I understand from the figures. 01:00:25 Rahul T: They say at the end of page 31 01:04:20 David Severski: We’re coming up on time, do we want to find a good point to pause until next week? 01:04:43 Stijn: The idea that a biostats college professor also finds some of these sections challenging, makes me feel much more comfortable 😅 01:05:12 Kim Martin: 😅👍 01:05:38 Mei Ling Soh: I think we should stop at the bias-variance trade-off 01:07:46 shamsuddeen: Dev set 01:09:23 August: p36 explains this 01:09:59 David Severski: I’ve got to jet. Thanks for presenting and to everyone for the discussion! 01:10:42 Kim Martin: Is the aim to do the lab next week, or the week after? 01:10:49 Keuntae Kim: Good session today. Too many confusing things I have to digest!!! Need to study of my own more!!! haha 😅 01:11:16 Kim Martin: (I&#39;ll confess I didn&#39;t get through the Chapter, despite intending to have read it all by today) 01:11:31 Keuntae Kim: Ray, thank you very much. You did a great job to explain these complex things! 01:11:34 Mei Ling Soh: So, we have to try out the exercises before the next study session? 01:11:46 shamsuddeen: Thank you Ray. 01:11:50 August: try the lectures online if you struggle with the book, it&#39;ll help reading the material. 01:11:53 Kim Martin: It might be nice to try to share code / visuals for (trying to) explore these topics more 01:12:01 Mei Ling Soh: Thank you, Ray! 01:12:08 Laura Rose: Thanks, Ray! 01:12:09 collinberke: Thanks, Ray! 01:12:17 Rahul T: This was very helpful. Thank you, Ray! 01:12:26 Till: yes, many thanks! 01:12:31 Kaustav Sen: Thanks Ray! 01:12:51 Kim Martin: @August you mean these: https://www.dataschool.io/15-hours-of-expert-machine-learning-videos/ 01:13:09 August: https://emilhvitfeldt.github.io/ISLR-tidymodels-labs/statistical-learning.html 01:13:37 Fran Barton: thanks everyone 01:14:10 Ryan S: Thanks everyone! 01:14:14 Kim Martin: Thanks all! 01:14:16 Keuntae Kim: Thank you everyone! Meeting chat log 2 00:05:01 A. S. Ghatpande: Hello 00:05:35 jonathan.bratt: hello! 00:07:02 Ryan Metcalf: Good morning/afternoon/evening everyone! 00:08:02 jonathan.bratt: I missed last week; Ray, were you going to finish going through chapter 2 today? 00:09:41 David Severski: I just noticed Raymond’s Skinny Puppy poster. Awesome. Now I need to queue that up for today. 00:09:45 David Severski: 😄 00:12:11 Ryan Metcalf: Ryan S. 00:19:33 A. S. Ghatpande: How would you calculate bias in the example given? 00:19:48 Jon Harmon (jonthegeek): r4ds.io/islr 00:22:40 shamsuddeen: We do have validation test also sometimes. Is that called validation error ? 00:24:00 shamsuddeen: The chapter seems not discuss anything about validation set 00:25:14 SriRam: K is number of neighbors to consider I think 00:27:29 Rahul T: I think it’s (f - E(f_hat)) 00:28:11 Rahul T: f is true function and E(f_hat) in different samples expected value 00:29:03 SriRam: Bias is error predicted vs observed , from my understanding 00:29:29 August: might be useful: http://scott.fortmann-roe.com/docs/BiasVariance.html 00:30:12 Keuntae Kim: https://www.value-at-risk.net/bias/ &lt;-- about bias in a mathematical way, but it is basically about the difference between the actual and estimated values. 00:31:33 A. S. Ghatpande: thanks for all the answers here about bias, enough food for thought 00:37:52 Ryan Metcalf: Would you be able to wrangle the data or clean the data more? 00:38:01 Ryan Metcalf: If it is noise, likely not. 00:38:12 A. S. Ghatpande: you need more data! 00:38:33 August: Yes and not depends on data and what you can bring in i.e weather data or create from the data ie pca 00:53:53 Rahul T: Got it, that’s helpful. Thank you! 00:56:43 August: basically predictive power is desirable, but not the main concern with inferential. Sometime you have to sacrifice predictive accuracy for explainability. Generally you hope to approach consensus within the zeitgeist. 00:57:01 shamsuddeen: Email spam classification 01:03:05 A. S. Ghatpande: spammers wrote inferential models! 01:03:14 SriRam: Lol 01:04:58 Raymond Balise: so sorry I need to be in another meeting in two minutes 01:05:23 Jon Harmon (jonthegeek): No problem, we&#39;ll stop very soon. 01:06:06 shamsuddeen: I need to leave now. See u all next. Thanks 01:06:46 Rahul T: Thank you! 01:06:50 A. S. Ghatpande: very good thankseveryone 01:06:51 collinberke: Thank you! 01:06:52 Ryan S: thanks! 2.4.2 Cohort 2 Meeting chat log 00:22:35 Ricardo Serrano: Simpson’s Paradox is a statistical phenomenon where an association between two variables in a population emerges, disappears or reverses when the population is divided into subpopulations. https://plato.stanford.edu/entries/paradox-simpson/ 00:28:46 Michael Haugen: the book defines irreducible error as measurement errors and/or unmeasured variables. 00:33:45 Michael Haugen: Interesting that baseball has the concept of “unforced error.”To create error due to not having the best functional form (reducible error) is kind of like an unforced error. 00:42:27 Ricardo Serrano: NLP is another unsupervised learning methodology 00:48:50 Michael Haugen: the green line is a good example of being too wiggly right? 01:01:16 Federica Gazzelloni: Dimensionality reduction: https://juliasilge.com/blog/billboard-100/ 01:02:05 Michael Haugen: Online version of book: http://www.feat.engineering/ 01:03:59 Michael Haugen: one stats book to another Meeting chat log 00:54:05 Ricardo: MNIST dataset 2.4.3 Cohort 3 Meeting chat log 00:09:52 Fariborz Soroush: none here :) 00:31:06 Mei Ling Soh: More on parametric and non-parametric tests: https://byjus.com/maths/difference-between-parametric-and-nonparametric/#:~:text=The%20key%20difference%20between%20parametric,tendency%20with%20the%20median%20value. 00:31:28 Mei Ling Soh: Sometimes, it is better to use non-parametric tests as they are based on median 00:35:15 Mei Ling Soh: ggplot-2 book: https://ggplot2-book.org/ 00:36:06 Mei Ling Soh: Bookclub on slack `#book_club-ggplot2` 00:39:58 Jeremy Selva: Welch t test is not equal variance one. Student is the equal variance one. Both are parametric because they assume a normal (known) distribution. 00:41:43 Mei Ling Soh: Thanks, Jeremy. I thought welch was a non-parametric test 😅 00:54:26 Rahul: This is my derivation for the earlier equation we were talking about. 01:05:56 Rahul: This has a good info on parametric vs non parametric https://sebastianraschka.com/faq/docs/parametric_vs_nonparametric.html 01:06:03 Rahul: For later read 01:06:07 Rahul: Thank you! Meeting chat log 00:27:35 Mei Ling Soh: https://www.crumplab.com/statisticsLab/lab-10-factorial-anova.html 00:28:41 Mei Ling Soh: The answers👆 00:31:51 Rose Franzen: Sorry Mei Ling, the answers for what? That link doesn’t seem to be related to this content (unless I’m totally missing something, which is possible 🙂 ) 00:32:44 Mei Ling Soh: https://onmee.github.io/assets/docs/ISLR/Statistical-Learning.pdf 00:55:06 Rose Hartman: My preference is to rush through so we can keep getting to new content 🙂 00:55:11 Celine H: I’d rather take our time to go through the material 00:55:13 shamsuddeen: Where is the sign-up sheet? 00:55:25 Nilay Yönet: https://docs.google.com/spreadsheets/d/1xab0RUdnUC6V-RkXvZqTcLvJrkY6T2ZHAZSUDA_krn4/edit#gid=0 00:55:30 Celine H: I’m ok either way. 00:55:39 Rose Hartman: Me too 🙂 00:57:59 Celine H: I’d love to see the tidy model example. 00:58:06 Celine H: tidyverse 2.4.4 Cohort 4 Meeting chat log ADD LOG HERE "],["linear.html", "Chapter 3 Linear Regression", " Chapter 3 Linear Regression Learning objectives: Perform linear regression with a single predictor variable. Estimate the standard error of regression coefficients. Evaluate the goodness of fit of a regression. Perform linear regression with multiple predictor variables. Evaluate the relative importance of variables in a multiple linear regression. Include interaction effects in a multiple linear regression. Perform linear regression with qualitative predictor variables. Model non-linear relationships using polynomial regression. Identify non-linearity in a data set. Compare and contrast linear regression with KNN regression. "],["questions-to-answer.html", "3.1 Questions to Answer", " 3.1 Questions to Answer Recall the Advertising data from Chapter 2. Here are a few important questions that we might seek to address: Is there a relationship between advertising budget and sales? How strong is the relationship between advertising budget and sales? Does knowledge of the advertising budget provide a lot of information about product sales? Which media are associated with sales? How large is the association between each medium and sales? For every dollar spent on advertising in a particular medium, by what amount will sales increase? How accurately can we predict future sales? Is the relationship linear? If there is approximately a straight-line relationship between advertising expenditure in the various media and sales, then linear regression is an appropriate tool. If not, then it may still be possible to transform the predictor or the response so that linear regression can be used. Is there synergy among the advertising media? Or, in stats terms, is there an interaction effect? "],["simple-linear-regression-definition.html", "3.2 Simple Linear Regression: Definition", " 3.2 Simple Linear Regression: Definition Simple linear regression: Very straightforward approach to predicting response \\(Y\\) on predictor \\(X\\). \\[Y \\approx \\beta_{0} + \\beta_{1}X\\] Read “\\(\\approx\\)” as “is approximately modeled by.” \\(\\beta_{0}\\) = intercept \\(\\beta_{1}\\) = slope \\[\\hat{y} = \\hat{\\beta}_{0} + \\hat{\\beta}_{1}x\\] \\(\\hat{\\beta}_{0}\\) = our approximation of intercept \\(\\hat{\\beta}_{1}\\) = our approximation of slope \\(x\\) = sample of \\(X\\) \\(\\hat{y}\\) = our prediction of \\(Y\\) from \\(x\\) hat symbol denotes “estimated value” Linear regression is a simple approach to supervised learning "],["simple-linear-regression-visualization.html", "3.3 Simple Linear Regression: Visualization", " 3.3 Simple Linear Regression: Visualization Figure 3.1: For the Advertising data, the least squares fit for the regression of sales onto TV is shown. The fit is found by minimizing the residual sum of squares. Each grey line segment represents a residual. In this case a linear fit captures the essence of the relationship, although it overestimates the trend in the left of the plot. "],["simple-linear-regression-math.html", "3.4 Simple Linear Regression: Math", " 3.4 Simple Linear Regression: Math RSS = residual sum of squares \\[\\mathrm{RSS} = e^{2}_{1} + e^{2}_{2} + \\ldots + e^{2}_{n}\\] \\[\\mathrm{RSS} = (y_{1} - \\hat{\\beta}_{0} - \\hat{\\beta}_{1}x_{1})^{2} + (y_{2} - \\hat{\\beta}_{0} - \\hat{\\beta}_{1}x_{2})^{2} + \\ldots + (y_{n} - \\hat{\\beta}_{0} - \\hat{\\beta}_{1}x_{n})^{2}\\] \\[\\hat{\\beta}_{1} = \\frac{\\sum_{i=1}^{n}{(x_{i}-\\bar{x})(y_{i}-\\bar{y})}}{\\sum_{i=1}^{n}{(x_{i}-\\bar{x})^{2}}}\\] \\[\\hat{\\beta}_{0} = \\bar{y} - \\hat{\\beta}_{1}\\bar{x}\\] \\(\\bar{x}\\), \\(\\bar{y}\\) = sample means of \\(x\\) and \\(y\\) 3.4.1 Visualization of Fit Figure 3.2: Contour and three-dimensional plots of the RSS on the Advertising data, using sales as the response and TV as the predictor. The red dots correspond to the least squares estimates \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\), given by (3.4) Learning Objectives: Perform linear regression with a single predictor variable. ✔️ "],["assessing-accuracy-of-coefficient-estimates.html", "3.5 Assessing Accuracy of Coefficient Estimates", " 3.5 Assessing Accuracy of Coefficient Estimates \\[Y = \\beta_{0} + \\beta_{1}X + \\epsilon\\] RSE = residual standard error Estimate of \\(\\sigma\\) \\[\\mathrm{RSE} = \\sqrt{\\frac{\\mathrm{RSS}}{n - 2}}\\] \\[\\mathrm{SE}(\\hat\\beta_0)^2 = \\sigma^2 \\left[\\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum_{i=1}^n (x_i - \\bar{x})^2}\\right],\\ \\ \\mathrm{SE}(\\hat\\beta_1)^2 = \\frac{\\sigma^2}{\\sum_{i=1}^n (x_i - \\bar{x})^2}\\] 95% confidence interval: a range of values such that with 95% probability, the range will contain the true unknown value of the parameter If we take repeated samples and construct the confidence interval for each sample, 95% of the intervals will contain the true unknown value of the parameter \\[\\hat\\beta_1 \\pm 2\\ \\cdot \\ \\mathrm{SE}(\\hat\\beta_1)\\] \\[\\hat\\beta_0 \\pm 2\\ \\cdot \\ \\mathrm{SE}(\\hat\\beta_0)\\] Learning Objectives: Estimate the standard error of regression coefficients. ✔️ "],["assessing-the-accuracy-of-the-model.html", "3.6 Assessing the Accuracy of the Model", " 3.6 Assessing the Accuracy of the Model RSE can be considered a measure of the lack of fit of the model. a \\(R^2\\) statistic provides an alternative that is in the form of a proportion of the variance explained, ranges from 0 to 1, a good value depends on teh application. \\[R^2 = 1 - \\frac{RSS}{TSS}\\] where TSS is the total sum of squarse: \\[TSS = \\Sigma (y_i - \\bar{y})^2\\] "],["multiple-linear-regression.html", "3.7 Multiple Linear Regression", " 3.7 Multiple Linear Regression Multiple linear regression extends simple linear regression for p predictors: \\[Y = \\beta_{0} + \\beta_{1}X_1 + \\beta_{2}X_2 + ... +\\beta_{p}X_p + \\epsilon_i\\] - \\(\\beta_{j}\\) is the average effect on \\(Y\\) from \\(X_{j}\\) holding all other predictors fixed. Fit is once again choosing the \\(\\beta_{j}\\) that minimizes the RSS. Example in book shows that although fitting sales against newspaper alone indicated a significant slope (0.055 +- 0.017), when you include radio in a multiple regression, newspaper no longer has any significant effect. (-0.001 +- 0.006) 3.7.1 Important Questions Is at least one of the predictors \\(X_1\\), \\(X_2\\), … , \\(X_p\\) useful in predicting the response? F statistic close to 1 when there is no relationship, otherwise greater then 1. \\[F = \\frac{(TSS-RSS)/p}{RSS/(n-p-1)}\\] Do all the predictors help to explain \\(Y\\) , or is only a subset of the predictors useful? p-values can help identify important predictors, but it is possible to be mislead by this especially with large number of predictors. Variable selection methods include Forward selection, backward selection and mixed. Topic is continued in Chapter 6. How well does the model fit the data? \\(R^2\\) still gives proportion of the variance explained, so look for values “close” to 1. Can also look at RSE which is generalized for multiple regression as: \\[RSE = \\sqrt{\\frac{1}{n-p-1}RSS}\\] Given a set of predictor values, what response value should we predict, and how accurate is our prediction? Three sets of uncertainty in predictions: Uncertainty in the estimates of \\(\\beta_i\\) Model bias Irreducible error \\(\\epsilon\\) "],["qualitative-predictors.html", "3.8 Qualitative Predictors", " 3.8 Qualitative Predictors Dummy variables: if there are \\(k\\) levels, introduce \\(k-1\\) dummy variables which are equal to one (“one hot”) when the underlying qualitative predictor takes that value. For example if there are 3 levels, introduce two new dummy variables and fit the model: \\[y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\epsilon_i\\] Qualitative Predicitor \\(x_{i1}\\) \\(x_{i2}\\) level 0 (baseline) 0 0 level 1 1 0 level 2 0 1 Coefficients are interpreted the average effect relative to the baseline. Alternative is to use index variables, a different coefficient for each level: \\[y_i = \\beta_{0 1} + \\beta_{0 2} +\\beta_{0 3} + \\epsilon_i\\] "],["extensions.html", "3.9 Extensions", " 3.9 Extensions Interaction / Synergy effects Include a product term to account for synergy where one changes in one variable changes the association of the Y with another: \\[Y = \\beta_{0} + \\beta_{1}X_1 + \\beta_{2}X_2 + \\beta_{3}X_1 X_2 + \\epsilon_i\\] Non-linear relationships (e.g. polynomial fits) \\[Y = \\beta_{0} + \\beta_{1}X + \\beta_{2}X^2 + ... \\beta_{n}X^n + \\epsilon_i\\] "],["potential-problems.html", "3.10 Potential Problems", " 3.10 Potential Problems Non-linear relationships Residual plots are useful tool to see if any remaining trends exist. If so consider fitting transformation of the data. Correlation of Error Terms Linear regression assumes that the error terms \\(\\epsilon_i\\) are uncorrelated. Residuals may indicate that this is not correct (obvious tracking in the data). One could also look at the autocorrelation of the residuals. What to do about it? Non-constant variance of error terms Again this can be revealed by examining the residuals. Consider transformation of the predictors to remove non-constant variance. The figure below shows residuals demonstrating non-constant variance, and shows this being mitigated to a great extent by log transforming the data. Figure 3.11 from book Outliers Outliers are points with for which \\(y_i\\) is far from value predicted by the model (including irreducible error). See point labeled ‘20’ in figure 3.13. Detect outliers by plotting studentized residuals (residual \\(e_i\\) divided by the estimated error) and look for residuals larger then 3 standard deviations in absolute value. An outlier may not effect the fit much but can have dramatic effect on the RSE. Often outliers are mistakes in data collection and can be removed, but could also be an indicator of a deficient model. High Leverage Points These are points with unusual values of \\(x_i\\). Examples is point labeled ‘41’ in figure 3.13. These points can have large impact on the fit, as in the example, including point 41 pulls slope up significantly. Use leverage statistic to identify high leverage points, which can be hard to identify in multiple regression. 6. Collinearity Two or more predictor variables are closely related to one another. Simple collinearity can be identified by looking at correlations between predictors. Causes the standard error to grow (and p-values to grow) Often can be dealt with by removing one of the highly correlated predictors or combining them. Multicollinearity (involving 3 or more predictors) is not so easy to identify. Use Variance inflation factor, which is the ratio of the variance of \\(\\hat{\\beta_j}\\) when fitting the full model to fitting the parameter on its own. Can be computed using the formula: \\[VIF(\\hat{\\beta_j}) = \\frac{1}{1-R^2_{X_j|X_{-j}}}\\] where \\(R^2_{X_j|X_{-j}}\\) is the \\(R^2\\) from a regression of \\(X_j\\) onto all the other predictors. "],["answers-to-the-marketing-plan-questions.html", "3.11 Answers to the Marketing Plan questions", " 3.11 Answers to the Marketing Plan questions Is there a relationship between advertising budget and sales? Tool: Multiple regression, look at F-statistic. How strong is the relationship between advertising budget and sales? Tool: \\(R^2\\) and RSE Which media are associated with sales? Tool: p-values for each predictor’s t-statistic. Explored further in chapter 6. How large is the association between each medium and sales? Tool: Confidence intervals on \\(\\hat{\\beta_j}\\) How accurately can we predict future sales? Tool:: Prediction intervals for individual response, confidence intervals for average response. Is the relationship linear? Tool: Residual Plots Is there synergy among the advertising media? Tool: Interaction terms and associated p-vales. "],["comparison-of-linear-regression-with-k-nearest-neighbors.html", "3.12 Comparison of Linear Regression with K-Nearest Neighbors", " 3.12 Comparison of Linear Regression with K-Nearest Neighbors This section examines the K-nearest neighbor (KNN) method (a non-parameteric method). This is essentially a k-point moving average. This serves to illustrate the Bias-Variance trade-off nicely "],["meeting-videos-2.html", "3.13 Meeting Videos", " 3.13 Meeting Videos 3.13.1 Cohort 1 Meeting chat log 00:07:48 A. S. Ghatpande: Good morning 00:07:56 Daniel Lupercio: Good morning everyone! 00:08:56 Kim Martin: 👋😁 00:10:32 Kim Martin: If linear regression had a motto: &quot;I contain multitudes!&quot; 00:11:06 Raymond Balise: +1 @Kim 00:15:43 Gianluca Pegoraro: Is the “approximately modeled by” sign due to the existence of the irreducible error + reducible error? 00:15:45 Kim Martin: Notation can be a surprisingly large barrier... and not hard to get over, when handled head-on. 00:16:18 Kim Martin: @Gianluca I think the irreducible error is a given - even the &#39;true&#39; function will suffer from irreducible error. 00:16:53 Gianluca Pegoraro: Thanks everyone for the answer 00:16:56 Kim Martin: God plays dice? ;) 00:18:08 Ryan Metcalf: For the team, has anyone ever created the plot in Figure 3.1? The same that Jon is sharing? 00:18:10 Kim Martin: Anyone know if there are any spaced repetition (eg AnkiDroid) cards for ISLR2 terms? 00:18:31 Kim Martin: If not, should we create them? 00:18:59 Raymond Balise: there is a package for that :) 00:19:07 Raymond Balise: no idea what it is but there is one 00:19:39 Raymond Balise: @ryan yep I have it somewhere 00:19:53 Raymond Balise: will share it in the channel 00:20:21 collinberke: I also have some resources as well, @Ryan Metcalf. 00:21:38 Kim Martin: RSS seems worded strangely: wouldn&#39;t it make more sense to call it &quot;sum of residual squares&quot;? 00:22:01 Daniel Lupercio: Some partial derivatives are used to find the minimization 00:22:05 jonathan.bratt: Yeah, I think “SRS” or “SSR” would be easier to remember. :) 00:23:55 Kim Martin: I think this might be a good walkthrough of the proof, if my notes are correct: https://www.khanacademy.org/math/statistics-probability/describing-relationships-quantitative-data/more-on-regression/v/proof-part-4-minimizing-squared-error-to-regression-line 00:28:12 Kaustav Sen: is there any particular reason for doing (n-2) instead of n in the RSE formula? 00:28:34 jonathan.bratt: something something degrees of freedom 00:28:38 Rahul T: Might be related to Degree of freedom? 00:28:54 Rahul T: We are estimating 2 beta params 00:29:22 Rahul T: My guess not sure 00:29:33 Kaustav Sen: ah.. makes sense. thanks! 00:30:23 August: I&#39;m happy to read on 00:30:23 Kim Martin: I&#39;m struggling to keep up - life getting in the way... wouldn&#39;t say no to a go slow 00:30:53 Kim Martin: What is more important: speed or solidity ;) 00:31:08 Raymond Balise: I like your presentation would rather wait 00:31:57 A. S. Ghatpande: sounds good, thanks Meeting chat log 00:09:43 jonathan.bratt: Is that Curious George as the Joker? 00:12:15 Kim Martin: 👋😁 00:13:58 Kim Martin: Time to grab a coffee still? 00:14:08 SriRam: Hi All, I missed the last class, may I know where we stopped the last time 00:14:50 Kim Martin: Signup https://docs.google.com/spreadsheets/d/1_pIPi68R_FwpzK_uMCRSKen9gdQLBx4sN3uifL-g_Nw/edit?usp=sharing 00:27:20 Raymond Balise: what was pluck() function? why use it instead of pull() 00:28:01 SriRam: May be revisit your code, I think values in the book are incorrect ? I may be wrong 00:31:35 SriRam: Eq 3.14 vs table 3.1, I don’t get the same values 00:37:39 Keuntae Kim: 7.0325/0.4578 = 15.36... 00:38:16 Keuntae Kim: The null hypothesis in this case is that the coefficient estimate is zero, which is not our expectation. 00:39:42 Keuntae Kim: Ha - H0 / SE = t-value 00:46:59 Jon Harmon (jonthegeek): &quot;Akaike information criterion&quot; 00:47:29 Jon Harmon (jonthegeek): I was hoping to hear someone pronounce that 🙃 00:48:23 SriRam: Thank you Kim, its the second value that seems incorrect 0.0475/0.0027 00:50:20 Rahul T: Also allows to invert X’X 00:50:43 Rahul T: I thought that was one of the reason 00:57:25 jonathan.bratt: Who can say “heteroscedasticity” fast? 00:59:03 Kim Martin: (good drinking game material, that) 00:59:39 Raymond Balise: +1 Kim 01:00:58 Keuntae Kim: The book does not mention about rule of thumbs for the VIF to evaluate the multicollinearity in the regression. What about your field? 01:02:04 Keuntae Kim: Rule of thumbs for VIF to test collinearity when you create a linear regression model? 01:03:45 Raymond Balise: I usually look in a SAS book at the office with a real formula…. but I think 5 is where you worry 01:04:35 Raymond Balise: van Belle has a book on statistical rules of thumb that is another place to look 01:04:42 Keuntae Kim: Someone says 2.5 or 3 and others also says 10. 01:05:00 Raymond Balise: 10 is a for sure you have a prolem 01:05:13 Keuntae Kim: Got it. 01:05:24 Federica Gazzelloni: can you share the github thing? 01:05:44 Raymond Balise: If I see something over 5 I will drop variables and see how much the betas shift around 01:08:45 A. S. Ghatpande: thanks August Meeting chat log 00:04:59 Kim Martin: 👋😁 00:05:12 Mei Ling Soh: Hihi! 00:21:08 August: python plots work the saem way 00:21:13 August: ^same 00:21:14 Laura Rose: yes 00:23:21 Laura Rose: i agree 00:23:26 Daniel Lupercio: Spot on 00:24:23 Federica Gazzelloni: tidymodels 00:36:16 David Severski: FYI, in zoom you can now share multiple apps at once by shift clicking on the window inside the share tray. 🙂 00:41:49 Mei Ling Soh: I&#39;m not following the tidymodels. Is there any introductory kind books for me to read on? 00:42:19 SriRam: https://www.tidymodels.org/learn/ 00:42:26 David Severski: As a tidymodels convert, the real power comes when you are doing lots of different model types with different specifications and/or want to do tuning in a model-implementation independent fashion. 00:43:22 A. S. Ghatpande: ISLR is much heavier lift than tidymodels 00:44:38 SriRam: I prefer tidy style as it allows dplyr with it, I find dplyr easy to handle data 00:46:46 Federica Gazzelloni: is there any difference in fitting interaction terms between classical model and tidymodels? 00:49:39 Daniel Lupercio: The interaction being that the variables are multiplied? 00:49:50 SriRam: I am getting a feeling that this (tidy) steps is like ggplot, adding layers to core model 00:50:13 Ryan Metcalf: @SriRam, I would agree! 00:50:44 David Severski: I’ve never found a complete reference to all the special operators within R model formulas… Anyone got one? Interaction, I(), power, multiplicity, etc... 00:50:59 Jon Harmon (jonthegeek): step_interact(terms = ~ body_mass_g:starts_with(&quot;species&quot;)) 00:53:32 Federica Gazzelloni: in general the use of step_interact is with all predictors, but in particular case you might need to choose just one or two predictors 00:54:31 SriRam: I was also confused with the colon : and the word “step”, I thought, this was stepwise regression with all predictors 00:55:19 Mei Ling Soh: Me too. I was thinking about the stepwise regression 00:56:40 Daniel Lupercio: Execersie 10 seems like a good start 01:02:02 SriRam: parsnip is by max ? So if I wish, I can do these exercises using parsnip ? I believe there is also a book by max….. predictive models something??? 01:05:52 David Severski: Parsnip is part of the overall tidymodels ecosystem, but Max and the team. The tidymodels books and learning references earlier in the chat cover parsnip. 01:07:34 David Severski: Oh dear, I never got the caret -&gt; parsnip transition. Total Dad joke. 😛 01:07:57 SriRam: 😀 01:07:58 jonathan.bratt: Yeah, it’s painfully clever :) 01:08:06 Laura Rose: yeah i didn&#39;t get that either. good dad joke tho 01:08:34 David Severski: There are three problems in computer science. Naming things and counting. ;) 01:08:39 SriRam: So what is the latest, so I train myself in that :( 01:09:29 David Severski: Tidymodels: parsnip + recipes + tuning + … 01:09:42 David Severski: tidyverse: dplyr + ggplot + forcats + … 01:10:15 David Severski: Both have metapackages that load the commonly used bits: library(tidymodels) and or library(tidyverse) 01:10:30 SriRam: So my caret book becomes a paper weight ? :( :( 01:10:47 David Severski: Caret is still supported, but will no longer get new features. 01:10:56 SriRam: Oke 01:11:24 Federica Gazzelloni: it depends if you need to set different parameters to your model, otherwise that is still to use 01:13:20 SriRam: Time to catch up on chapters !! 01:13:47 Kim Martin: 😐 01:14:01 Federica Gazzelloni: thanks 3.13.2 Cohort 2 Meeting chat log 00:54:05 Ricardo Serrano: https://github.com/rserran/melbourne_housing_kaggle 01:12:29 Michael Haugen: So what do you do instead of p values? Meeting chat log 00:15:30 Ricardo Serrano: https://github.com/rserran/melbourne_housing_kaggle 00:26:23 Jim Gruman: 😃 cool - TIL about `ggpubr` - thanks 00:32:04 jlsmith3: Very cool! 01:03:46 Jim Gruman: sounds great. thank you Ricardo! 01:06:45 Michael Haugen: https://docs.google.com/spreadsheets/d/1bqZ5EO_ilCDsCuSr5N0MRJqGFj-Sy0fVRrt5Mw21p48/edit#gid=0 01:06:48 Michael Haugen: Sign up 01:06:57 jlsmith3: Thanks for the link! Meeting chat log 00:24:30 Ricardo Serrano: https://github.com/rserran/melbourne_housing_kaggle 00:24:39 Federica Gazzelloni: thanks 00:59:01 Jim Gruman: tidy(x, conf.int = FALSE, conf.level = 0.95, exponentiate = FALSE, ...) the exponentiate = TRUE will back out of the log to give just the odds 00:59:04 Anna-Leigh Brown: https://www.wolframalpha.com/input/?i=logistic+function 01:06:46 Jim Gruman: I will always question the glm breakpoint of 0.5, A good discussion for offline of a means of adjustment here: https://towardsdatascience.com/bank-customer-churn-with-tidymodels-part-2-decision-threshold-analysis-c658845ef1f 01:07:22 Jim Gruman: thank you Michael ! 01:09:39 jlsmith3: Thank you, Michael! 01:09:44 Ricardo Serrano: Thanks, Michael! 3.13.3 Cohort 3 Meeting chat log 00:12:38 Rose Hartman: I think the edx videos are recorded by the authors, right? 00:12:39 Rahul: https://www.youtube.com/playlist?list=PLOg0ngHtcqbPTlZzRHA2ocQZqB1D_qZ5V 00:50:26 Mei Ling Soh: https://onmee.github.io/ISLR-Solutions/ 01:02:13 Rose Hartman: Yeah, this was great! Thanks! 3.13.4 Cohort 4 Meeting chat log ADD LOG HERE "],["classification.html", "Chapter 4 Classification", " Chapter 4 Classification Learning objectives: Compare and contrast classification with linear regression. Perform classification using logistic regression. Perform classification using linear discriminant analysis (LDA). Perform classification using quadratic discriminant analysis (QDA). Perform classification using naive Bayes. Identify the strengths and weaknesses of the various classification models. Model count data using Poisson regression. "],["an-overview-of-classification.html", "4.1 An Overview of Classification", " 4.1 An Overview of Classification Classification: Approaches to make inference and/or predict qualitative (categorical) response variable Few common classification techniques (classifiers): logistic regression linear discriminant analysis (LDA) quadratic discriminant analysis (QDA) naive Bayes K-nearest neighbors - Examples of classification problems: A person arrives at the emergency room with a set of symptoms that could possibly be attributed to one of three medical conditions. Which of the three conditions does the individual have? Predictor variable: Symptoms Response variable: Type of medical conditions An online banking service must be able to determine whether or not a transaction being performed on the site is fraudulent, on the basis of the user’s IP address, past transaction history, and so forth. Predictor variable: User’s IP address, past transaction history, etc Response variable: Fraudulent activity (Yes/No) On the basis of DNA sequence data for a number of patients with and without a given disease, a biologist would like to figure out which DNA mutations are deleterious (disease-causing) and which are not. Predictor variable: DNA sequence data Response variable: Presence of deleterious gene (Yes/No) In the following section, we are going to explore the Default dataset. The annual incomes (\\(X_1\\) = income) and monthly credit card balances (\\(X_2\\) =balance) are used to predict whether whether an individual will default on his or her credit card payment. Figure 4.1: The distribution of balance and income split by the binary default variable respectively; Note. Defaulters represented as orange plus sign; non-defaulters represented as blue circle "],["why-not-linear-regression.html", "4.2 Why NOT Linear Regression?", " 4.2 Why NOT Linear Regression? a regression method cannot convert a qualitative response variable with more than two levels into a quantitative response that is ready for linear regression \\[Y = \\left\\{ \\begin{array}{ll} 1 &amp; \\mbox{if stroke};\\\\ 2 &amp; \\mbox{if epileptic seizure};\\\\ 3 &amp; \\mbox{if drug overdose}.\\end{array} \\right.\\] Depending on the complexity of the problem, a regression method will not provide meaningful estimates of Pr(Y |X); There are times that a binary qualitative responses can be modeled using dummy variables approach. Example: \\[Y = \\left\\{ \\begin{array}{ll} 0 &amp; \\mbox{if stroke};\\\\ 1 &amp; \\mbox{if drug overdose}.\\end{array} \\right.\\] in such cases, the prediction of \\(\\hat{Y} &gt; 0.5\\), can be associated with . The main issue is partial estimates might be outside the [0, 1] probability interval, e.g. fig4-2: Figure 4.2: Classification using the Default data. Left: Estimated probability of default using linear regression. Some estimated probabilities are negative! The orange ticks indicate the 0/1 values coded for default(No or Yes). Right: Predicted probabilities of default using logistic regression. All probabilities lie between 0 and 1. "],["logistic-regression.html", "4.3 Logistic Regression", " 4.3 Logistic Regression 4.3.1 The Logistic Model Logistic regression: models the probability that Y belongs to a particular category (X) X is binary (0/1) \\[p(X) = β_0 + β_1X \\space \\Longrightarrow {Linear \\space regression}\\] \\[p (X) = \\frac{e^{\\beta_{0} + \\beta_{1}X}}{1 + e^{\\beta_{0} + \\beta_{1}X}} \\space \\Longrightarrow {Logistic \\space function}\\] \\[odds = \\frac{p (X)}{1 - p (X)} = e^{\\beta_{0} + \\beta_{1}X}} \\Longrightarrow {odds \\space value [0, ∞]}\\] By logging the whole equation, we get \\[\\log \\biggl(\\frac{p(X)}{1- p(X)}\\bigg) = \\beta_{0} + \\beta_{1}X \\Longrightarrow {log \\space odds/logit}\\] 4.3.2 Estimating the Regression Coefficient To estimate the regression coefficient, we use maximum likelihood (ME). Likelihood Function \\[ℓ (\\beta_{0}, \\beta_{1}) = \\prod_{i: y_{i}= 1} p (x_i) \\prod_{i&#39;: y_{i&#39;}= 0} (1- p (x_{i&#39;})) \\Longrightarrow {Likelihood \\space function}\\] The aim is to find beta values such that \\[ℓ\\] is maximum. The Least square method is the special case of maximum likelihood function. 4.3.3 Multiple Logistic Regression \\[\\log \\biggl(\\frac{p(X)}{1- p(X)}\\bigg) = \\beta_{0} + \\beta_{1}X_1 + ... + \\beta_{p}X_p \\\\ \\Downarrow \\\\ p(X) = \\frac{e^{\\beta_{0} + \\beta_{1}X_1 + ... + \\beta_{p}X_p}}{1 + \\beta_{0} + \\beta_{1}X_1 + ... + \\beta_{p}X_p}\\] Figure 4.3: Confounding in the Default data. Left: Default rates are shown for students (orange) and non-students (blue). The solid lines display default rate as a function of balance, while the horizontal broken lines display the overall default rates. Right: Boxplots of balance for students (orange) and non-students (blue) are shown. 4.3.4 Multinomial Logistic Regression This is used in the setting where K &gt; 2 classes. In multinomial, we select a single class to serve as the baseline. However, the interpretation of the coefficients in a multinomial logistic regression model must be done with care, since it is tied to the choice of baseline. Alternatively, you can use `Softmax coding, where we treat all K classes symmetrically, and assume that for k = 1, . . . ,K, rather than selecting a baseline. This means, we estimate coefficients for all K classes, rather than estimating coefficients for K − 1 classes. "],["generative-models-for-classification.html", "4.4 Generative Models for Classification", " 4.4 Generative Models for Classification Why Logistic Regression is not ideal? When there is substantial separation between the two classes, the parameter estimates for the logistic regression model are surprisingly unstable. If the distribution of the predictors X is approximately normal in each of the classes and the sample size is small, then the generative modelling may be more accurate than logistic regression. Generative modelling can be naturally extended to the case of more than two response classes. Common notations: - K \\(\\Longrightarrow\\) response class \\(π_k \\Longrightarrow\\) overall or prior probability that a randomly chosen observation comes from the prior kth class; can be obtained from the random sample from the population \\(f_k(X) ≡ Pr(X|Y = k)^1 \\Longrightarrow\\) the density function of X density for an observation that comes from the kth class; requires some underlying assumption to estimate Bayes’ theorem states that \\[Pr(Y = k|X = x) = \\frac {π_k f_k(x)}{\\sum_{l =1}^{k} π_lf_l(x)}\\] \\(p_k(x) = Pr(Y = k|X = x) \\Longrightarrow\\) posterior probability that an observation posterior X = x belongs to the kth class; computed from \\(f_k(X)\\) "],["a-comparison-of-classification-methods.html", "4.5 A Comparison of Classification Methods", " 4.5 A Comparison of Classification Methods Each of the classifiers below uses different estimates of \\(f_k(x)\\). linear discriminant analysis; quadratic discriminant analysis; naive Bayes 4.5.1 Linear Discriminant Analysis for p = 1 one predictor classify an observation to the class for which \\(p_k(x)\\) is greatest Assumptions: - we assume that \\(f_k(x)\\) is normal or Gaussian with a classs pecific mean and, - a shared variance term across all K classes [\\(σ^2_1 = · · · = σ^2_K\\) ] The normal density takes the form \\[f_k(x) = \\frac{1}{\\sqrt{2πσ_k}}exp(- \\frac{1}{2σ^2_k}(x- \\mu_k)^2)\\] Then, the posterior probability (probability that the observation belongs to the kth class, given the predictor value for that observation) is \\[p_k(x) = \\frac{π_k \\frac{1}{\\sqrt{2πσ}}exp(- \\frac{1}{2σ^2}(x- \\mu_k)^2)}{\\sum^k_{l=1} π_l \\frac{1}{\\sqrt{2πσ}}exp(- \\frac{1}{2σ^2}(x- \\mu_l)^2)}\\] Additional mathematical formula After you log and rearrange the above equation, you will the following formula. The Bayes’ classifier assign to one class if \\(2x (μ_1 − μ_2) &gt; μ_1^2 − μ_2^2\\) and otherwise. \\[δ_k(x) = x . \\frac{\\mu_k}{\\sigma^2} - \\frac{\\mu_k^2}{2\\sigma^2} + log(π_k) \\Longrightarrow {Equation \\space 4.18}\\] The Bayes decision boundary is the point for which \\(δ_1(x) = δ_2(x)\\) \\[x = \\frac{μ_1^2 − μ_2^2}{2(μ_1 − μ_2)} = \\frac{μ_1 + μ_2}{2}\\] Figure 4.4: Left: Two one-dimensional normal density functions are shown. The dashed vertical line represents the Bayes decision boundary. Right: 20 observations were drawn from each of the two classes, and are shown as histograms. The Bayes decision boundary is again shown as a dashed vertical line. The solid vertical line represents the LDA decision boundary estimated from the training data. The linear discriminant analysis (LDA) method approximates the linear discriminant analysis Bayes classifier by plugging estimates for \\(π_k\\), \\(μ_k\\), and \\(σ^2\\) into equation 4.18. \\(\\hat μ_k\\) is the average of all the training observations from the kth class \\[\\hat{\\mu}_{k} = \\frac{1}{n_{k}}\\sum_{i: y_{i}= k} x_{i}\\] \\(\\hat σ^2\\) is the weighted average of the sample variances for each of the K classes \\[\\hat{\\sigma}^2 = \\frac{1}{n - K} \\sum_{k = 1}^{K} \\sum_{i: y_{i}= k} (x_{i} - \\hat{\\mu}_{k})^2\\] Note. n = total number of training observations, \\(n_k\\) = number of training observations in the kth class \\(π_k\\) is estimated from the proportion of the training observations that belong to the kth class. \\[π_k = \\frac{n_k}{n}\\] LDA classifier assigns an observation X = x to the class for which \\(δ_k(x)\\) is largest. \\[δ_k(x) = x . \\frac{\\mu_k}{\\sigma^2} - \\frac{\\mu_k^2}{2\\sigma^2} + log(π_k) \\Longrightarrow {Equation \\space 4.18} \\\\ \\Downarrow \\\\ \\hat δ_k(x) = x \\cdot \\frac{\\hat \\mu_k}{\\hat \\sigma^2} - \\frac{\\hat \\mu_k^2}{2\\hat \\sigma^2} + log(\\hat π_k)\\] 4.5.2 Linear Discriminant Analysis for p &gt; 1 multiple predictors; p &gt; 1 predictors observations come from a multivariate Gaussian (or multivariate normal) distribution, with a class-specific mean vector and a common covariance matrix; \\[N(μ_k,Σ)\\] Assumptions: - each individual predictor follows a one-dimensional normal distribution, with predictors having some correlation Figure 4.5: Two multivariate Gaussian density functions are shown, with p = 2. Left: The two predictors are uncorrelated and it has a circular base. Var(X_1) = Var(X_2) and Cor(X_1,X_2) = 0; Right: The two variables have a correlation of 0.7 with a elliptical base \\(\\exp\\) The multivariate Gaussian density is defined as: \\[f(x) = \\frac{1}{(2π)^{\\frac{p}{2}}|Σ|^{\\frac{1}{2}}}\\exp -\\frac{1}{2}(x - \\mu)^T Σ^{−1}(x − μ))\\] Bayes classifier assigns an observation X = x to the class for which \\[δ_k(x)\\] is largest. \\[δ_k(x) = x^T Σ^{−1}μ_k - \\frac{1}{2}μ_k^T Σ^{−1} μ_k + log π_k \\Longrightarrow vector/matrix \\space version \\\\ δ_k(x) = x . \\frac{\\mu_k}{\\sigma^2} - \\frac{\\mu_k^2}{2\\sigma^2} + log(π_k) \\Longrightarrow {Equation \\space 4.18}\\] Figure 4.6: An example with three classes. The observations from each class are drawn from a multivariate Gaussian distribution with p = 2, with a class-specific mean vector and a common covariance matrix. Left: Ellipses that contain 95% of the probability for each of the three classes are shown. The dashed lines are the Bayes decision boundaries. Right: 20 observations were generated from each class, and the corresponding LDA decision boundaries are indicated using solid black lines. The Bayes decision boundaries are once again shown as dashed lines. Overall, the LDA decision boundaries are pretty close to the Bayes decision boundaries, shown again as dashed lines. The test error rates for the Bayes and LDA classifiers are 0.0746 and 0.0770, respectively. All classification models have training error rate, which can be displayed with a confusion matrix. Caveats of error rate: training error rates will usually be lower than test error rates, which are the real quantity of interest. The higher the ratio of parameters p to number of samples n, the more we expect this overfitting to play a role. the trivial null classifier will achieve an error rate that is only a bit higher than the LDA training set error rate a binary classifier such as this one can make two types of errors (Type I and II) Class-specific performance (sensitivity and specificity) is important in certain fields (e.g., medicine) LDA has low sensitivity due to 1. LDA is trying to approximate the Bayes classifier, which has the lowest total error rate out of all classifiers 2. In the process, the Bayes classifier will yield the smallest possible total number of misclassified observations, regardless of the class from which the errors stem. 3. It also uses a threshold of 50% for the posterior probability of default in order to assign an observation to the default class \\[Pr(default = Yes|X = x) &gt; 0.5. \\\\ Pr(default = Yes|X = x) &gt; 0.2.\\] Figure 4.7: The figure illustrates the trade-off that results from modifying the threshold value for the posterior probability of default. For the Default data set, error rates are shown as a function of the threshold value for the posterior probability that is used to perform the assignment. The black solid line displays the overall error rate. The blue dashed line represents the fraction of defaulting customers that are incorrectly classified, and the orange dotted line indicates the fraction of errors among the non-defaulting customers. As the threshold is reduced, the error rate among individuals who default decreases steadily, but the error rate among the individuals who do not default increases. The decision on the threshold must be based on domain knowledge (e.g., detailed information about the costs associated with default) ROC curve is a way to illustrate the two type of errors at all possible thresholds. Figure 4.8: The true positive rate is the sensitivity: the fraction of defaulters that are correctly identified, using a given threshold value. The false positive rate is 1-specificity: the fraction of non-defaulters that we classify incorrectly as defaulters, using that same threshold value. The ideal ROC curve hugs the top left corner, indicating a high true positive rate and a low false positive rate. The dotted line represents the “no information” classifier; this is what we would expect if student status and credit card balance are not associated with probability of default. An ideal ROC curve will hug the top left corner, so the larger area under the ROC curve (AUC), the better the classifier. True class Neg. or Null Pos. or Non-null Total Predicted class    − or Null True Neg. (TN) False Neg. (FN) N∗    + or Non-null False Pos. (FP) True Pos. (TP) P∗   Total N P Important measures for classification and diagnostic testing: False Positive rate (FP/N) \\(\\Longrightarrow\\) Type I error, 1−Specificity True Positive rate (TP/P) \\(\\Longrightarrow\\) 1−Type II error, power, sensitivity, recall Pos. Predicted value (TP/P∗) \\(\\Longrightarrow\\) Precision, 1−false discovery proportion Neg. Predicted value (TN/N∗) 4.5.3 Quadratic Discriminant Analysis (QDA) Assumptions similar to LDA, in which observations from each class are drawn from a Gaussian distribution, and plugging estimates for the parameters into Bayes’ theorem in order to perform prediction QDA assumes that each class has its own covariance matrix \\[X ∼ N(μ_k,Σ_k) \\Longrightarrow {Σ_k is \\space covariance \\space matrix \\space for \\space the \\space kth \\space class}\\] Bayes classifier \\[δ_k(x) = - \\frac{1}{2}(x - \\mu_k)^T Σ_k^{−1}(x - \\mu_k) - \\frac{1}{2}log|Σ_k| + log(π_k) \\\\ \\Downarrow \\\\ δ_k(x) = - \\frac{1}{2}x^T Σ_k^{−1}x - x^T Σ_k^{−1} \\mu_k - \\frac{1}{2}μ_k^T Σ_k^{−1} μ_k - \\frac{1}{2}log|Σ_k| + log π_k\\] QDA classifier involves plugging estimates for \\(Σ_k\\), \\(μ_k\\), and \\(π_k\\) into the above equation, and then assigning an observation X = x to the class for which this quantity is largest. The quantity x appears as a quadratic function, hence the name. Why the LDA to QDA is preferred or vice-versa? 1. Bias-variance trade-off - Pro LDA: LDA assumes that the K classes share a common covariance matrix and the quantity X becomes linear, which means there are \\(K_p\\) linear coefficients to estimate.LDA is a much less flexible classifier than QDA, and so has substantially lower variance; improved prediction performance. Con LDA: If the assumption K classes share a common covariance matrix is badly off, LDA can suffer from high bias Conclusion: Use LDA when there is a few training observations; use QDA when the training set is very large or common covariance matrix is untennable. Figure 4.9: Left: The Bayes (purple dashed), LDA (black dotted), and QDA (green solid) decision boundaries for a two-class problem with Σ1 = Σ2. The shading indicates the QDA decision rule. Since the Bayes decision boundary is linear, it is more accurately approximated by LDA than by QDA. Right: Details are as given in the left-hand panel, except that Σ1 ̸= Σ2. Since the Bayes decision boundary is non-linear, it is more accurately approximated by QDA than by LDA. 4.5.4 Naive Bayes Estimating a p-dimensional density function is challenging; naive bayes make a different assumption than LDA and QDA. an alternative to LDA that does not assume normally distributed predictors \\[f_k(x) = f_{k1}(x_1) × f_{k2}(x_2)×· · ·×f{k_p}(x_p),\\] where \\(f_{kj}\\) is the density function of the jth predictor among observations in the kth class Within the kth class, the p predictors are independent. Why naive Bayes is better/powerful? By assuming that the p covariates are independent within each class, we assumed that there is no association between the predictors! When estimating a p-dimensional density function, it is difficult to calculate the marginal distribution of each predictor and joint distribution of the predictors. Although p covariates might not be independent within each class, it is convenient and we obtain pretty decent results when the n is small, p is large. It reduces variance, though it has some bias (Bias-variance trade-off) Options to estimate the one-dimensional density function fkj using training data [For Quantitative \\(X_j\\)] -&gt; We assume \\(X_j |Y = k ∼ N(μ_{jk},σ_{jk}^2)\\), where within each class, the jth predictor is drawn from a (univariate) normal distribution. It is QDA-like with diagonal class-specific covariance matrix [For Quantitative \\(X_j\\)] -&gt; Use a non-parametric estimate for \\(f_{kj}\\). First, a histogram for the within-class observations and then estimate \\(f_{kj}(x_j)\\). Or else, use kernel density estimator. [For Qualitative \\(X_j\\)] -&gt;Count the proportion of training observations for the jth predictor corresponding to each class. Note: Fixing the threshold, the Naive Bayes has a higher error rate than LDA, but better prediction (higher sensitivity). "],["summary-of-the-classification-methods.html", "4.6 Summary of the classification methods", " 4.6 Summary of the classification methods 4.6.1 An Analytical Comparison LDA and logistic regression assume that the log odds of the posterior probabilities is linear in x. QDA assumes that the log odds of the posterior probabilities is quadratic in x. LDA is simply a restricted version of QDA with \\(Σ_1 = · · · = Σ_K = Σ\\) LDA is a special case of naive Bayes and vice-versa! LDA assumes that the features are normally distributed with a common within-class covariance matrix, and naive Bayes instead assumes independence of the features. Naive Bayes can produce a more flexible fit. QDA might be more accurate in settings where interactions among the predictors are important in discriminating between classes. LDA &gt; logistic regression when the observations at each Kth class is normal. K-nearest neighbors (KNN) will be better classifiers when decision boudary is non-linear, n is large, and p is small. KNN has low bias but large variance; as such, KNN requires a lot of observations relative to the number of predictors. If decision boundary is non-linear but n is and p are small, then QDA may be preferred to KNN. KNN does not tell us which predictors are important! Final note. The choice of method depends on (1) the true distribution of the predictors in each of the K classes,(2) the values of n and p - bias-variance trade-off 4.6.2 An Empirical Comparison Figure 4.10: Boxplots of the test error rates for each of the linear scenarios described in the main text. When Bayes decision boundary is linear, Scenario 1: Binary class response, equal observations in each class, uncorrelated predictors Scenario 2: Similar to Scenario 1, but the predictors had a correlation of −0.5. Scenario 3: Predictors had a negative correlation, t-distribution (more extreme points at the tails) Figure 4.11: Boxplots of the test error rates for each of the non-linear scenarios described in the main text When Bayes decision boundary is non-linear, Scenario 4: normal distiibution, correlation of 0.5 between the predictors in the first class, and correlation of −0.5 between the predictors in the second class. Scenario 5: Normal distribution, uncorrelated predictors Scenario 6: Normal distribution, different diagonal covariance matrix for each class, small n "],["generalized-linear-models.html", "4.7 Generalized Linear Models", " 4.7 Generalized Linear Models Count data (e.g. number of bikers per hour) is neither quantitative nor qualitative =&gt; neither linear regression nor the classification approaches considered so far are applicable. "],["linear-regression-with-count-data---negative-values.html", "4.8 Linear regression with count data - negative values", " 4.8 Linear regression with count data - negative values The results of fitting a least squares regression model to the Bikeshare data provides some reasonable results: as weather progressively worsens, the number of bikers decreases (coefficients become negative wrt baseline) the coefficients associated with season and time of day match expected patterns (lowest in winter, and highest during peak commute times) Figure 4.12: Results for a least squares linear model fit to predict bikers in the Bikeshare data. For the qualitative variable weathersit, the baseline level corresponds to clear skies. Figure 4.13: A least squares linear regression model was fit to predict bikers in the Bikeshare data set. Left: The coefficients associated with the month of the year. Bike usage is highest in the spring and fall, and lowest in the winter. Right: The coefficients associated with the hour of the day. Bike usage is highest during peak commute times, and lowest overnight. Problem 1: model predicts negative numbers of bikers at times "],["linear-regression-with-count-data---heteroscedasticity.html", "4.9 Linear regression with count data - heteroscedasticity", " 4.9 Linear regression with count data - heteroscedasticity In this example, the variance of biker numbers changes as the mean number changes: during worse conditions, there are few bikers, and little variation in the number of bikers during better conditions, there are many bikers on average, but also larger variation in the number of bikers Figure 4.14: Left: On the Bikeshare dataset, the number of bikers is displayed on the y-axis, and the hour of the day is displayed on the x-axis. For the most part, as the mean number of bikers increases, so does the variance in the number of bikers. A smoothing spline fit is shown in green. Right: The log of the number of bikers is displayed on the y-axis. Problem 2: observed heteroscedasticity is a violation of linear model assumptions \\[Y = \\beta_{0} + \\sum_{j=1}^p \\beta_{j} + \\epsilon\\] where \\(\\epsilon\\) is a mean-zero error term with a constant variance Transforming to log improves the variance, but cannot be used where the response can take on a 0 value. Log transformation also results in challenges in interpretation: e.g. “a one-unit increase in \\(X_j\\) is associated with an increase in the mean of the log of \\(Y\\) by an amount \\(β_j\\)” "],["problems-with-linear-regression-of-count-data.html", "4.10 Problems with linear regression of count data", " 4.10 Problems with linear regression of count data Problem 1: model predicts negative numbers of bikers at times Problem 2: observed heteroscedasticity is a violation of linear model assumptions Problem 3: integer values (bikers) predicted using a continuous response \\(Y\\) “[A] Poisson regression model provides a much more natural and elegant approach for this task.” "],["poisson-distribution.html", "4.11 Poisson distribution", " 4.11 Poisson distribution A count response variable \\(Y\\) (which takes on non-negative integer values) can be modeled using the Poisson distribution, where the probability that \\(Y\\) takes on a given count value \\(k\\) can be calculated as: \\(Pr(Y = k) = \\frac{e^{-\\lambda}\\lambda^k}{k!}\\) for \\(k\\) = 0, 1, 2, … where \\(\\lambda\\) represents both the expected value (mean) and variance of \\(Y\\): \\(Y = E(Y) = Var(Y)\\) =&gt; “[I]f \\(Y\\) follows the Poisson distribution, then the larger the mean of \\(Y\\), the larger its variance.” par(mfrow = c(2,2)) lambda &lt;- c(1:4) k &lt;- c(0:10) for (lam in lambda) { Prk &lt;- (exp(-lam)*lam^k)/factorial(k) plot(k, Prk, type = &#39;b&#39;, ylim = c(0, 0.4), main = paste(&quot;lambda =&quot;, lam)) } Figure 4.15: Plots of Poisson Distributions with different lambda values, showing how variance increases with increasing lambda. Note all values are non-negative integer values, suitable for modelling counts, k. "],["poisson-regression-model-mean-lambda.html", "4.12 Poisson Regression Model mean (lambda)", " 4.12 Poisson Regression Model mean (lambda) “[R]ather than modeling [a count response variable], \\(Y\\), as a Poisson distribution with a fixed mean value like \\(\\lambda\\) = 5, we would like to allow the mean to vary as a function of the covariates.” The mean \\(\\lambda\\) can be modeled as a function of the predictor variables as follows: \\(log(\\lambda(X_1, ..., X_p) = \\beta_0 + \\beta_1X_1 + ... + \\beta_pX_p\\) NB: taking the log ensures that \\(\\lambda\\) can only be non-negative. This is equivalent to representing the mean \\(\\lambda\\) as follows: \\(\\lambda = \\text{E}(Y) = \\lambda(X_1, ..., X_p) = e^{\\beta_0 + \\beta_1X_1 + ... + \\beta_pX_p}\\) "],["estimating-the-poisson-regression-parameters.html", "4.13 Estimating the Poisson Regression parameters", " 4.13 Estimating the Poisson Regression parameters The calculation of \\(\\lambda\\) can then be used in the formula of the Poisson Distribution, allowing the Maximum Likelihood approach to be used in estimating the parameters, \\(\\beta_0\\), \\(\\beta_1\\),…, \\(\\beta_p\\): Poisson Distribution Formula: \\(Pr(Y = k) = \\frac{e^{-\\lambda}\\lambda^k}{k!}\\) for \\(k\\) = 0, 1, 2, … Maximum likelihood: \\(l(\\beta_0, \\beta_1, ..., \\beta_p) = \\Pi_{i=1}^n\\frac{e^{-\\lambda(x_i)}\\lambda(x_i)^{y_i}}{y_i!}\\) where \\(\\lambda(x_i) = e^{\\beta_0 + \\beta_1x_{i1} + ... + \\beta_px_{ip}}\\) Coefficients that maximize the likelihood \\(l(\\beta_0, \\beta_1, ..., \\beta_p)\\) (make the observed data as likely as possible) are chosen. "],["interpreting-poisson-regression.html", "4.14 Interpreting Poisson Regression", " 4.14 Interpreting Poisson Regression An increase in \\(X_j\\) by one unit is associated with a change in \\(E(Y) = \\lambda\\) by a factor of \\(exp(\\beta_j)\\) Figure 4.16: Results for Poisson regression model fit to predict bikers in the Bikeshare data. For the qualitative variable weathersit, the baseline level corresponds to clear skies. A change in weather from clear to cloudy skies is associated with a change in mean bike usage by a factor of exp(-0.08) = 0.923 i.e. on average, only 92.3% as many people will use bikes compared to when it is clear (baseline weather). "],["advantages-of-poisson-regression.html", "4.15 Advantages of Poisson Regression", " 4.15 Advantages of Poisson Regression Poisson regression has several advantages in modeling count data: Mean-variance relationship We implicitly assume that mean bike usage in a given hour equals the variance of bike usage during that hour (cf use constant variance in linear regression). Non-negative fitted values There are no negative predictions using the Poisson regression model. "],["generalized-linear-models-1.html", "4.16 Generalized Linear Models", " 4.16 Generalized Linear Models Generalized linear models (GLMs) all follow the same ‘recipe’: use a set of predictors \\(X_1\\), …, \\(X_p\\) to predict a response \\(Y\\) model the response \\(Y\\) as coming from a particular distribution e.g. Poisson Distribution, for Poisson regression transform the mean of the response (via a link function \\(\\eta\\)) so that the transformed mean is a linear function of the predictors e.g. for Poisson regression, \\(log(\\lambda(X_1, ..., X_p) = \\beta_0 + \\beta_1X_1 + ... + \\beta_pX_p\\) "],["addendum---logistic-regression-assumptions.html", "4.17 Addendum - Logistic Regression Assumptions", " 4.17 Addendum - Logistic Regression Assumptions library(dplyr) library(titanic) library(car) ## Loading required package: carData Source: The 6 Assumptions of Logistic Regression (With Examples) Source: Assumptions of Logistic Regression, Clearly Explained Logistic regression is a method to fit a regression model usually when the response variable is binary. 4.17.0.1 Assumption #1 - The response variable is binary Examples: Yes or No Male or Female Pass or Fail For more tha two possible outcomes, an ordinal regression is the model of choice. 4.17.0.2 Assumption #2 - Observations are independent As the OLS regression, the logistic regression requires that observations are iid (independent and identical distributed). Easiest way is to create a plot of residuals against time (i.e. order of observations) and observe if the pattern is random or not. 4.17.0.3 Assumption #3 - No multicollinearity among predictors Multicollinearity occurs when two or more explanatory variables are highly correlated to each other, such that they do not provide unique or independent information in the regression model. If the degree of correlation is high enough between variables, it can cause problems when fitting and interpreting the model. Use VIF to check multicollinearity (&gt; 10 show strong collinearity among predcitors) 4.17.0.4 Assumption #4 - No extreme outliers Logistic regression assumes that there are no extreme outliers or influential observations in the dataset. Use Cook's distance for each observation. 4.17.0.5 Assumption #5 - There is a Linear Relationship Between Explanatory Variables and the Logit of the Response Variable Logistic regression assumes that there exists a linear relationship between each explanatory variable and the logit of the response variable. Recall that the logit is defined as: Logit(p) = log(p / (1-p)) where p is the probability of a positive outcome. Use Box-Tidwell test to check this assumption. Example: titanic &lt;- titanic_train %&gt;% select(Survived, Age, Fare) %&gt;% na.omit() %&gt;% janitor::clean_names() glimpse(titanic) ## Rows: 714 ## Columns: 3 ## $ survived &lt;int&gt; 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1… ## $ age &lt;dbl&gt; 22, 38, 26, 35, 35, 54, 2, 27, 14, 4, 58, 20, 39, 14, 55, 2, … ## $ fare &lt;dbl&gt; 7.2500, 71.2833, 7.9250, 53.1000, 8.0500, 51.8625, 21.0750, 1… Build the model # survived (target ~ age + fare) log_reg &lt;- glm(survived ~ age + fare, data = titanic, family = binomial(link = &quot;logit&quot;)) summary(log_reg) ## ## Call: ## glm(formula = survived ~ age + fare, family = binomial(link = &quot;logit&quot;), ## data = titanic) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.7605 -0.9232 -0.8214 1.2362 1.7820 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.417055 0.185976 -2.243 0.02493 * ## age -0.017578 0.005666 -3.103 0.00192 ** ## fare 0.017258 0.002617 6.596 4.23e-11 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 964.52 on 713 degrees of freedom ## Residual deviance: 891.34 on 711 degrees of freedom ## AIC: 897.34 ## ## Number of Fisher Scoring iterations: 5 Box_Tidwell test titanic &lt;- titanic %&gt;% mutate(age_1 = age + 1, fare_1 = fare + 1) boxTidwell(survived ~ age_1 + fare_1, data = titanic) ## MLE of lambda Score Statistic (z) Pr(&gt;|z|) ## age_1 -0.48631 1.3237 0.1856 ## fare_1 0.10111 -5.5619 2.668e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## iterations = 10 4.17.0.6 Assumption #6 - Sample size must be sufficiently large Logistic regression assumes that the sample size of the dataset if large enough to draw valid conclusions from the fitted logistic regression model. As a rule of thumb, you should have a minimum of 10 cases with the least frequent outcome for each explanatory variable. For example, if you have 3 explanatory variables and the expected probability of the least frequent outcome is 0.20, then you should have a sample size of at least (10*3) / 0.20 = 150. "],["lab-classification-methods.html", "4.18 Lab: Classification Methods", " 4.18 Lab: Classification Methods "],["meeting-videos-3.html", "4.19 Meeting Videos", " 4.19 Meeting Videos 4.19.1 Cohort 1 Meeting chat log 00:08:27 Kim M: 👋😁 00:09:40 SriRam: Yay, summer time in SA 00:12:51 SriRam: lol 00:13:42 SriRam: Can someone help me with zoom, my Audio doesn’t work. I tried many microphones 🙁, all kinds of settings 00:14:01 SriRam: Any expert advice? 00:14:05 Raymond Balise: Mac or Windows? 00:14:10 SriRam: Mac 00:14:16 Raymond Balise: I supper common 00:14:27 Raymond Balise: there is an up arrow next to the mic 00:14:28 SriRam: 🙁 00:14:30 Raymond Balise: pick the mic there 00:14:40 Raymond Balise: Zoom usually guesses worng 00:14:53 SriRam: It says “same as system” 00:15:24 August: document html knit open that 00:15:24 Ryan Metcalf: Ctrl + Shift + B for building. Or Knit the single file. 00:15:29 Kim M: No other option? I see &#39;same as system&#39; but the other option (&#39;Microphone Array...&#39;) is selected 00:15:36 Raymond Balise: are you using an external mic 00:15:55 SriRam: Yes &lt; I tried wired and wireless 00:36:18 Jon Harmon (jonthegeek): FYI: No newline between $$ and the equation to get it to render properly in HTML output. So: $$Pr(Y = k|X = x) … {(x)}$$ works (even if it&#39;s multi-line in-between) 01:08:09 Raymond Balise: Lovely work. I need to get to another meeting. 01:12:36 Ryan Metcalf: Great job Mei Ling! 01:13:04 Kim M: Me. But I&#39;m not sure I&#39;ll be able to get through. 01:13:14 Kim M: I&#39;ll try. 01:13:48 Laura Rose: sounds good! 01:14:05 Kim M: Ciao Meeting chat log 00:22:00 Wayne Defreitas: YES tidymodels 00:31:06 August: cs = cubic spline 00:32:57 August: some additional stuff about splines in r 00:32:58 August: https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/s12874-019-0666-3 00:42:58 Wayne Defreitas: #can create custom function [metric_set()] to specify which model metrics we want. #available functions: accuracy(), kap(), sens(),spec(), ppv(), npv(), mcc(), j_index(), bal_accuracy, detection_prevalence(), precision(), recall(), f_meas() custom_metrics &lt;- metric_set (accuracy, sens, spec) 00:44:24 Wayne Defreitas: custom_metrics(leads_results, truth=purchased, estimate=.pred_class) 00:44:43 August: https://yardstick.tidymodels.org/reference/metric_set.html 01:02:54 Wayne Defreitas: Running to another meeting…thanks everyone Meeting chat log 00:20:03 Raymond Balise: nicely done 00:21:33 Mei Ling Soh: https://www.theanalysisfactor.com/count-data-considered-continuous/#:~:text=The%20issue%20with%20count%20variables,model%2C%20which%20require%20continuous%20data.&amp;text=Treating%20that%20count%20variable%20as,in%20your%20particular%20data%20set. 00:30:04 SriRam: Just realised that GLM is not in the first edition (the book I have) :( 00:30:54 August: https://www.statlearning.com/ 00:31:05 August: link at the bottom of page for 2nd edition 00:31:24 SriRam: Thank you August &amp; Mei 00:32:25 August: 👍 anytime 00:42:27 Federica Gazzelloni: Thanks Raymond!! 4.19.2 Cohort 2 Meeting chat log 00:24:30 Ricardo Serrano: https://github.com/rserran/melbourne_housing_kaggle 00:24:39 Federica Gazzelloni: thanks 00:59:01 Jim Gruman: tidy(x, conf.int = FALSE, conf.level = 0.95, exponentiate = FALSE, ...) the exponentiate = TRUE will back out of the log to give just the odds 00:59:04 Anna-Leigh Brown: https://www.wolframalpha.com/input/?i=logistic+function 01:06:46 Jim Gruman: I will always question the glm breakpoint of 0.5, A good discussion for offline of a means of adjustment here: https://towardsdatascience.com/bank-customer-churn-with-tidymodels-part-2-decision-threshold-analysis-c658845ef1f 01:07:22 Jim Gruman: thank you Michael ! 01:09:39 jlsmith3: Thank you, Michael! 01:09:44 Ricardo Serrano: Thanks, Michael! Meeting chat log 00:10:56 Ricardo: https://www.polyu.edu.hk/cbs/sjpolit/logisticregression.html 00:14:57 Federica Gazzelloni: https://www.coursera.org/specializations/biostatistics-public-health Dr. John McGready, PhD, MS 01:10:26 Jim Gruman: thank you Michael! don&#39;t eat the mushrooms🍄 01:10:50 jlsmith3: Thank you, Michael!! 01:11:18 Ricardo: Thanks, Michael 👍 4.19.3 Cohort 3 Meeting chat log 00:43:07 Soroush, Fariborz: https://www.youtube.com/watch?v=HZGCoVF3YvM 00:43:12 Soroush, Fariborz: https://www.youtube.com/watch?v=lG4VkPoG3ko 00:43:20 Soroush, Fariborz: https://www.youtube.com/watch?v=o2Tpws5C2Eg Meeting chat log 00:05:38 Jeremy Selva: 10pm 00:44:38 Rose Hartman: This has been super helpful for me, at least! Thank you 🙂 01:03:10 Mei Ling Soh: We will need to end soon. 01:03:22 Mei Ling Soh: I can present the lab next week 01:03:31 Mei Ling Soh: I have prepared the slides up to KNN 01:05:10 Rose Hartman: Thank you so much for this explanation today! Very helpful. 4.19.4 Cohort 4 Meeting chat log ADD LOG HERE "],["resampling-methods.html", "Chapter 5 Resampling Methods", " Chapter 5 Resampling Methods Learning objectives: Use a validation set to estimate the test error of a predictive model. Use leave-one-out cross-validation to estimate the test error of a predictive model. Use K-fold cross-validation to estimate the test error of a predictive model. Use the bootstrap to estimate the test error of a predictive model. Describe the advantages and disadvantages of the various methods for estimating model test error. "],["validation-set-approach.html", "5.1 Validation Set Approach", " 5.1 Validation Set Approach This involves randomly splitting the data into a training set and validation set. Note that in certain applications, such as time series analysis, it is not feasible to randomly split the data. The advantage of the validation set approach is that it is conceptually simple to understand and implement. However, the validation error rate is variable depending on the assignment of the training and validation sets. Additionally, we are giving up valuable data points by not using all of the data to estimate the model. Thus the validation error rate will tend to overestimate the test error rate. "],["validation-error-rate-varies-depending-on-data-set.html", "5.2 Validation Error Rate Varies Depending on Data Set", " 5.2 Validation Error Rate Varies Depending on Data Set Figure 5.1: Left: The validation set approach was used to estimated the test mean squared error from predicting mpg as a polynomial function of horsepower. Right: The estimated test error varies depending on the validation and training sets used. "],["leave-one-out-cross-validation-loocv.html", "5.3 Leave-One-Out Cross-Validation (LOOCV)", " 5.3 Leave-One-Out Cross-Validation (LOOCV) LOOCV aims to address some of the drawbacks of the validation set approach. Similar to validation set approach, LOOCV involves splitting the data into a training set and validation set. However, the validation set includes one observation, and the training set includes \\(n-1\\) observations. This process is repeated for all observations such that \\(n\\) models are estimated. Having a large training set avoids the problems from not using all (or almost all) of the data in estimating the model. Conversely, the validation error for a given model is highly variable since it consists of one observation, although it is unbiased. LOOCV estimate of test error is averaged over the \\(n\\) models: \\[CV_{n} = \\frac{1}{n}{\\sum_{i=1}^{n}}(y_{i}-\\hat{y_{i}})^2\\] "],["advantages-of-loocv-over-validation-set-approach.html", "5.4 Advantages of LOOCV over Validation Set Approach", " 5.4 Advantages of LOOCV over Validation Set Approach There are several advantages to LOOCV over validation set approach. It has less bias since models are repeatedly fitted on slightly different data sets, so it tends to not overestimate the test error as much as the validation set approach. The estimated test error will always be the same when LOOCV is performed on the entire data set. The major disadvantage to LOOCV is that it is computationally expensive. An easy shortcut for estimating the LOOCV test error for linear or polynomial regression models from a single model is as follows. \\[CV_{n} = \\frac{1}{n}{\\sum_{i=1}^{n}}\\left(\\frac{y_{i} - \\hat{y_{i}}}{1 - h_{i}}\\right)^2\\] where \\(h_{i}\\) is the leverage for a given residual as defined in equation 3.37 in the book for a simple linear regression. Its value falls between 1 and \\(1/n\\), so that observations whose residual has high leverage will contribute relatively more to the CV statistic. In general, LOOCV can be used for various kinds of models, including logistic regression, LDA, and QDA. "],["k-fold-cross-validation.html", "5.5 k-fold Cross-Validation", " 5.5 k-fold Cross-Validation This is an alternative to LOOCV which involves dividing the data set into \\(k\\) groups (folds) of approximately equal size. The percent of the data set that is in the validation set can be thought of as \\(1/k\\). E.g., for \\(k=5\\) groups, 20% of the data would be withheld for testing. "],["graphical-illustration-of-k-fold-approach.html", "5.6 Graphical Illustration of k-fold Approach", " 5.6 Graphical Illustration of k-fold Approach Figure 5.2: The data set is split five times such that all observations are included in one validation set. The model is estimated on 80% of the data five different times, the predictions are made for the remaining 20%, and the test MSEs are averaged. Thus, LOOCV is a special case of k-fold cross-validation, where \\(k=n\\). The equation for the CV statistic is below: \\[CV_{k} = \\frac{1}{k}{\\sum_{i=1}^{k}}MSE_{i}\\] "],["advantages-of-k-fold-cross-validation-over-loocv.html", "5.7 Advantages of k-fold Cross-Validation over LOOCV", " 5.7 Advantages of k-fold Cross-Validation over LOOCV The main advantage of k-fold over LOOCV is computational. However, there are other advantages related to the bias-variance tradeoff. The figure below shows the true test error for the simulated data sets from Chapter 2 compared to the LOOCV error and k-fold cross-validation error. Figure 5.3: The estimated test errors for LOOCV and k-fold cross validation is compared to the true test error for the three simulated data sets from Chapter 2. True test error is shown in blue, LOOCV error is the dashed black line, and 10-fold error is shown in orange. "],["bias-variance-tradeoff-and-k-fold-cross-validation.html", "5.8 Bias-Variance Tradeoff and k-fold Cross-Validation", " 5.8 Bias-Variance Tradeoff and k-fold Cross-Validation As mentioned previously, the validation approach tends to overestimate the true test error, but there is low variance in the estimate since we just have one estimate of the test error. Conversely, the LOOCV method has little bias, since almost all observations are used to create the models. Since the mean of the highly correlated \\(n\\) models has a higher variance, LOOCV mean estimated error has a higher variance. The k-fold method (\\(k&lt;n\\)) suffers from intermediate bias and variance levels. For this reason, \\(k=5\\) or \\(k=10\\) is often used in modeling since it has been demonstrated to yield results that do not have either too much bias or variance. "],["cross-validation-on-classification-problems.html", "5.9 Cross-Validation on Classification Problems", " 5.9 Cross-Validation on Classification Problems Previous examples have focused on measuring cross-validated test error where \\(Y\\) is quantitative. We can also use cross validation for classification problems, using the equation below, which is an example of LOOCV. \\(Err_{i} = I(Y_{i}\\neq\\hat{Y}_{i})\\) \\[CV_{n} = \\frac{1}{n}{\\sum_{i=1}^{n}}Err_{i}\\] "],["logistic-polynomial-regression-bayes-decision-boundaries-and-k-fold-cross-validation.html", "5.10 Logistic Polynomial Regression, Bayes Decision Boundaries, and k-fold Cross Validation", " 5.10 Logistic Polynomial Regression, Bayes Decision Boundaries, and k-fold Cross Validation Figure 5.4: Estimated decision boundaries of polynomial logistic regression models for simulated data are shown. The Bayes decision boundary is the dashed purple line. In practice, the true test error and Bayes error rate are unknown, so we need to estimate the test error rate. This can be done via k-fold cross-validation, which is often a good estimate of the true test error rate. Figure 5.5: The test error rate (beige), the training error rate (blue), and the 10-fold cross validation error rate (black) are shown for polynomial logistic regression and KNN classification. "],["the-bootstrap.html", "5.11 The Bootstrap", " 5.11 The Bootstrap The bootstrap can be used in a wide variety of modeling frameworks to estimate the uncertainty associated with a given estimator. For example, the bootstrap is useful to estimate the standard errors of a coefficient. The bootstrap involves repeated sampling with replacement from the original data set to form a distribution of the statistic in question. "],["population-distribution-compared-to-bootstrap-distribution.html", "5.12 Population Distribution Compared to Bootstrap Distribution", " 5.12 Population Distribution Compared to Bootstrap Distribution Figure 5.6: The distribution of the mean of alpha is shown on the left, with 1000 samples generated from the true population. A bootstrap distribution is shown in the middle, with 1000 samples taken from the original sample. Note that both confidence intervals contain the true alpha (pink line) in the right panel, and that the spread of both distributions is similar. "],["bootstrap-standard-error.html", "5.13 Bootstrap Standard Error", " 5.13 Bootstrap Standard Error The bootstrap standard error formula is somewhat more complex. The equation below gives the standard error for the \\(\\hat{\\alpha}\\) example in the book. The bootstrap standard error functions as an estimate of the standard error of \\(\\hat{\\alpha}\\) estimated from the original data set. \\[SE_{B(\\hat{\\alpha})} = \\sqrt{\\frac{1}{B-1}\\sum_{r=1}^{B}\\left(\\hat{\\alpha}^{*r} - \\frac{1}{B}\\sum_{j=1}^{B}\\hat{\\alpha}^{*j}\\right)^2}\\] "],["lab-cross-validation-and-the-bootstrap.html", "5.14 Lab: Cross-Validation and the Bootstrap", " 5.14 Lab: Cross-Validation and the Bootstrap Resampling techniques 5.14.1 The Validation Set Approach to estimate the test error rates library(ISLR2) library(tidyverse) #library(tidymodels) #tidymodels_prefer() library(patchwork) data(&quot;Auto&quot;);dim(Auto) ## [1] 392 9 set.seed(1) train1 &lt;- sample(392,196) # 392/2 = 196 split the set in two groups Check the trend of the data ggplot(data=Auto, aes(x = horsepower, y = mpg))+ geom_point() + geom_smooth() ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; The estimated test MSE for the linear regression fit is 23.27 lm.fit1 &lt;- lm(mpg ~ horsepower, data = Auto, subset = train1) # calculate MSE mse &lt;- cbind(Auto,pred=predict(lm.fit1, Auto))%&gt;% select(mpg,pred)%&gt;% slice(-train1)%&gt;% mutate(res=(mpg-pred)^2)%&gt;% summarize(mse=mean(res)) poly() function to estimate the test error for the quadratic and cubic regressions lm.fit2 &lt;- lm(mpg ~ poly(horsepower,2),data=Auto,subset=train1) mean((Auto$mpg - predict(lm.fit2, Auto))[-train1]^2) ## [1] 18.71646 lm.fit3 &lt;- lm(mpg ~ poly(horsepower,3),data=Auto,subset=train1) mean((Auto$mpg - predict(lm.fit3, Auto))[-train1]^2) ## [1] 18.79401 mse_df &lt;- data.frame(sample= &quot;seed_1&quot;, poly=1:3, mse=c(mean((Auto$mpg - predict(lm.fit1, Auto))[-train1]^2), mean((Auto$mpg - predict(lm.fit2, Auto))[-train1]^2), mean((Auto$mpg - predict(lm.fit3, Auto))[-train1]^2))) Trying with a different sample found that results are consistent and using a quadradic function is better than a simple liners function… set.seed(2) train2 &lt;- sample(392,196) lm.fit4 &lt;- lm(mpg ~ horsepower, data = Auto, subset = train2) lm.fit5 &lt;- lm(mpg ~ poly(horsepower, 2), data = Auto,subset = train2) lm.fit6 &lt;- lm(mpg ~ poly(horsepower, 3), data = Auto,subset = train2) mse_df_full &lt;- mse_df %&gt;% rbind(data.frame(sample=&quot;seed_2&quot;, poly=1:3, mse=c(mean((Auto$mpg - predict(lm.fit4, Auto))[-train2]^2), mean((Auto$mpg - predict(lm.fit5, Auto))[-train2]^2), mean((Auto$mpg - predict(lm.fit6, Auto))[-train2]^2)) )) %&gt;% group_by(sample)%&gt;% arrange(mse) mse_df_full ## # A tibble: 6 × 3 ## # Groups: sample [2] ## sample poly mse ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 seed_1 2 18.7 ## 2 seed_1 3 18.8 ## 3 seed_2 3 20.4 ## 4 seed_2 2 20.4 ## 5 seed_1 1 23.3 ## 6 seed_2 1 25.7 p1 &lt;-ggplot(mse_df_full, aes(x=factor(1:6),y=mse,color=sample))+ geom_line(group=1,color=&quot;grey35&quot;)+ geom_point()+ labs(x=&quot;model fit&quot;)+ theme_bw()+ theme(legend.position = &quot;none&quot;) p2 &lt;-ggplot(mse_df_full, aes(x=factor(poly),y=mse,color=sample))+ geom_point()+ labs(x=&quot;polynomial&quot;)+ theme_bw()+ theme(axis.title.y = element_blank()) p1+p2 5.14.2 Leave-One-Out Cross-Validation LOOCV estimate for generalized linear models glm() cv.glm() These linear regressions are the same glm.fit &lt;- glm(mpg ~ horsepower, data = Auto) lm.fit &lt;- lm(mpg ~ horsepower, data = Auto) coef(glm.fit) ## (Intercept) horsepower ## 39.9358610 -0.1578447 coef(lm.fit) ## (Intercept) horsepower ## 39.9358610 -0.1578447 In this section we use glm() as it can be used with `cv.glm() function to compare results. In the cv.glm function, the two numbers in the delta vector contain the cross-validation results or the LOOCV statistic LOOCV estimate for the MSE test error is the average of \\(n\\) tests error estimates: \\[CV_{n} = \\frac{1}{n}{\\sum_{i=1}^{n}}MSE_{i}\\] library(boot) glm.fit_cv &lt;- glm(mpg ~ horsepower, data = Auto) cv.err &lt;- cv.glm(Auto, glm.fit_cv) cv.err$delta ## [1] 24.23151 24.23114 cv.error &lt;- rep(0, 10) # iteratively fits polynomial regressions for polynomials of order i = 1 to i = 10 for (i in 1:10) { glm.fit &lt;- glm(mpg ~ poly(horsepower, i), data = Auto) cv.error[i] &lt;- cv.glm(Auto, glm.fit)$delta[1] } cv.error ## [1] 24.23151 19.24821 19.33498 19.42443 19.03321 18.97864 18.83305 18.96115 ## [9] 19.06863 19.49093 Sharp drop in the estimated test MSE between the linear and quadratic fits. ggplot(data = data.frame(cv.error), aes(x = factor(1:10), y =cv.error) ) + geom_line(group=1,color=&quot;blue&quot;)+ geom_point(color=&quot;white&quot;,stroke=1,shape=21,size=2)+ geom_point(color=&quot;blue&quot;,size=1.5)+ labs(x=&quot;Degree of Polynomial&quot;,y=&quot;MSE&quot;,title=&quot;LOOCV&quot;)+ scale_y_continuous(limits = c(16,28),breaks = seq(16,28,1))+ theme_bw()+ theme(panel.grid = element_line(size=0.2)) 5.14.3 k-Fold Cross-Validation cv.glm() function is also used to implement k-fold CV. computation time shorter k &lt;- 10 set.seed(17) cv.error.10 &lt;- rep(0,10) for (i in 1:10) { glm.fit &lt;- glm(mpg ~ poly(horsepower, i), data = Auto) cv.error.10[i] &lt;- cv.glm(Auto, glm.fit, K=10)$delta[1] } cv.error.10 ## [1] 24.27207 19.26909 19.34805 19.29496 19.03198 18.89781 19.12061 19.14666 ## [9] 18.87013 20.95520 data.frame(id=1:10, cv.error=cv.error, cv.error.10=cv.error.10) %&gt;% pivot_longer(cols=c(cv.error,cv.error.10),names_to=&quot;cv&quot;,values_to=&quot;cv_values&quot;)%&gt;% ggplot(aes(x = id, y =cv_values,group=cv)) + geom_line(aes(color=cv))+ geom_point(shape=&quot;.&quot;)+ labs(x=&quot;Degree of Polynomial&quot;,y=&quot;MSE&quot;,title=&quot;10-fold CV&quot;)+ scale_y_continuous(limits = c(16,28),breaks = seq(16,28,1))+ theme_bw()+ theme(panel.grid = element_line(size=0.2)) 5.14.4 The Bootstrap Bootstrap approach can be applied in almost all situations. data(&quot;Portfolio&quot;) 5.14.4.1 Estimating the Accuracy of a Statistic of Interest to assess the variability associated with the regression coefficients in a linear model fit, we need to quantify the uncertainty, and choose \\(\\alpha\\) in order minimize the total risk, or variance, of the investment. \\(\\alpha\\) is the fraction we want to invest in \\(X\\) and \\((1-\\alpha)\\) is the fraction we want to invest in \\(Y\\) \\[Var=\\alpha X+(1-\\alpha)Y)\\] \\[\\alpha=\\frac{\\sigma^2_{Y}-\\sigma_{XY}}{\\sigma^2_{X}+\\sigma^2_{Y}-2\\sigma_{XY}}\\] alpha.fn &lt;- function(data,index) { X &lt;- data$X[index] Y &lt;- data$Y[index] (var(Y)-cov(X,Y))/(var(X)+var(Y)-2*cov(X,Y)) } First \\(\\alpha\\) calculation alpha.fn(Portfolio, 1:100) ## [1] 0.5758321 Bootstrap contruction Select 100 random observations from the range 1 to 100, with replacement, recompute \\(\\hat{\\alpha}\\) based on new data. set.seed(7) alpha.fn(Portfolio, sample(100, 100, replace = T)) ## [1] 0.5385326 Applying the boot() function with the alpha.fn and 1000 replications. boot(Portfolio, alpha.fn, R = 1000) ## ## ORDINARY NONPARAMETRIC BOOTSTRAP ## ## ## Call: ## boot(data = Portfolio, statistic = alpha.fn, R = 1000) ## ## ## Bootstrap Statistics : ## original bias std. error ## t1* 0.5758321 0.0007959475 0.08969074 5.14.4.2 Estimating the Accuracy of a Linear Regression Model to assess the variability of the coefficient estimates and predictions In particular, to assess the variability of the estimates for \\(\\beta_{1}\\) and \\(\\beta_{2}\\) boot.fn &lt;- function(data, index) coef(lm(mpg ~ horsepower, data = data, subset = index)) boot.fn(Auto, 1:392) ## (Intercept) horsepower ## 39.9358610 -0.1578447 If we try with different samples we obtain different results, with random observations. Calculate the standard errors of 1,000 bootstrap estimates for the intercept and slope terms, applying the boot.fn to the bootstrap function boot(): boot(Auto, boot.fn, 1000) ## ## ORDINARY NONPARAMETRIC BOOTSTRAP ## ## ## Call: ## boot(data = Auto, statistic = boot.fn, R = 1000) ## ## ## Bootstrap Statistics : ## original bias std. error ## t1* 39.9358610 0.0815499334 0.846979326 ## t2* -0.1578447 -0.0009816788 0.007234668 summary(lm(mpg ~ horsepower, data = Auto))$coef ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 39.9358610 0.717498656 55.65984 1.220362e-187 ## horsepower -0.1578447 0.006445501 -24.48914 7.031989e-81 What if we change the model adding I(horsepower^2) term in the model? boot.fn &lt;- function(data, index) coef(lm(mpg ~ horsepower + I(horsepower^2), data = data, subset = index)) set.seed(1) boot(Auto, boot.fn, 1000) ## ## ORDINARY NONPARAMETRIC BOOTSTRAP ## ## ## Call: ## boot(data = Auto, statistic = boot.fn, R = 1000) ## ## ## Bootstrap Statistics : ## original bias std. error ## t1* 56.900099702 3.511640e-02 2.0300222526 ## t2* -0.466189630 -7.080834e-04 0.0324241984 ## t3* 0.001230536 2.840324e-06 0.0001172164 summary(lm(mpg ~ horsepower + I(horsepower^2), data = Auto))$coef ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 56.900099702 1.8004268063 31.60367 1.740911e-109 ## horsepower -0.466189630 0.0311246171 -14.97816 2.289429e-40 ## I(horsepower^2) 0.001230536 0.0001220759 10.08009 2.196340e-21 "],["meeting-videos-4.html", "5.15 Meeting Videos", " 5.15 Meeting Videos 5.15.1 Cohort 1 Meeting chat log 00:14:53 jonathan.bratt: Sorry, didn’t realize I was unmuted. :) 00:15:03 Raymond Balise: be right back… I am listening 00:15:16 August: time series can also be analysed with features, which means you can use decision trees, and not rely on the sequential indexing. 00:17:08 August: for those wanting to understand when we aren&#39;t modelling on features https://otexts.com/fpp3/tscv.html 00:17:18 August: application in modeltime https://cran.r-project.org/web/packages/modeltime.resample/vignettes/getting-started.html 00:25:53 Jon Harmon (jonthegeek): 10-fold CV is orange, LOOCV is black dashed, true is blue. 00:27:24 Mei Ling Soh: How do we decide on the k-value? 00:27:47 August: Its kind of a choice. 00:29:05 jonathan.bratt: because we have five fingers on each hand :) 00:29:39 Jon Harmon (jonthegeek): rsample::vfold_cv defaults to 10 so I generally do 10 🙃 00:29:47 August: the answer is in section 7.10.1 of ESL 00:29:50 Mei Ling Soh: 😅 00:30:04 August: By answer I mean explination 00:31:09 August: With time series I tend to use about 5 or 6 k when using models which utilise sequential indexing. 00:32:10 jonathan.bratt: Can you use the arrow keys? 00:38:11 August: This is quite interesting: https://machinelearningmastery.com/how-to-configure-k-fold-cross-validation/ 00:39:53 Mei Ling Soh: Thanks, August! Meeting chat log 00:11:39 Jon Harmon (jonthegeek): https://twitter.com/whyRconf 00:14:38 SriRam: go for one page view, not scrolling view 00:25:46 Wayne Defreitas: lol 00:33:53 SriRam: it should be square root of mean, not mean of square root ? Or did i read it wrong? 00:45:45 Wayne Defreitas: This was great thank you 00:46:27 Mei Ling Soh: Great! Thanks 00:49:19 jonathan.bratt: Ch6 is long 5.15.2 Cohort 2 Meeting chat log 00:29:18 Ricardo Serrano: https://www.statology.org/assumptions-of-logistic-regression/ 00:29:26 Ricardo Serrano: https://towardsdatascience.com/assumptions-of-logistic-regression-clearly-explained-44d85a22b290 01:06:18 Jim Gruman: thank you Federica!!! more horsepower 🐎 01:07:47 Jim Gruman: I need to jump off. Talk to you all next week. Ciao Meeting chat log 00:18:27 Federica Gazzelloni: Hello Jenny! 00:18:44 jlsmith3: Good morning! 00:39:57 Ricardo Serrano: References for bias/variance https://www.bmc.com/blogs/bias-variance-machine-learning/ 00:40:26 Ricardo Serrano: https://youtu.be/EuBBz3bI-aA 00:40:36 Federica Gazzelloni: Thanks Ricardo! 00:57:55 Jim Gruman: 🐎 thank you everybody! 5.15.3 Cohort 3 Meeting chat log 00:07:07 Nilay Yönet: https://emilhvitfeldt.github.io/ISLR-tidymodels-labs/resampling-methods.html 00:33:41 Mei Ling Soh: Maybe we can wrap up the lab soon? 00:37:48 Fariborz Soroush: 👍 01:04:24 Mei Ling Soh: Two more minutes to go 01:05:19 Nilay Yönet: https://onmee.github.io/assets/docs/ISLR/Resampling-Methods.pdf 01:05:22 Nilay Yönet: https://waxworksmath.com/Authors/G_M/James/WWW/chapter_5.html 5.15.4 Cohort 4 Meeting chat log ADD LOG HERE "],["linear-model-selection-and-regularization.html", "Chapter 6 Linear Model Selection and Regularization", " Chapter 6 Linear Model Selection and Regularization Learning objectives: Select a subset of features to include in a linear model. Compare and contrast the forward stepwise, backward stepwise, hybrid, and best subset methods of subset selection. Use shrinkage methods to constrain the flexibility of linear models. Compare and contrast the lasso and ridge regression methods of shrinkage. Reduce the dimensionality of the data for a linear model. Compare and contrast the PCR and PLS methods of dimension reduction. Explain the challenges that may occur when fitting linear models to high-dimensional data. "],["subset-selection.html", "6.1 Subset Selection", " 6.1 Subset Selection "],["context-for-this-chapter.html", "Context for This Chapter", " Context for This Chapter lm(y ~ ., data) \\(p \\approx n, p = n, p &gt; n\\) perdiction error low bias (by assumption) … but variance \\(p \\approx n\\) … or meaninglessness \\(p = n\\) … or impossibility \\(p &gt; n\\) model interpretability (?) "],["best-subset-selection-bss.html", "Best Subset Selection (BSS)", " Best Subset Selection (BSS) “To perform best subset selection, we fit a separate least squares regression best subset for each possible combination of the p preditors.” “That is, we fit all p models selection that contain exactly one predictor, all \\((^p_2) = p(p - 1)/2\\) models that contain exactly two predictors, and so forth.” "],["bss-algorithm.html", "BSS Algorithm", " BSS Algorithm Start with the null model (intercept-only model), \\(\\mathcal{M}_0\\). For \\(k = 1, 2, ..., p\\): Fit all \\((^p_k)\\) models containing \\(k\\) predictors Let \\(\\mathcal{M}_k\\) denote the best of these \\((^p_k)\\) models, where best is defined as having the lowest RSS, lowest deviance, etc Choose the best model among \\(\\mathcal{M}_0, ..., \\mathcal{M}_p\\), where best is defined as having the lowest \\(C_p\\), \\(BIC\\), \\(AIC\\), cross-valided MSE, or, alternatively, highest adjusted \\(R^2\\) "],["bss-algorithm-1.html", "BSS Algorithm", " BSS Algorithm (Figure 6.1 from book) "],["behind-the-dots-bssing-islr2credit.html", "Behind the dots, BSS’ing ISLR2::Credit", " Behind the dots, BSS’ing ISLR2::Credit \\((^{11}_1) = 11\\) \\((^{11}_2) = 55\\) \\((^{11}_3) = 165\\) \\((^{11}_4) = 330\\) \\((^{11}_5) = 462\\) 462, 300, 165, 55, 11, 1 … "],["best-subset-selection-bss-1.html", "Best Subset Selection (BSS)", " Best Subset Selection (BSS) Pros Selects the best subset Cons Overfitting I don’t know how to incorporate polynomial terms and interactions (Go “Nike” and Just Do It?) Computationally expensive "],["forward-stepwise-subset-selection-fsss.html", "Forward Stepwise Subset Selection (FsSS)", " Forward Stepwise Subset Selection (FsSS) Let \\(\\mathcal{M}_0\\) denote the null model (no predictors) For \\(k = 1, ..., p\\): Fit all \\(p - (k - 1)\\) predictors not in model \\(\\mathcal{M}_{k - 1}\\) Select the predictor that raises \\(R^2\\) the most and add it to model \\(\\mathcal{M}_{k - 1}\\) to create model \\(\\mathcal{M}_k\\) Select the model among \\(\\mathcal{M}_0, ..., \\mathcal{M}_k\\) that minimizes validation error (or some estimate of it) "],["fsssing-islr2credit.html", "FsSS’ing ISLR2::Credit", " FsSS’ing ISLR2::Credit \\((^{11}_1) = 11\\) (same as BSS) \\((^{10}_1) = 10\\) (instead of 55) \\((^9_1) = 9\\) (instead of 165) \\((^8_1) = 8\\) (instead of 330) \\((^7_1) = 7\\) (instead of 462) 6 instead of 462, 5 instead of 330, etc “When \\(p = 20\\), best subset selection requires fitting 1,048,576 models, whereas forward stepwise selection requires fitting only 211 models.” "],["backward-stepwise-subset-selection-bsss.html", "Backward Stepwise Subset Selection (BsSS)", " Backward Stepwise Subset Selection (BsSS) Make sure that \\(n &gt; p\\) Let \\(\\mathcal{M}_p\\) denote the full model with all p predictors For \\(k = p, p - 1, ..., 1\\): Consider all \\(k\\) models that result in dropping a single predictor from \\(\\mathcal{M}_k\\) (thus containing \\(k - 1\\) predictors) Choose the best among these \\(k\\) models, and christen it \\(\\mathcal{M}_{k-1}\\) Select the model among \\(\\mathcal{M}_0, ..., \\mathcal{M}_k\\) that minimizes validation error (or some estimate of it) "],["guided-searches.html", "“Guided” searches", " “Guided” searches FsSS and BsSS perform “guided” searches Very fun footnote: “Though forward stepwise selection considers \\(p(p + 1)/2 + 1\\) models, it performs a guided search overl model space, and so the effective model space considered contains substantially more than \\(p(p + 1)/2 + 1\\) models.” (Table 6.1) "],["choosing-the-best-model.html", "Choosing the best model", " Choosing the best model You have to punish models for having too many predictors Whatever the method, \\(RSS\\) decreases / \\(R^2\\) increases as we go from \\(\\mathcal{M}_k\\) to \\(\\mathcal{M}_{k+1}\\). Thus, \\(\\mathcal{M}_p\\) always wins that contest. (Figure 6.1?) Going with \\(\\mathcal{M}_p\\) doesn’t provide either of the benefits: model interpretability and variance reduction (overfitting) We’ll need to estimate test error! "],["adjustment-methods.html", "Adjustment Methods", " Adjustment Methods \\(C_p = \\frac{1}{n}(Rss + 2k\\hat{\\sigma}^2)\\) \\(\\hat{\\sigma}^2\\) is an “estimate of variance of the error \\(\\epsilon\\) associated with each response measurement” typically estimated using \\(\\mathcal{M}_p\\) like any estimator, can approach its estimand biasedly or unbiasedly (pray for the latter) \\(AIC = 2k - 2ln(\\hat{L})\\) \\(BIC = k \\cdot ln(N) - 2ln(\\hat{L})\\) adjusted \\(R^2 = 1 - \\frac{RSS}{TSS} \\cdot \\frac{n-1}{n-k-1}\\) "],["avoiding-adjustment-methods.html", "Avoiding Adjustment Methods", " Avoiding Adjustment Methods \\(\\hat{\\sigma}^2\\) can be hard to come by model df, too adjustment methods make assumptions about true model so cross-validate! "],["various-variable-selection-methods-on-islr2credit.html", "Various variable selection methods on ISLR2::Credit", " Various variable selection methods on ISLR2::Credit (figure comparing different measures) "],["in-conclusion.html", "In conclusion…", " In conclusion… A way to discard redundant/noise features atheoretically Different estimate of test error return different optima You’re selecting/tuning complexity, not a particular set of predictors "],["shrinkage-methods.html", "6.2 Shrinkage Methods", " 6.2 Shrinkage Methods (Costanza meme) "],["overview.html", "Overview", " Overview Shrinkage reduces variance and can perform variable selection ‘Substantial’ reduction in variance for a ‘slight’ increase in bias Achieves these desiderata by ‘penalizing’ parameters Produce models ‘between’ the null model and the OLS estimates "],["ols-review.html", "OLS review", " OLS review \\(\\hat{\\beta}^{OLS} \\equiv \\underset{\\hat{\\beta}}{argmin}(\\sum_{i=1}^{n}{(y_i - \\hat{\\beta} - \\sum_{k=1}^{p}{\\beta_kx_{ik}})^2})\\) \\(\\hat{\\beta}^{OLS} \\equiv \\underset{\\hat{\\beta}}{argmin}(RSS)\\) "],["ridge-regression.html", "Ridge Regression", " Ridge Regression \\(\\hat{\\beta}^{OLS} \\equiv \\underset{\\hat{\\beta}}{argmin}(RSS)\\) \\(\\hat{\\beta}^R \\equiv \\underset{\\hat{\\beta}}{argmin}(RSS+\\lambda\\sum_{k=1}^p{\\beta_k^2})\\) \\(\\lambda\\) tuning parameter (hyperparameter) for the shrinkage penalty there’s one model parameter \\(\\lambda\\) doesn’t shrink (\\(\\hat{\\beta_0}\\)) "],["ridge-regression-visually.html", "Ridge Regression, Visually", " Ridge Regression, Visually (figure showing ridge regression) \\[\\|\\beta\\|_2 = \\sqrt{\\sum_{j=1}^p{\\beta_j^2}}\\] "],["ridge-good.html", "Ridge, good?", " Ridge, good? (figure of ridge success(?)) “One can show that computations required to solve (6.5), simultaneously for all values of \\(\\lambda\\), are almost identical to those for fitting a model using least squares.” "],["preprocessing.html", "Preprocessing", " Preprocessing \\(\\beta_j^R\\) aren’t scale invariant, so: \\[\\tilde{x}_{ij} = \\frac{x_{ij}}{\\sqrt{\\frac{1}{n}\\sum_i^n{(x_{ij} - \\bar{x}_j)^2}}}\\] Applies to the least absolute shrinkage and selection operator as well "],["the-lasso.html", "The Lasso", " The Lasso \\(\\hat{\\beta}^L \\equiv \\underset{\\hat{\\beta}}{argmin}(RSS + \\lambda\\sum_{k=1}^p{|\\beta_k|})\\) Shrinks coefficients to 0 (sparse models) In glmnet \\(\\alpha = 1\\) is the default in: \\[\\lambda[(1 - \\alpha)\\|\\beta\\|_2^2/2 + \\alpha\\|\\beta\\|_1]\\] ## The Lasso, Visually {.unnumbered} (figure of the lasso) \\[\\|\\beta\\|_1 = \\sum_{j=1}^p{|\\beta_j|}\\] "],["justin-grimmer-homework.html", "Justin Grimmer Homework", " Justin Grimmer Homework (figure from homework Justin found online) "],["visualization-of-such-that-formulations.html", "Visualization of “such that” formulations", " Visualization of “such that” formulations (figure of contours) \\[|\\beta_1|+|\\beta_2| \\leq s\\ \\mathrm{and}\\ \\beta_1^2+\\beta_2^2 \\leq s\\] "],["bayesian-interpretation.html", "Bayesian Interpretation", " Bayesian Interpretation \\[X = (X_1, ..., X_p)\\] \\[\\beta = (\\beta_0, \\beta_1, ..., \\beta_p)^T\\] \\[P(\\beta|X, Y) \\propto f(Y|X,\\beta)P(\\beta|X) = f(Y|X, \\beta)P(\\beta)\\] "],["bayesian-interpretation-cont.html", "Bayesian Interpretation (cont)", " Bayesian Interpretation (cont) \\[P(\\beta) = \\prod_{j=1}^p{g(\\beta_j)}\\] (gaussian prior images) "],["choosing-the-tuning-parameter.html", "Choosing the tuning parameter", " Choosing the tuning parameter (figures) Left: Lasso, Right: Lasso is solid, ridge is dotted "],["tuning-lambda.html", "Tuning \\(\\lambda\\)", " Tuning \\(\\lambda\\) (figures) "],["dimension-reduction-methods.html", "6.3 Dimension Reduction Methods", " 6.3 Dimension Reduction Methods Transform predictors before use. \\(Z_1, Z_2, ..., Z_M\\) represent \\(M &lt; p\\) linear combinations of original p predictors. \\[Z_m = \\sum_{j=1}^p{\\phi_{jm}X_j}\\] Linear regression using the transformed predictors can “often” outperform linear regression using the original predictors. "],["the-math.html", "The Math", " The Math \\[Z_m = \\sum_{j=1}^p{\\phi_{jm}X_j}\\] \\[y_i = \\theta_0 + \\sum_{m=1}^M{\\theta_mz_{im} + \\epsilon_i}, i = 1, ..., n\\] \\[\\sum_{m=1}^M{\\theta_mz_{im}} = \\sum_{m=1}^M{\\theta_m}\\sum_{j=1}^p{\\phi_{jm}x_ij}\\] \\[\\sum_{m=1}^M{\\theta_mz_{im}} = \\sum_{j=1}^p\\sum_{m=1}^M{\\theta_m\\phi_{jm}x_ij}\\] \\[\\sum_{m=1}^M{\\theta_mz_{im}} = \\sum_{j=1}^p{\\beta_jx_ij}\\] \\[\\beta_j = \\sum_{m=1}^M{\\theta_m\\phi_{jm}}\\] Dimension reduction constrains \\(\\beta_j\\) Can increase bias, but (significantly) reduce variance when \\(M \\ll p\\) "],["principal-components-regression.html", "Principal Components Regression", " Principal Components Regression Allison Horst’s whale-shark-vs-krill: imagine you’re a whale shark… "],["principal-components-regression-1.html", "Principal Components Regression", " Principal Components Regression Allison Horst’s whale-shark-vs-krill: …approaching this delicious krill swarm "],["principal-components-regression-2.html", "Principal Components Regression", " Principal Components Regression (Future cohorts: Try to get a screenshot of everyone turning their head like the shark; Cohort 1 mostly had their cameras off, so I didn’t get a head turn) PCA chooses \\(\\phi\\)s to capture as much variance as possible. Will be discussed in more detail in Chapter 12. First principal component = line of best fit to the data Second principal component = orthoganol to 1st, best fit remaining Etc "],["principal-components-regression-3.html", "Principal Components Regression", " Principal Components Regression Figure 6.15 \\[Z_1 = 0.839 \\times (\\mathrm{pop} - \\overline{\\mathrm{pop}}) + 0.544 \\times (\\mathrm{ad} - \\overline{\\mathrm{ad}})\\] "],["principal-components-regression-4.html", "Principal Components Regression", " Principal Components Regression Figure 6.19 Assume most of the variation in \\(X\\) is associated with variation in \\(Y\\). Mitigate overfitting by reducing number of variables. When assumption is true, PCR can do very well. Note: PCR isn’t feature selection, since PCs depend on all \\(p\\)s. More like ridge than lasso. Best to standardize variables before PCR. "],["principal-components-regression-5.html", "Principal Components Regression", " Principal Components Regression Figure 6.18 When variation in \\(X\\) isn’t strongly correlated with variation in \\(Y\\), PCR isn’t as effective. "],["partial-least-squares.html", "Partial Least Squares", " Partial Least Squares Partial Least Squares (PLS) is like PCA, but supervised (use \\(Y\\) to choose). In this figure, pop is more related to \\(Y\\) than is ad. In practice, PLS often isn’t better than ridge or PCR. Supervision can reduce bias (vs PCR), but can also increase variance. Figure 6.21 "],["considerations-in-high-dimensions.html", "6.4 Considerations in High Dimensions", " 6.4 Considerations in High Dimensions Modern data can have a huge number of predictors (eg: 500k SNPs, every word ever entered in a search) When \\(n = p\\), linear regression memorizes the training data… Figure 6.22 "],["considerations-in-high-dimensions-1.html", "Considerations in High Dimensions", " Considerations in High Dimensions Modern data can have a huge number of predictors (eg: 500k SNPs, every word ever entered in a search) When \\(n &lt; p\\), linear regression memorizes the training data, but can suck on test data. Figure 6.23 "],["lasso-etc-vs-dimensionality.html", "Lasso (etc) vs Dimensionality", " Lasso (etc) vs Dimensionality Reducing flexibility (all the stuff in this chapter) can help. It’s important to choose good tuning parameters for whatever method you use. Features that aren’t associated with \\(Y\\) increase test error (“curse of dimensionality”). Fit to noise in training, noise in test is different. When \\(p &gt; n\\), never use train MSE, p-values, \\(R^2\\), etc, because they’re likely to be wildly different from test values. Figure 6.24 "],["meeting-videos-5.html", "6.5 Meeting Videos", " 6.5 Meeting Videos 6.5.1 Cohort 1 Meeting chat log 00:10:43 jonathan.bratt: :-D 00:11:02 Laura Rose: chapter 6 is really long! 00:11:07 jonathan.bratt: Yes, it is! 00:11:38 Laura Rose: I can probably not do 12/21 either, due to another meeting 00:11:50 jonathan.bratt: second 00:11:52 Laura Rose: 2 00:12:52 SriRam: except Ryan :D 00:13:17 Ryan Metcalf: People run away if I were to grow a beard! Patchy! 00:13:24 SriRam: :D 00:35:32 jonathan.bratt: &gt; morphemepiece::morphemepiece_tokenize(&quot;unbiasedly&quot;) [[1]] un## bias ##ed ##ly 11995 4903 4030 4057 00:36:06 Jon Harmon (jonthegeek): lol I did the same thing Jonathan 00:36:32 jonathan.bratt: 😆 00:36:48 Jon Harmon (jonthegeek): (that&#39;s a package Jonathan and I are working on for NLP) 01:01:05 jonathan.bratt: That was really good. Meeting chat log 00:34:42 Federica Gazzelloni: tidy models: https://www.tmwr.org/dimensionality.html 00:35:05 Ryan Metcalf: https://www.tmwr.org/dimensionality.html#partial-least-squares 00:35:09 Federica Gazzelloni: pca for unsupervised 00:35:18 Federica Gazzelloni: pls for supervised 6.5.2 Cohort 2 Meeting chat log 00:08:25 Jim Gruman: Hello 00:08:36 Federica Gazzelloni: Hi 00:17:45 Michael Haugen: Hi, Sorry I am late. 00:35:35 Ricardo: Check the broom package 00:36:25 Ricardo: https://broom.tidymodels.org/reference/augment.lm.html 00:36:56 Jim Gruman: +1 yeah, to uncover the method help details, you can run class() on the object to see the S3 class, and then ask for help on autoplot.class to see the specific help on the autoplot in the dependent package 00:50:40 Ricardo: PCA is unsupervised, PLS is supervised (check the outcome argument) 6.5.3 Cohort 3 Meeting chat log 00:07:07 Nilay Yönet: https://emilhvitfeldt.github.io/ISLR-tidymodels-labs/resampling-methods.html 00:33:41 Mei Ling Soh: Maybe we can wrap up the lab soon? 00:37:48 Fariborz Soroush: 👍 01:04:24 Mei Ling Soh: Two more minutes to go 01:05:19 Nilay Yönet: https://onmee.github.io/assets/docs/ISLR/Resampling-Methods.pdf 01:05:22 Nilay Yönet: https://waxworksmath.com/Authors/G_M/James/WWW/chapter_5.html Meeting chat log 00:23:20 Mei Ling Soh: Each resample is created within the stratification variable. Numeric strata are binned into quartiles. 00:34:05 Jeremy Selva: https://dionysus.psych.wisc.edu/iaml/unit-06.html#elastic-net-regression 00:54:24 Jeremy Selva: Here are the link to the slides and the lab https://jauntyjjs.github.io/islr2-bookclub-cohort3-chapter6 https://jauntyjjs.github.io/islr2-bookclub-cohort3-chapter6-lab/ Source codes for the slides and the lab https://github.com/JauntyJJS/islr2-bookclub-cohort3-chapter6 https://github.com/JauntyJJS/islr2-bookclub-cohort3-chapter6-lab 00:55:57 Rose Hartman: Thanks, Jeremy! 00:56:01 Nilay Yönet: thank you Jeremy! 6.5.4 Cohort 4 Meeting chat log ADD LOG HERE "],["moving-beyond-linearity.html", "Chapter 7 Moving Beyond Linearity", " Chapter 7 Moving Beyond Linearity Learning objectives: Model relationships between a predictor and an outcome with polynomial regression step functions regression splines smoothing splines local regression generalized additive models Chapter 7 resources: The text at https://www.statlearning.com/ Cohort 1 videos Author videos Author slides Emil Hvitfeldt’s Tidymodels examples Kim Larsen’s GAM: The Predictive Modeling Silver Bullet Black motorcycle parked at road toward mountain suppressPackageStartupMessages({ library(splines) library(tidymodels) }) tidymodels_prefer() "],["polynomial-and-step-regression.html", "7.1 Polynomial and Step Regression", " 7.1 Polynomial and Step Regression The truth is never linear! But often linearity is good enough. This chapter presents a hierarchy of methods that offer more flexibility without losing much of the ease and interpret ability of linear models. The first is a polynomial expansion. \\[y_i = \\beta_0+\\beta_1x_i+\\beta_2x_i^2...+\\beta_dx_i^d+\\epsilon_i\\] A degree d polynomial Wage &lt;- as_tibble(ISLR2::Wage) %&gt;% mutate(high = factor(wage &gt; 250, levels = c(TRUE, FALSE), labels = c(&quot;High&quot;, &quot;Low&quot;))) rec_poly &lt;- recipe(wage ~ age, data = Wage) %&gt;% step_poly(age, degree = 4) lm_spec &lt;- linear_reg() %&gt;% set_mode(&quot;regression&quot;) %&gt;% set_engine(&quot;lm&quot;) poly_wf &lt;- workflow() %&gt;% add_model(lm_spec) %&gt;% add_recipe(rec_poly) poly_fit &lt;- fit(poly_wf, data = Wage) age_range &lt;- tibble(age = seq(min(Wage$age), max(Wage$age))) regression_lines &lt;- bind_cols( augment(poly_fit, new_data = age_range), predict(poly_fit, new_data = age_range, type = &quot;conf_int&quot;) ) regression_plot &lt;- Wage %&gt;% ggplot(aes(age, wage)) + geom_point(alpha = 0.1) + geom_line(aes(y = .pred), color = &quot;darkgreen&quot;, size = 1.5, data = regression_lines) + geom_line( aes(y = .pred_lower), data = regression_lines, linetype = &quot;dashed&quot;, color = &quot;blue&quot; ) + geom_line( aes(y = .pred_upper), data = regression_lines, linetype = &quot;dashed&quot;, color = &quot;blue&quot; ) + theme_bw() + theme( strip.placement = &quot;outside&quot;, panel.grid = element_blank(), strip.background = element_blank(), plot.title.position = &quot;plot&quot; ) + labs(x = &quot;Age&quot;, y = &quot;Wage&quot;, subtitle = &quot;Regression&quot;) rec_poly &lt;- recipe(high ~ age, data = Wage) %&gt;% step_poly(age, degree = 4) lr_spec &lt;- logistic_reg() %&gt;% set_engine(&quot;glm&quot;) %&gt;% set_mode(&quot;classification&quot;) lr_poly_wf &lt;- workflow() %&gt;% add_model(lr_spec) %&gt;% add_recipe(rec_poly) lr_poly_fit &lt;- fit(lr_poly_wf, data = Wage) classification_lines &lt;- bind_cols( augment(lr_poly_fit, new_data = age_range, type = &quot;prob&quot;), predict(lr_poly_fit, new_data = age_range, type = &quot;conf_int&quot;) ) classification_plot &lt;- classification_lines %&gt;% ggplot(aes(age)) + ylim(c(0, 0.2)) + geom_line(aes(y = .pred_High), color = &quot;darkgreen&quot;, size = 1.5) + geom_line(aes(y = .pred_lower_High), color = &quot;blue&quot;, linetype = &quot;dashed&quot;) + geom_line(aes(y = .pred_upper_High), color = &quot;blue&quot;, linetype = &quot;dashed&quot;) + geom_jitter( aes(y = (high == &quot;High&quot;) / 5), data = Wage, shape = &quot;|&quot;, height = 0, width = 0.2 ) + theme_bw() + theme( strip.placement = &quot;outside&quot;, panel.grid = element_blank(), strip.background = element_blank() ) + labs(x = &quot;Age&quot;, y = &quot;Pr(Wage&gt;250 | Age)&quot;, subtitle = &quot;Classification&quot;) title_theme &lt;- cowplot::ggdraw() + cowplot::draw_label(&quot;ISLR2 Wage Dataset 4th degree polynomial fits and 2x pointwise SEs&quot;, x = 0.05, hjust = 0) cowplot::plot_grid(title_theme, cowplot::plot_grid(regression_plot, classification_plot), ncol = 1, rel_heights = c(0.1, 1)) ## Warning: Removed 8 row(s) containing missing values (geom_path). Details Create new variables \\(X_1 = X, X_2 = X^2\\) etc and then treat the problem the same as multiple linear regression. Under the hood, the expansion happens with the poly function Wage %&gt;% select(age) %&gt;% bind_cols(as_tibble(round(poly(Wage$age, 3), 3))) %&gt;% head() ## # A tibble: 6 × 4 ## age `1` `2` `3` ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 18 -0.039 0.056 -0.072 ## 2 24 -0.029 0.026 -0.015 ## 3 45 0.004 -0.015 0 ## 4 43 0.001 -0.015 0.005 ## 5 50 0.012 -0.01 -0.011 ## 6 54 0.018 -0.002 -0.017 What R does, and what is a best practice, is to force an orthogonal expansion to avoid correlations in the new variables. This behavior is somewhat different than what might have been expected, possibly as Wage %&gt;% select(age) %&gt;% bind_cols(as_tibble(round(poly(Wage$age, 3, raw = TRUE), 3))) %&gt;% head() ## # A tibble: 6 × 4 ## age `1` `2` `3` ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 18 18 324 5832 ## 2 24 24 576 13824 ## 3 45 45 2025 91125 ## 4 43 43 1849 79507 ## 5 50 50 2500 125000 ## 6 54 54 2916 157464 We are not really interested in the coefficients; more interested in the fitted function values at any value \\(x_0\\) \\[f\\hat(x_0) = \\hat\\beta_0+\\hat\\beta_1x_0+\\hat\\beta_2x_0^2+\\hat\\beta_3x_0^3+\\hat\\beta_4x_0^4\\] Since \\(f\\hat(x_0)\\) is a linear function, we can get a simple expression for pointwise-variances at any value \\(x_0\\). We either fix the degree d at some reasonably low value, or else use cross-validation to choose d. Logistic regression follows naturally. Here we mutate a categorical variable high for \\(y_i &gt; 250 |x_i\\) To get confidence intervals, compute upper and lower bounds on the logit scale, and then invert to get on probability scale. Can apply the polynomial expansion separately on several variables. See GAMs later for a better approach. Important Caveat: polynomials have notorious tail behavior, which can be very bad for extrapolation. Step Functions Another way of creating transformations of a variable — cut the variable into distinct regions. rec_cut &lt;- recipe(wage ~ age, data = Wage) %&gt;% step_cut(age, breaks = c(30, 50, 70)) lm_spec &lt;- linear_reg() %&gt;% set_mode(&quot;regression&quot;) %&gt;% set_engine(&quot;lm&quot;) cut_wf &lt;- workflow() %&gt;% add_model(lm_spec) %&gt;% add_recipe(rec_cut) cut_fit &lt;- fit(cut_wf, data = Wage) regression_lines &lt;- bind_cols( augment(cut_fit, new_data = age_range), predict(cut_fit, new_data = age_range, type = &quot;conf_int&quot;) ) regression_plot &lt;- Wage %&gt;% ggplot(aes(age, wage)) + geom_point(alpha = 0.1) + geom_line(aes(y = .pred), color = &quot;darkgreen&quot;, size = 1.5, data = regression_lines) + geom_line( aes(y = .pred_lower), data = regression_lines, linetype = &quot;dashed&quot;, color = &quot;blue&quot; ) + geom_line( aes(y = .pred_upper), data = regression_lines, linetype = &quot;dashed&quot;, color = &quot;blue&quot; ) + theme_bw() + theme( strip.placement = &quot;outside&quot;, panel.grid = element_blank(), strip.background = element_blank(), plot.title.position = &quot;plot&quot; ) + labs(x = &quot;Age&quot;, y = &quot;Wage&quot;, subtitle = &quot;Regression&quot;) rec_cut &lt;- recipe(high ~ age, data = Wage) %&gt;% step_cut(age, breaks = c(30, 50, 70)) lr_spec &lt;- logistic_reg() %&gt;% set_engine(&quot;glm&quot;) %&gt;% set_mode(&quot;classification&quot;) cut_wf &lt;- workflow() %&gt;% add_model(lr_spec) %&gt;% add_recipe(rec_cut) cut_fit &lt;- fit(cut_wf, data = Wage) classification_lines &lt;- bind_cols( augment(cut_fit, new_data = age_range, type = &quot;prob&quot;), predict(cut_fit, new_data = age_range, type = &quot;conf_int&quot;) ) classification_plot &lt;- classification_lines %&gt;% ggplot(aes(age)) + ylim(c(0, 0.2)) + geom_line(aes(y = .pred_High), color = &quot;darkgreen&quot;, size = 1.5) + geom_line(aes(y = .pred_lower_High), color = &quot;blue&quot;, linetype = &quot;dashed&quot;) + geom_line(aes(y = .pred_upper_High), color = &quot;blue&quot;, linetype = &quot;dashed&quot;) + geom_jitter( aes(y = (high == &quot;High&quot;) / 5), data = Wage, shape = &quot;|&quot;, height = 0, width = 0.2 ) + theme_bw() + theme( strip.placement = &quot;outside&quot;, panel.grid = element_blank(), strip.background = element_blank() ) + labs(x = &quot;Age&quot;, y = &quot;Pr(Wage&gt;250 | Age)&quot;, subtitle = &quot;Classification&quot;) title_theme &lt;- cowplot::ggdraw() + cowplot::draw_label(&quot;ISLR2 Wage Dataset Step Function fits and 2x pointwise SEs&quot;, x = 0.05, hjust = 0) cowplot::plot_grid(title_theme, cowplot::plot_grid(regression_plot, classification_plot), ncol = 1, rel_heights = c(0.1, 1)) ## Warning: Removed 10 row(s) containing missing values (geom_path). Easy to work with. Creates a series of dummy variables representing each group. Useful way of creating interactions that are easy to interpret. Choice of cutpoints or knots can be problematic. For crafting the nonlinearities, smoother alternatives such as splines are available. Piecewise Polynomials Instead of a single polynomial in X over its whole domain, we can rather use different polynomials in regions defined by knots. Better to add constraints to the polynomials, e.g. continuity Splines have the “maximum” amount of continuity "],["splines.html", "7.2 Splines", " 7.2 Splines A linear spline is a piecewise linear polynomial continuous at each knot. We can represent this model as \\[ y_i = \\beta_0 + \\beta_1b_1(x_i) + \\beta_2b_2(x_i) + · · · + \\beta_{K+1}b_{K+1}(x_i) + \\epsilon_i \\] where the \\(b_k\\) are basis functions. ### fit &lt;- lm(wage ~ bs(age, knots = c(25, 40, 60)), data = Wage) agelims &lt;- range(Wage$age) age.grid &lt;- seq(from = agelims[1], to = agelims[2]) pred &lt;- predict(fit, newdata = list(age = age.grid), se = T) plot(Wage$age, Wage$wage, col = &quot;gray&quot;) title(&quot;Poly B-Spline Basis w 3 knots and Natural Cubic Spline&quot;) lines(age.grid, pred$fit, lwd = 2) lines(age.grid, pred$fit + 2 * pred$se, lty = &quot;dashed&quot;) lines(age.grid, pred$fit - 2 * pred$se, lty = &quot;dashed&quot;) ### fit2 &lt;- lm(wage ~ ns(age, df = 4), data = Wage) pred2 &lt;- predict(fit2, newdata = list(age = age.grid), se = T) lines(age.grid, pred2$fit, col = &quot;red&quot;, lwd = 2) Cubic Splines A cubic spline is a piecewise cubic polynomial with continuous derivatives up to order 2 at each knot. To apply a cubic spline, the knot locations have to be defined. Intuitively, one might locate them at the quartile breaks or at range boundaries that are significant for the data domain. Natural Cubic Splines A natural cubic spline extrapolates linearly beyond the boundary knots. This adds 4 = 2 × 2 extra constraints, and allows us to put more internal knots for the same degrees of freedom as a regular cubic spline above. Fitting natural splines is easy. A cubic spline with K knots has K + 4 parameters or degrees of freedom. A natural spline with K knots has K degrees of freedom. The result is often a simpler, smoother, more generalizable function with less bias. Smoothing Splines The solution is a natural cubic spline, with a knot at every unique value of \\(x_i\\). A roughness penalty controls the roughness via \\(\\lambda\\). Smoothing splines avoid the knot-selection issue mathematically, leaving a single λ to be chosen. in the R function smooth.spline the degrees of freedom is often specified, not the λ smooth.spline has a built-in cross-validation function to choose a suitable DF automatically, as ordinary leave-one-out (TRUE) or ‘generalized’ cross-validation (GCV) when FALSE. The ‘generalized’ cross-validation method GCV technique will work best when there are duplicated points in x. ### plot(Wage$age, Wage$wage, xlim = agelims, cex = .5, col = &quot;darkgrey&quot;) title(&quot;Smoothing Spline&quot;) fit &lt;- smooth.spline(Wage$age, Wage$wage, df = 16) fit2 &lt;- smooth.spline(Wage$age, Wage$wage, cv = TRUE) ## Warning in smooth.spline(Wage$age, Wage$wage, cv = TRUE): cross-validation with ## non-unique &#39;x&#39; values seems doubtful lines(fit, col = &quot;red&quot;, lwd = 2) lines(fit2, col = &quot;blue&quot;, lwd = 2) legend(&quot;topright&quot;, legend = c(&quot;16 DF&quot;, &quot;6.8 DF&quot;), col = c(&quot;red&quot;, &quot;blue&quot;), lty = 1, lwd = 2, cex = .8) Local Regression Local regression is a slightly different approach for fitting which involves computing the fit at a target point \\(x_i\\) using only nearby observations. The loess fit yields a walk with a span parameter to arrive at a spline fit. ### plot(Wage$age, Wage$wage, xlim = agelims, cex = .5, col = &quot;darkgrey&quot;) title(&quot;Local Regression&quot;) fit &lt;- loess(wage ~ age, span = .2, data = Wage) fit2 &lt;- loess(wage ~ age, span = .5, data = Wage) lines(age.grid, predict(fit, data.frame(age = age.grid)), col = &quot;red&quot;, lwd = 2) lines(age.grid, predict(fit2, data.frame(age = age.grid)), col = &quot;blue&quot;, lwd = 2) legend(&quot;topright&quot;, legend = c(&quot;Span = 0.2&quot;, &quot;Span = 0.5&quot;), col = c(&quot;red&quot;, &quot;blue&quot;), lty = 1, lwd = 2, cex = .8) So far, every example model has shown one independent variable and one dependent variable. Much more utility comes when these techniques are generalized broadly for many independent variables. A side note - normal ggplot geom_smooth for small datasets applies the loess method. as_tibble(ISLR2::Hitters) %&gt;% ggplot(aes(RBI, Runs)) + geom_point(alpha = 0.2, color = &quot;gray20&quot;) + stat_smooth(method = &quot;loess&quot;, span = 0.2, color = &quot;midnightblue&quot;, se = F) + stat_smooth(method = &quot;loess&quot;, span = 0.5, color = &quot;darkgreen&quot;, se = F) + stat_smooth(method = &quot;loess&quot;, span = 0.8, color = &quot;red&quot;, se = F) + labs(title = &quot;Loess Spans on ISLR2::Hitters&quot;, x = &quot;Runs Batted In&quot;, y = &quot;Runs&quot;, caption = &quot;Spans:\\nBlue: 0.2, Green: 0.5, Red: 0.8&quot;) + theme_minimal() ## `geom_smooth()` using formula &#39;y ~ x&#39; ## `geom_smooth()` using formula &#39;y ~ x&#39; ## `geom_smooth()` using formula &#39;y ~ x&#39; "],["generalized-additive-models.html", "7.3 Generalized Additive Models", " 7.3 Generalized Additive Models Generalized additive models were originally invented by Trevor Hastie and Robert Tibshirani in 1986 as a natural way to extend the conventional multiple linear regression model \\[ y_i = \\beta_0+\\beta_1x_{i1}+\\beta_2x_{i2}...+\\beta_px_{ip}+\\epsilon_i\\] Mathematically speaking, GAM is an additive modeling technique where the impact of the predictive variables is captured through smooth functions which, depending on the underlying patterns in the data, can be nonlinear: We can write the GAM structure as: \\[g(E(Y))=\\alpha + s_1(x_1)+...+s_p(x_p) \\] where \\(Y\\) is the dependent variable (i.e., what we are trying to predict), \\(E(Y)\\) denotes the expected value, and \\(g(Y)\\) denotes the link function that links the expected value to the predictor variables \\(x_1,…,x_p\\). The terms \\(s_1(x_1),…,s_p(x_p)\\) denote smooth, nonparametric functions. Note that, in the context of regression models, the terminology nonparametric means that the shape of predictor functions are fully determined by the data as opposed to parametric functions that are defined by a typically small set of parameters. This can allow for more flexible estimation of the underlying predictive patterns without knowing upfront what these patterns look like. The mgcv version: gam.m3 &lt;- mgcv::gam(wage ~ s(as.numeric(year), k = 4) + s(as.numeric(age), k = 5) + education, data = Wage) summary(gam.m3) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## wage ~ s(as.numeric(year), k = 4) + s(as.numeric(age), k = 5) + ## education ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 85.529 2.152 39.746 &lt; 2e-16 *** ## education2. HS Grad 10.896 2.429 4.487 7.51e-06 *** ## education3. Some College 23.415 2.556 9.159 &lt; 2e-16 *** ## education4. College Grad 38.058 2.541 14.980 &lt; 2e-16 *** ## education5. Advanced Degree 62.568 2.759 22.682 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(as.numeric(year)) 1.073 1.142 11.44 0.000392 *** ## s(as.numeric(age)) 3.381 3.787 57.96 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.289 Deviance explained = 29.1% ## GCV = 1241.3 Scale est. = 1237.4 n = 3000 par(mfrow = c(1, 3)) mgcv::plot.gam( gam.m3, residuals = FALSE, all.terms = TRUE, shade = TRUE, shade.col = 2, rug = TRUE, scale = 0 ) A comparable model, built with lm and natural cubic splines, plotted as the GAM contributions to the wage: gam::plot.Gam( lm(wage ~ ns(year, df = 4) + ns(age, df = 5) + education, data = Wage), residuals = FALSE, rugplot = TRUE, se = TRUE, scale = 0 ) Why Use GAMs? There are at least three good reasons why you want to use GAM: interpretability, flexibility/automation, and regularization. Hence, when your model contains nonlinear effects, GAM provides a regularized and interpretable solution, while other methods generally lack at least one of these three features. In other words, GAMs strike a nice balance between the interpretable, yet biased, linear model, and the extremely flexible, “black box” learning algorithms. Coefficients not that interesting; fitted functions are. Can mix terms — some linear, some nonlinear Can use smoothing splines AND local regression as well GAMs are simply additive, although low-order interactions can be included in a natural way using, e.g. bivariate smoothers or interactions Fitting GAMs in R The two main packages in R that can be used to fit generalized additive models are gam and mgcv. The gam package was written by Trevor Hastie and is more or less frequentist. The mgcv package was written by Simon Wood, and, while it follows the same framework in many ways, it is much more general because it considers GAM to be any penalized GLM (and Bayesian). The differences are described in detail in the documentation for mgcv. I discovered that gam and mgcv do not work well when loaded at the same time. Restart the R session if you want to switch between the two packages – detaching one of the packages is not sufficient. An example of a classification GAM model: gam.lr &lt;- mgcv::gam(I(wage &gt; 250) ~ as.numeric(year) + s(as.numeric(age), k = 5) + education, data = Wage) summary(gam.lr) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## I(wage &gt; 250) ~ as.numeric(year) + s(as.numeric(age), k = 5) + ## education ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.5459574 2.8324135 -0.193 0.84717 ## as.numeric(year) 0.0002724 0.0014121 0.193 0.84702 ## education2. HS Grad 0.0048026 0.0107994 0.445 0.65656 ## education3. Some College 0.0111980 0.0113639 0.985 0.32451 ## education4. College Grad 0.0313156 0.0112829 2.775 0.00555 ** ## education5. Advanced Degree 0.1034530 0.0122366 8.454 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(as.numeric(age)) 1.333 1.593 2.896 0.0439 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.0452 Deviance explained = 4.73% ## GCV = 0.024548 Scale est. = 0.024488 n = 3000 par(mfrow = c(1, 3)) mgcv::plot.gam( gam.lr, residuals = FALSE, all.terms = TRUE, shade = TRUE, shade.col = 2, rug = TRUE, scale = 0 ) "],["meeting-videos-6.html", "7.4 Meeting Videos", " 7.4 Meeting Videos 7.4.1 Cohort 1 Meeting chat log 00:51:58 Jon Harmon (jonthegeek): Because I&#39;m obsessed now with &quot;Why ξ?&quot;: &quot;KS&quot; for &quot;knot/spline&quot; = &quot;ks&quot; = &quot;ξ&quot;? I&#39;m going to try to use that now, at least 🙃 00:54:45 Laura Rose: 👍 01:01:55 Federica Gazzelloni: h(x,ξ) taylor : https://en.wikipedia.org/wiki/Taylors_theorem Meeting chat log 00:20:42 Federica Gazzelloni: https://en.wikipedia.org/wiki/Smoothing_spline 00:23:23 Federica Gazzelloni: lambda &gt; = 0 is a smoothing parameter, 7.4.2 Cohort 2 Meeting chat log 00:20:40 Ricardo Serrano: Orthogonal expansion in polynomial regression https://www.dropbox.com/s/j39rd74l51q2vch/Chapter12-Regression-PolynomialRegression.pdf?dl=0 00:21:05 jlsmith3: Thank you, Ricardo! 00:21:36 Federica Gazzelloni: @jenny you can have a look at the picture in this article for getting a sense of orthogonal expansion (in this case it is searching for next values that follow these paths) (https://www.researchgate.net/figure/Orthogonal-and-diagonal-node-expansion-in-the-A-search-algorithm_fig2_222666188) 00:22:26 Ricardo Serrano: 👍 00:48:51 jlsmith3: Perfect, thank you Federica! 00:52:02 Federica Gazzelloni: analysis of wage-education relationship 00:52:16 Federica Gazzelloni: brand choice 00:52:28 Jim Gruman: 👍🏼 00:52:37 Federica Gazzelloni: number of trips to a doctor&#39;s office 00:54:52 Federica Gazzelloni: Generalized additive models (GAMs) are a powerful generalization of linear, logistic, and Poisson regression models. 00:56:06 Federica Gazzelloni: should we do the lab? 00:59:16 Ricardo Serrano: Let&#39;s split the lab problems https://en.wikipedia.org/wiki/Taylors_theorem Meeting chat log 00:34:11 Ricardo Serrano: https://www.andreaperlato.com/mlpost/polynomial-regression-smoothing-splines/ 00:34:25 Ricardo Serrano: https://stats.stackexchange.com/questions/517375/splines-relationship-of-knots-degree-and-degrees-of-freedom 00:34:50 Ricardo Serrano: https://parsnip.tidymodels.org/reference/mars.html 00:35:00 Ricardo Serrano: https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/smooth.spline 00:35:10 Ricardo Serrano: https://www.youtube.com/watch?v=bESJ81dyYro 00:35:25 Ricardo Serrano: https://www.youtube.com/watch?v=Vf7oJ6z2LCc 00:36:22 Ricardo Serrano: https://emilhvitfeldt.github.io/ISLR-tidymodels-labs/moving-beyond-linearity.html 00:38:13 Federica Gazzelloni: mgcv::gam() 00:38:44 Jim Gruman: 🐕 looking for attention :) 00:44:06 Ricardo Serrano: I’m back! 00:50:57 Ricardo Serrano: lo() - loess fit in a gam model 7.4.3 Cohort 3 7.4.4 Cohort 4 Meeting chat log ADD LOG HERE "],["tree-based-methods.html", "Chapter 8 Tree-Based Methods", " Chapter 8 Tree-Based Methods Learning objectives: Use basic decision trees to model relationships between predictors and an outcome. Compare and contrast tree-based models with other model types. Use tree-based ensemble methods to build predictive models. Compare and contrast the various methods of building tree ensembles: bagging, boosting, random forests, and Bayesian Additive Regression Trees. Original script source: https://emilhvitfeldt.github.io/ISLR-tidymodels-labs/tree-based-methods.html "],["decision-tree-terminology.html", "8.1 Decision Tree Terminology", " 8.1 Decision Tree Terminology Source: https://medium.com/@scid2230/decision-tree-basics-34d864483c42 "],["decision-trees-classification-explained-statquest.html", "8.2 Decision Trees (Classification) Explained (StatQuest)", " 8.2 Decision Trees (Classification) Explained (StatQuest) "],["fitting-classification-trees.html", "8.3 8.1 Fitting Classification Trees", " 8.3 8.1 Fitting Classification Trees We will also use the Carseats data set from the ISLR package to demonstrate a classification model. ## Loading required package: rpart ## ## Attaching package: &#39;rpart&#39; ## The following object is masked from &#39;package:dials&#39;: ## ## prune Carseats ## Sales CompPrice Income Advertising Population Price ShelveLoc Age Education ## 1 9.50 138 73 11 276 120 Bad 42 17 ## 2 11.22 111 48 16 260 83 Good 65 10 ## 3 10.06 113 35 10 269 80 Medium 59 12 ## 4 7.40 117 100 4 466 97 Medium 55 14 ## 5 4.15 141 64 3 340 128 Bad 38 13 ## 6 10.81 124 113 13 501 72 Bad 78 16 ## 7 6.63 115 105 0 45 108 Medium 71 15 ## 8 11.85 136 81 15 425 120 Good 67 10 ## 9 6.54 132 110 0 108 124 Medium 76 10 ## 10 4.69 132 113 0 131 124 Medium 76 17 ## 11 9.01 121 78 9 150 100 Bad 26 10 ## 12 11.96 117 94 4 503 94 Good 50 13 ## 13 3.98 122 35 2 393 136 Medium 62 18 ## 14 10.96 115 28 11 29 86 Good 53 18 ## 15 11.17 107 117 11 148 118 Good 52 18 ## 16 8.71 149 95 5 400 144 Medium 76 18 ## 17 7.58 118 32 0 284 110 Good 63 13 ## 18 12.29 147 74 13 251 131 Good 52 10 ## 19 13.91 110 110 0 408 68 Good 46 17 ## 20 8.73 129 76 16 58 121 Medium 69 12 ## 21 6.41 125 90 2 367 131 Medium 35 18 ## 22 12.13 134 29 12 239 109 Good 62 18 ## 23 5.08 128 46 6 497 138 Medium 42 13 ## 24 5.87 121 31 0 292 109 Medium 79 10 ## 25 10.14 145 119 16 294 113 Bad 42 12 ## 26 14.90 139 32 0 176 82 Good 54 11 ## 27 8.33 107 115 11 496 131 Good 50 11 ## 28 5.27 98 118 0 19 107 Medium 64 17 ## 29 2.99 103 74 0 359 97 Bad 55 11 ## 30 7.81 104 99 15 226 102 Bad 58 17 ## 31 13.55 125 94 0 447 89 Good 30 12 ## 32 8.25 136 58 16 241 131 Medium 44 18 ## 33 6.20 107 32 12 236 137 Good 64 10 ## 34 8.77 114 38 13 317 128 Good 50 16 ## 35 2.67 115 54 0 406 128 Medium 42 17 ## 36 11.07 131 84 11 29 96 Medium 44 17 ## 37 8.89 122 76 0 270 100 Good 60 18 ## 38 4.95 121 41 5 412 110 Medium 54 10 ## 39 6.59 109 73 0 454 102 Medium 65 15 ## 40 3.24 130 60 0 144 138 Bad 38 10 ## 41 2.07 119 98 0 18 126 Bad 73 17 ## 42 7.96 157 53 0 403 124 Bad 58 16 ## 43 10.43 77 69 0 25 24 Medium 50 18 ## 44 4.12 123 42 11 16 134 Medium 59 13 ## 45 4.16 85 79 6 325 95 Medium 69 13 ## 46 4.56 141 63 0 168 135 Bad 44 12 ## 47 12.44 127 90 14 16 70 Medium 48 15 ## 48 4.38 126 98 0 173 108 Bad 55 16 ## 49 3.91 116 52 0 349 98 Bad 69 18 ## 50 10.61 157 93 0 51 149 Good 32 17 ## 51 1.42 99 32 18 341 108 Bad 80 16 ## 52 4.42 121 90 0 150 108 Bad 75 16 ## 53 7.91 153 40 3 112 129 Bad 39 18 ## 54 6.92 109 64 13 39 119 Medium 61 17 ## 55 4.90 134 103 13 25 144 Medium 76 17 ## 56 6.85 143 81 5 60 154 Medium 61 18 ## 57 11.91 133 82 0 54 84 Medium 50 17 ## 58 0.91 93 91 0 22 117 Bad 75 11 ## 59 5.42 103 93 15 188 103 Bad 74 16 ## 60 5.21 118 71 4 148 114 Medium 80 13 ## 61 8.32 122 102 19 469 123 Bad 29 13 ## 62 7.32 105 32 0 358 107 Medium 26 13 ## 63 1.82 139 45 0 146 133 Bad 77 17 ## 64 8.47 119 88 10 170 101 Medium 61 13 ## 65 7.80 100 67 12 184 104 Medium 32 16 ## 66 4.90 122 26 0 197 128 Medium 55 13 ## 67 8.85 127 92 0 508 91 Medium 56 18 ## 68 9.01 126 61 14 152 115 Medium 47 16 ## 69 13.39 149 69 20 366 134 Good 60 13 ## 70 7.99 127 59 0 339 99 Medium 65 12 ## 71 9.46 89 81 15 237 99 Good 74 12 ## 72 6.50 148 51 16 148 150 Medium 58 17 ## 73 5.52 115 45 0 432 116 Medium 25 15 ## 74 12.61 118 90 10 54 104 Good 31 11 ## 75 6.20 150 68 5 125 136 Medium 64 13 ## 76 8.55 88 111 23 480 92 Bad 36 16 ## 77 10.64 102 87 10 346 70 Medium 64 15 ## 78 7.70 118 71 12 44 89 Medium 67 18 ## 79 4.43 134 48 1 139 145 Medium 65 12 ## 80 9.14 134 67 0 286 90 Bad 41 13 ## 81 8.01 113 100 16 353 79 Bad 68 11 ## 82 7.52 116 72 0 237 128 Good 70 13 ## 83 11.62 151 83 4 325 139 Good 28 17 ## 84 4.42 109 36 7 468 94 Bad 56 11 ## 85 2.23 111 25 0 52 121 Bad 43 18 ## 86 8.47 125 103 0 304 112 Medium 49 13 ## 87 8.70 150 84 9 432 134 Medium 64 15 ## 88 11.70 131 67 7 272 126 Good 54 16 ## 89 6.56 117 42 7 144 111 Medium 62 10 ## 90 7.95 128 66 3 493 119 Medium 45 16 ## 91 5.33 115 22 0 491 103 Medium 64 11 ## 92 4.81 97 46 11 267 107 Medium 80 15 ## 93 4.53 114 113 0 97 125 Medium 29 12 ## 94 8.86 145 30 0 67 104 Medium 55 17 ## 95 8.39 115 97 5 134 84 Bad 55 11 ## 96 5.58 134 25 10 237 148 Medium 59 13 ## 97 9.48 147 42 10 407 132 Good 73 16 ## 98 7.45 161 82 5 287 129 Bad 33 16 ## 99 12.49 122 77 24 382 127 Good 36 16 ## 100 4.88 121 47 3 220 107 Bad 56 16 ## 101 4.11 113 69 11 94 106 Medium 76 12 ## 102 6.20 128 93 0 89 118 Medium 34 18 ## 103 5.30 113 22 0 57 97 Medium 65 16 ## 104 5.07 123 91 0 334 96 Bad 78 17 ## 105 4.62 121 96 0 472 138 Medium 51 12 ## 106 5.55 104 100 8 398 97 Medium 61 11 ## 107 0.16 102 33 0 217 139 Medium 70 18 ## 108 8.55 134 107 0 104 108 Medium 60 12 ## 109 3.47 107 79 2 488 103 Bad 65 16 ## 110 8.98 115 65 0 217 90 Medium 60 17 ## 111 9.00 128 62 7 125 116 Medium 43 14 ## 112 6.62 132 118 12 272 151 Medium 43 14 ## 113 6.67 116 99 5 298 125 Good 62 12 ## 114 6.01 131 29 11 335 127 Bad 33 12 ## 115 9.31 122 87 9 17 106 Medium 65 13 ## 116 8.54 139 35 0 95 129 Medium 42 13 ## 117 5.08 135 75 0 202 128 Medium 80 10 ## 118 8.80 145 53 0 507 119 Medium 41 12 ## 119 7.57 112 88 2 243 99 Medium 62 11 ## 120 7.37 130 94 8 137 128 Medium 64 12 ## 121 6.87 128 105 11 249 131 Medium 63 13 ## 122 11.67 125 89 10 380 87 Bad 28 10 ## 123 6.88 119 100 5 45 108 Medium 75 10 ## 124 8.19 127 103 0 125 155 Good 29 15 ## 125 8.87 131 113 0 181 120 Good 63 14 ## 126 9.34 89 78 0 181 49 Medium 43 15 ## 127 11.27 153 68 2 60 133 Good 59 16 ## 128 6.52 125 48 3 192 116 Medium 51 14 ## 129 4.96 133 100 3 350 126 Bad 55 13 ## 130 4.47 143 120 7 279 147 Bad 40 10 ## 131 8.41 94 84 13 497 77 Medium 51 12 ## 132 6.50 108 69 3 208 94 Medium 77 16 ## 133 9.54 125 87 9 232 136 Good 72 10 ## 134 7.62 132 98 2 265 97 Bad 62 12 ## 135 3.67 132 31 0 327 131 Medium 76 16 ## 136 6.44 96 94 14 384 120 Medium 36 18 ## 137 5.17 131 75 0 10 120 Bad 31 18 ## 138 6.52 128 42 0 436 118 Medium 80 11 ## 139 10.27 125 103 12 371 109 Medium 44 10 ## 140 12.30 146 62 10 310 94 Medium 30 13 ## 141 6.03 133 60 10 277 129 Medium 45 18 ## 142 6.53 140 42 0 331 131 Bad 28 15 ## 143 7.44 124 84 0 300 104 Medium 77 15 ## 144 0.53 122 88 7 36 159 Bad 28 17 ## 145 9.09 132 68 0 264 123 Good 34 11 ## 146 8.77 144 63 11 27 117 Medium 47 17 ## 147 3.90 114 83 0 412 131 Bad 39 14 ## 148 10.51 140 54 9 402 119 Good 41 16 ## 149 7.56 110 119 0 384 97 Medium 72 14 ## 150 11.48 121 120 13 140 87 Medium 56 11 ## 151 10.49 122 84 8 176 114 Good 57 10 ## 152 10.77 111 58 17 407 103 Good 75 17 ## 153 7.64 128 78 0 341 128 Good 45 13 ## 154 5.93 150 36 7 488 150 Medium 25 17 ## 155 6.89 129 69 10 289 110 Medium 50 16 ## 156 7.71 98 72 0 59 69 Medium 65 16 ## 157 7.49 146 34 0 220 157 Good 51 16 ## 158 10.21 121 58 8 249 90 Medium 48 13 ## 159 12.53 142 90 1 189 112 Good 39 10 ## 160 9.32 119 60 0 372 70 Bad 30 18 ## 161 4.67 111 28 0 486 111 Medium 29 12 ## 162 2.93 143 21 5 81 160 Medium 67 12 ## 163 3.63 122 74 0 424 149 Medium 51 13 ## 164 5.68 130 64 0 40 106 Bad 39 17 ## 165 8.22 148 64 0 58 141 Medium 27 13 ## 166 0.37 147 58 7 100 191 Bad 27 15 ## 167 6.71 119 67 17 151 137 Medium 55 11 ## 168 6.71 106 73 0 216 93 Medium 60 13 ## 169 7.30 129 89 0 425 117 Medium 45 10 ## 170 11.48 104 41 15 492 77 Good 73 18 ## 171 8.01 128 39 12 356 118 Medium 71 10 ## 172 12.49 93 106 12 416 55 Medium 75 15 ## 173 9.03 104 102 13 123 110 Good 35 16 ## 174 6.38 135 91 5 207 128 Medium 66 18 ## 175 0.00 139 24 0 358 185 Medium 79 15 ## 176 7.54 115 89 0 38 122 Medium 25 12 ## 177 5.61 138 107 9 480 154 Medium 47 11 ## 178 10.48 138 72 0 148 94 Medium 27 17 ## 179 10.66 104 71 14 89 81 Medium 25 14 ## 180 7.78 144 25 3 70 116 Medium 77 18 ## 181 4.94 137 112 15 434 149 Bad 66 13 ## 182 7.43 121 83 0 79 91 Medium 68 11 ## 183 4.74 137 60 4 230 140 Bad 25 13 ## 184 5.32 118 74 6 426 102 Medium 80 18 ## 185 9.95 132 33 7 35 97 Medium 60 11 ## 186 10.07 130 100 11 449 107 Medium 64 10 ## 187 8.68 120 51 0 93 86 Medium 46 17 ## 188 6.03 117 32 0 142 96 Bad 62 17 ## 189 8.07 116 37 0 426 90 Medium 76 15 ## 190 12.11 118 117 18 509 104 Medium 26 15 ## 191 8.79 130 37 13 297 101 Medium 37 13 ## 192 6.67 156 42 13 170 173 Good 74 14 ## 193 7.56 108 26 0 408 93 Medium 56 14 ## 194 13.28 139 70 7 71 96 Good 61 10 ## 195 7.23 112 98 18 481 128 Medium 45 11 ## 196 4.19 117 93 4 420 112 Bad 66 11 ## 197 4.10 130 28 6 410 133 Bad 72 16 ## 198 2.52 124 61 0 333 138 Medium 76 16 ## 199 3.62 112 80 5 500 128 Medium 69 10 ## 200 6.42 122 88 5 335 126 Medium 64 14 ## 201 5.56 144 92 0 349 146 Medium 62 12 ## 202 5.94 138 83 0 139 134 Medium 54 18 ## 203 4.10 121 78 4 413 130 Bad 46 10 ## 204 2.05 131 82 0 132 157 Bad 25 14 ## 205 8.74 155 80 0 237 124 Medium 37 14 ## 206 5.68 113 22 1 317 132 Medium 28 12 ## 207 4.97 162 67 0 27 160 Medium 77 17 ## 208 8.19 111 105 0 466 97 Bad 61 10 ## 209 7.78 86 54 0 497 64 Bad 33 12 ## 210 3.02 98 21 11 326 90 Bad 76 11 ## 211 4.36 125 41 2 357 123 Bad 47 14 ## 212 9.39 117 118 14 445 120 Medium 32 15 ## 213 12.04 145 69 19 501 105 Medium 45 11 ## 214 8.23 149 84 5 220 139 Medium 33 10 ## 215 4.83 115 115 3 48 107 Medium 73 18 ## 216 2.34 116 83 15 170 144 Bad 71 11 ## 217 5.73 141 33 0 243 144 Medium 34 17 ## 218 4.34 106 44 0 481 111 Medium 70 14 ## 219 9.70 138 61 12 156 120 Medium 25 14 ## 220 10.62 116 79 19 359 116 Good 58 17 ## 221 10.59 131 120 15 262 124 Medium 30 10 ## 222 6.43 124 44 0 125 107 Medium 80 11 ## 223 7.49 136 119 6 178 145 Medium 35 13 ## 224 3.45 110 45 9 276 125 Medium 62 14 ## 225 4.10 134 82 0 464 141 Medium 48 13 ## 226 6.68 107 25 0 412 82 Bad 36 14 ## 227 7.80 119 33 0 245 122 Good 56 14 ## 228 8.69 113 64 10 68 101 Medium 57 16 ## 229 5.40 149 73 13 381 163 Bad 26 11 ## 230 11.19 98 104 0 404 72 Medium 27 18 ## 231 5.16 115 60 0 119 114 Bad 38 14 ## 232 8.09 132 69 0 123 122 Medium 27 11 ## 233 13.14 137 80 10 24 105 Good 61 15 ## 234 8.65 123 76 18 218 120 Medium 29 14 ## 235 9.43 115 62 11 289 129 Good 56 16 ## 236 5.53 126 32 8 95 132 Medium 50 17 ## 237 9.32 141 34 16 361 108 Medium 69 10 ## 238 9.62 151 28 8 499 135 Medium 48 10 ## 239 7.36 121 24 0 200 133 Good 73 13 ## 240 3.89 123 105 0 149 118 Bad 62 16 ## 241 10.31 159 80 0 362 121 Medium 26 18 ## 242 12.01 136 63 0 160 94 Medium 38 12 ## 243 4.68 124 46 0 199 135 Medium 52 14 ## 244 7.82 124 25 13 87 110 Medium 57 10 ## 245 8.78 130 30 0 391 100 Medium 26 18 ## 246 10.00 114 43 0 199 88 Good 57 10 ## 247 6.90 120 56 20 266 90 Bad 78 18 ## 248 5.04 123 114 0 298 151 Bad 34 16 ## 249 5.36 111 52 0 12 101 Medium 61 11 ## 250 5.05 125 67 0 86 117 Bad 65 11 ## 251 9.16 137 105 10 435 156 Good 72 14 ## 252 3.72 139 111 5 310 132 Bad 62 13 ## 253 8.31 133 97 0 70 117 Medium 32 16 ## 254 5.64 124 24 5 288 122 Medium 57 12 ## 255 9.58 108 104 23 353 129 Good 37 17 ## 256 7.71 123 81 8 198 81 Bad 80 15 ## 257 4.20 147 40 0 277 144 Medium 73 10 ## 258 8.67 125 62 14 477 112 Medium 80 13 ## 259 3.47 108 38 0 251 81 Bad 72 14 ## 260 5.12 123 36 10 467 100 Bad 74 11 ## 261 7.67 129 117 8 400 101 Bad 36 10 ## 262 5.71 121 42 4 188 118 Medium 54 15 ## 263 6.37 120 77 15 86 132 Medium 48 18 ## 264 7.77 116 26 6 434 115 Medium 25 17 ## 265 6.95 128 29 5 324 159 Good 31 15 ## 266 5.31 130 35 10 402 129 Bad 39 17 ## 267 9.10 128 93 12 343 112 Good 73 17 ## 268 5.83 134 82 7 473 112 Bad 51 12 ## 269 6.53 123 57 0 66 105 Medium 39 11 ## 270 5.01 159 69 0 438 166 Medium 46 17 ## 271 11.99 119 26 0 284 89 Good 26 10 ## 272 4.55 111 56 0 504 110 Medium 62 16 ## 273 12.98 113 33 0 14 63 Good 38 12 ## 274 10.04 116 106 8 244 86 Medium 58 12 ## 275 7.22 135 93 2 67 119 Medium 34 11 ## 276 6.67 107 119 11 210 132 Medium 53 11 ## 277 6.93 135 69 14 296 130 Medium 73 15 ## 278 7.80 136 48 12 326 125 Medium 36 16 ## 279 7.22 114 113 2 129 151 Good 40 15 ## 280 3.42 141 57 13 376 158 Medium 64 18 ## 281 2.86 121 86 10 496 145 Bad 51 10 ## 282 11.19 122 69 7 303 105 Good 45 16 ## 283 7.74 150 96 0 80 154 Good 61 11 ## 284 5.36 135 110 0 112 117 Medium 80 16 ## 285 6.97 106 46 11 414 96 Bad 79 17 ## 286 7.60 146 26 11 261 131 Medium 39 10 ## 287 7.53 117 118 11 429 113 Medium 67 18 ## 288 6.88 95 44 4 208 72 Bad 44 17 ## 289 6.98 116 40 0 74 97 Medium 76 15 ## 290 8.75 143 77 25 448 156 Medium 43 17 ## 291 9.49 107 111 14 400 103 Medium 41 11 ## 292 6.64 118 70 0 106 89 Bad 39 17 ## 293 11.82 113 66 16 322 74 Good 76 15 ## 294 11.28 123 84 0 74 89 Good 59 10 ## 295 12.66 148 76 3 126 99 Good 60 11 ## 296 4.21 118 35 14 502 137 Medium 79 10 ## 297 8.21 127 44 13 160 123 Good 63 18 ## 298 3.07 118 83 13 276 104 Bad 75 10 ## 299 10.98 148 63 0 312 130 Good 63 15 ## 300 9.40 135 40 17 497 96 Medium 54 17 ## 301 8.57 116 78 1 158 99 Medium 45 11 ## 302 7.41 99 93 0 198 87 Medium 57 16 ## 303 5.28 108 77 13 388 110 Bad 74 14 ## 304 10.01 133 52 16 290 99 Medium 43 11 ## 305 11.93 123 98 12 408 134 Good 29 10 ## 306 8.03 115 29 26 394 132 Medium 33 13 ## 307 4.78 131 32 1 85 133 Medium 48 12 ## 308 5.90 138 92 0 13 120 Bad 61 12 ## 309 9.24 126 80 19 436 126 Medium 52 10 ## 310 11.18 131 111 13 33 80 Bad 68 18 ## 311 9.53 175 65 29 419 166 Medium 53 12 ## 312 6.15 146 68 12 328 132 Bad 51 14 ## 313 6.80 137 117 5 337 135 Bad 38 10 ## 314 9.33 103 81 3 491 54 Medium 66 13 ## 315 7.72 133 33 10 333 129 Good 71 14 ## 316 6.39 131 21 8 220 171 Good 29 14 ## 317 15.63 122 36 5 369 72 Good 35 10 ## 318 6.41 142 30 0 472 136 Good 80 15 ## 319 10.08 116 72 10 456 130 Good 41 14 ## 320 6.97 127 45 19 459 129 Medium 57 11 ## 321 5.86 136 70 12 171 152 Medium 44 18 ## 322 7.52 123 39 5 499 98 Medium 34 15 ## 323 9.16 140 50 10 300 139 Good 60 15 ## 324 10.36 107 105 18 428 103 Medium 34 12 ## 325 2.66 136 65 4 133 150 Bad 53 13 ## 326 11.70 144 69 11 131 104 Medium 47 11 ## 327 4.69 133 30 0 152 122 Medium 53 17 ## 328 6.23 112 38 17 316 104 Medium 80 16 ## 329 3.15 117 66 1 65 111 Bad 55 11 ## 330 11.27 100 54 9 433 89 Good 45 12 ## 331 4.99 122 59 0 501 112 Bad 32 14 ## 332 10.10 135 63 15 213 134 Medium 32 10 ## 333 5.74 106 33 20 354 104 Medium 61 12 ## 334 5.87 136 60 7 303 147 Medium 41 10 ## 335 7.63 93 117 9 489 83 Bad 42 13 ## 336 6.18 120 70 15 464 110 Medium 72 15 ## 337 5.17 138 35 6 60 143 Bad 28 18 ## 338 8.61 130 38 0 283 102 Medium 80 15 ## 339 5.97 112 24 0 164 101 Medium 45 11 ## 340 11.54 134 44 4 219 126 Good 44 15 ## 341 7.50 140 29 0 105 91 Bad 43 16 ## 342 7.38 98 120 0 268 93 Medium 72 10 ## 343 7.81 137 102 13 422 118 Medium 71 10 ## 344 5.99 117 42 10 371 121 Bad 26 14 ## 345 8.43 138 80 0 108 126 Good 70 13 ## 346 4.81 121 68 0 279 149 Good 79 12 ## 347 8.97 132 107 0 144 125 Medium 33 13 ## 348 6.88 96 39 0 161 112 Good 27 14 ## 349 12.57 132 102 20 459 107 Good 49 11 ## 350 9.32 134 27 18 467 96 Medium 49 14 ## 351 8.64 111 101 17 266 91 Medium 63 17 ## 352 10.44 124 115 16 458 105 Medium 62 16 ## 353 13.44 133 103 14 288 122 Good 61 17 ## 354 9.45 107 67 12 430 92 Medium 35 12 ## 355 5.30 133 31 1 80 145 Medium 42 18 ## 356 7.02 130 100 0 306 146 Good 42 11 ## 357 3.58 142 109 0 111 164 Good 72 12 ## 358 13.36 103 73 3 276 72 Medium 34 15 ## 359 4.17 123 96 10 71 118 Bad 69 11 ## 360 3.13 130 62 11 396 130 Bad 66 14 ## 361 8.77 118 86 7 265 114 Good 52 15 ## 362 8.68 131 25 10 183 104 Medium 56 15 ## 363 5.25 131 55 0 26 110 Bad 79 12 ## 364 10.26 111 75 1 377 108 Good 25 12 ## 365 10.50 122 21 16 488 131 Good 30 14 ## 366 6.53 154 30 0 122 162 Medium 57 17 ## 367 5.98 124 56 11 447 134 Medium 53 12 ## 368 14.37 95 106 0 256 53 Good 52 17 ## 369 10.71 109 22 10 348 79 Good 74 14 ## 370 10.26 135 100 22 463 122 Medium 36 14 ## 371 7.68 126 41 22 403 119 Bad 42 12 ## 372 9.08 152 81 0 191 126 Medium 54 16 ## 373 7.80 121 50 0 508 98 Medium 65 11 ## 374 5.58 137 71 0 402 116 Medium 78 17 ## 375 9.44 131 47 7 90 118 Medium 47 12 ## 376 7.90 132 46 4 206 124 Medium 73 11 ## 377 16.27 141 60 19 319 92 Good 44 11 ## 378 6.81 132 61 0 263 125 Medium 41 12 ## 379 6.11 133 88 3 105 119 Medium 79 12 ## 380 5.81 125 111 0 404 107 Bad 54 15 ## 381 9.64 106 64 10 17 89 Medium 68 17 ## 382 3.90 124 65 21 496 151 Bad 77 13 ## 383 4.95 121 28 19 315 121 Medium 66 14 ## 384 9.35 98 117 0 76 68 Medium 63 10 ## 385 12.85 123 37 15 348 112 Good 28 12 ## 386 5.87 131 73 13 455 132 Medium 62 17 ## 387 5.32 152 116 0 170 160 Medium 39 16 ## 388 8.67 142 73 14 238 115 Medium 73 14 ## 389 8.14 135 89 11 245 78 Bad 79 16 ## 390 8.44 128 42 8 328 107 Medium 35 12 ## 391 5.47 108 75 9 61 111 Medium 67 12 ## 392 6.10 153 63 0 49 124 Bad 56 16 ## 393 4.53 129 42 13 315 130 Bad 34 13 ## 394 5.57 109 51 10 26 120 Medium 30 17 ## 395 5.35 130 58 19 366 139 Bad 33 16 ## 396 12.57 138 108 17 203 128 Good 33 14 ## 397 6.14 139 23 3 37 120 Medium 55 11 ## 398 7.41 162 26 12 368 159 Medium 40 18 ## 399 5.94 100 79 7 284 95 Bad 50 12 ## 400 9.71 134 37 0 27 120 Good 49 16 ## Urban US ## 1 Yes Yes ## 2 Yes Yes ## 3 Yes Yes ## 4 Yes Yes ## 5 Yes No ## 6 No Yes ## 7 Yes No ## 8 Yes Yes ## 9 No No ## 10 No Yes ## 11 No Yes ## 12 Yes Yes ## 13 Yes No ## 14 Yes Yes ## 15 Yes Yes ## 16 No No ## 17 Yes No ## 18 Yes Yes ## 19 No Yes ## 20 Yes Yes ## 21 Yes Yes ## 22 No Yes ## 23 Yes No ## 24 Yes No ## 25 Yes Yes ## 26 No No ## 27 No Yes ## 28 Yes No ## 29 Yes Yes ## 30 Yes Yes ## 31 Yes No ## 32 Yes Yes ## 33 No Yes ## 34 Yes Yes ## 35 Yes Yes ## 36 No Yes ## 37 No No ## 38 Yes Yes ## 39 Yes No ## 40 No No ## 41 No No ## 42 Yes No ## 43 Yes No ## 44 Yes Yes ## 45 Yes Yes ## 46 Yes Yes ## 47 No Yes ## 48 Yes No ## 49 Yes No ## 50 Yes No ## 51 Yes Yes ## 52 Yes No ## 53 Yes Yes ## 54 Yes Yes ## 55 No Yes ## 56 Yes Yes ## 57 Yes No ## 58 Yes No ## 59 Yes Yes ## 60 Yes No ## 61 Yes Yes ## 62 No No ## 63 Yes Yes ## 64 Yes Yes ## 65 No Yes ## 66 No No ## 67 Yes No ## 68 Yes Yes ## 69 Yes Yes ## 70 Yes No ## 71 Yes Yes ## 72 No Yes ## 73 Yes No ## 74 No Yes ## 75 No Yes ## 76 No Yes ## 77 Yes Yes ## 78 No Yes ## 79 Yes Yes ## 80 Yes No ## 81 Yes Yes ## 82 Yes No ## 83 Yes Yes ## 84 Yes Yes ## 85 No No ## 86 No No ## 87 Yes No ## 88 No Yes ## 89 Yes Yes ## 90 No No ## 91 No No ## 92 Yes Yes ## 93 Yes No ## 94 Yes No ## 95 Yes Yes ## 96 Yes Yes ## 97 No Yes ## 98 Yes Yes ## 99 No Yes ## 100 No Yes ## 101 No Yes ## 102 Yes No ## 103 No No ## 104 Yes Yes ## 105 Yes No ## 106 Yes Yes ## 107 No No ## 108 Yes No ## 109 Yes No ## 110 No No ## 111 Yes Yes ## 112 Yes Yes ## 113 Yes Yes ## 114 Yes Yes ## 115 Yes Yes ## 116 Yes No ## 117 No No ## 118 Yes No ## 119 Yes Yes ## 120 Yes Yes ## 121 Yes Yes ## 122 Yes Yes ## 123 Yes Yes ## 124 No Yes ## 125 Yes No ## 126 No No ## 127 Yes Yes ## 128 Yes Yes ## 129 Yes Yes ## 130 No Yes ## 131 Yes Yes ## 132 Yes No ## 133 Yes Yes ## 134 Yes Yes ## 135 Yes No ## 136 No Yes ## 137 No No ## 138 Yes No ## 139 Yes Yes ## 140 No Yes ## 141 Yes Yes ## 142 Yes No ## 143 Yes No ## 144 Yes Yes ## 145 No No ## 146 Yes Yes ## 147 Yes No ## 148 No Yes ## 149 No Yes ## 150 Yes Yes ## 151 No Yes ## 152 No Yes ## 153 No No ## 154 No Yes ## 155 No Yes ## 156 Yes No ## 157 Yes No ## 158 No Yes ## 159 No Yes ## 160 No No ## 161 No No ## 162 No Yes ## 163 Yes No ## 164 No No ## 165 No Yes ## 166 Yes Yes ## 167 Yes Yes ## 168 Yes No ## 169 Yes No ## 170 Yes Yes ## 171 Yes Yes ## 172 Yes Yes ## 173 Yes Yes ## 174 Yes Yes ## 175 No No ## 176 Yes No ## 177 No Yes ## 178 Yes Yes ## 179 No Yes ## 180 Yes Yes ## 181 Yes Yes ## 182 Yes No ## 183 Yes No ## 184 Yes Yes ## 185 No Yes ## 186 Yes Yes ## 187 No No ## 188 Yes No ## 189 Yes No ## 190 No Yes ## 191 No Yes ## 192 Yes Yes ## 193 No No ## 194 Yes Yes ## 195 Yes Yes ## 196 Yes Yes ## 197 Yes Yes ## 198 Yes No ## 199 Yes Yes ## 200 Yes Yes ## 201 No No ## 202 Yes No ## 203 No Yes ## 204 Yes No ## 205 Yes No ## 206 Yes No ## 207 Yes Yes ## 208 No No ## 209 Yes No ## 210 No Yes ## 211 No Yes ## 212 Yes Yes ## 213 Yes Yes ## 214 Yes Yes ## 215 Yes Yes ## 216 Yes Yes ## 217 Yes No ## 218 No No ## 219 Yes Yes ## 220 Yes Yes ## 221 Yes Yes ## 222 Yes No ## 223 Yes Yes ## 224 Yes Yes ## 225 No No ## 226 Yes No ## 227 Yes No ## 228 Yes Yes ## 229 No Yes ## 230 No No ## 231 No No ## 232 No No ## 233 Yes Yes ## 234 No Yes ## 235 No Yes ## 236 Yes Yes ## 237 Yes Yes ## 238 Yes Yes ## 239 Yes No ## 240 Yes Yes ## 241 Yes No ## 242 Yes No ## 243 No No ## 244 Yes Yes ## 245 Yes No ## 246 No Yes ## 247 Yes Yes ## 248 Yes No ## 249 Yes Yes ## 250 Yes No ## 251 Yes Yes ## 252 Yes Yes ## 253 Yes No ## 254 No Yes ## 255 Yes Yes ## 256 Yes Yes ## 257 Yes No ## 258 Yes Yes ## 259 No No ## 260 No Yes ## 261 Yes Yes ## 262 Yes Yes ## 263 Yes Yes ## 264 Yes Yes ## 265 Yes Yes ## 266 Yes Yes ## 267 No Yes ## 268 No Yes ## 269 Yes No ## 270 Yes No ## 271 Yes No ## 272 Yes No ## 273 Yes No ## 274 Yes Yes ## 275 Yes Yes ## 276 Yes Yes ## 277 Yes Yes ## 278 Yes Yes ## 279 No Yes ## 280 Yes Yes ## 281 Yes Yes ## 282 No Yes ## 283 Yes No ## 284 No No ## 285 No No ## 286 Yes Yes ## 287 No Yes ## 288 Yes Yes ## 289 No No ## 290 Yes Yes ## 291 No Yes ## 292 Yes No ## 293 Yes Yes ## 294 Yes No ## 295 Yes Yes ## 296 No Yes ## 297 Yes Yes ## 298 Yes Yes ## 299 Yes No ## 300 No Yes ## 301 Yes Yes ## 302 Yes Yes ## 303 Yes Yes ## 304 Yes Yes ## 305 Yes Yes ## 306 Yes Yes ## 307 Yes Yes ## 308 Yes No ## 309 Yes Yes ## 310 Yes Yes ## 311 Yes Yes ## 312 Yes Yes ## 313 Yes Yes ## 314 Yes No ## 315 Yes Yes ## 316 Yes Yes ## 317 Yes Yes ## 318 No No ## 319 No Yes ## 320 No Yes ## 321 Yes Yes ## 322 Yes No ## 323 Yes Yes ## 324 Yes Yes ## 325 Yes Yes ## 326 Yes Yes ## 327 Yes No ## 328 Yes Yes ## 329 Yes Yes ## 330 Yes Yes ## 331 No No ## 332 Yes Yes ## 333 Yes Yes ## 334 Yes Yes ## 335 Yes Yes ## 336 Yes Yes ## 337 Yes No ## 338 Yes No ## 339 Yes No ## 340 Yes Yes ## 341 Yes No ## 342 No No ## 343 No Yes ## 344 Yes Yes ## 345 No Yes ## 346 Yes No ## 347 No No ## 348 No No ## 349 Yes Yes ## 350 No Yes ## 351 No Yes ## 352 No Yes ## 353 Yes Yes ## 354 No Yes ## 355 Yes Yes ## 356 Yes No ## 357 Yes No ## 358 Yes Yes ## 359 Yes Yes ## 360 Yes Yes ## 361 No Yes ## 362 No Yes ## 363 Yes Yes ## 364 Yes No ## 365 Yes Yes ## 366 No No ## 367 No Yes ## 368 Yes No ## 369 No Yes ## 370 Yes Yes ## 371 Yes Yes ## 372 Yes No ## 373 No No ## 374 Yes No ## 375 Yes Yes ## 376 Yes No ## 377 Yes Yes ## 378 No No ## 379 Yes Yes ## 380 Yes No ## 381 Yes Yes ## 382 Yes Yes ## 383 Yes Yes ## 384 Yes No ## 385 Yes Yes ## 386 Yes Yes ## 387 Yes No ## 388 No Yes ## 389 Yes Yes ## 390 Yes Yes ## 391 Yes Yes ## 392 Yes No ## 393 Yes Yes ## 394 No Yes ## 395 Yes Yes ## 396 Yes Yes ## 397 No Yes ## 398 Yes Yes ## 399 Yes Yes ## 400 Yes Yes skimr::skim(Carseats) (#tab:skim_dataset)Data summary Name Carseats Number of rows 400 Number of columns 11 _______________________ Column type frequency: factor 3 numeric 8 ________________________ Group variables None Variable type: factor skim_variable n_missing complete_rate ordered n_unique top_counts ShelveLoc 0 1 FALSE 3 Med: 219, Bad: 96, Goo: 85 Urban 0 1 FALSE 2 Yes: 282, No: 118 US 0 1 FALSE 2 Yes: 258, No: 142 Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist Sales 0 1 7.50 2.82 0 5.39 7.49 9.32 16.27 ▁▆▇▃▁ CompPrice 0 1 124.97 15.33 77 115.00 125.00 135.00 175.00 ▁▅▇▃▁ Income 0 1 68.66 27.99 21 42.75 69.00 91.00 120.00 ▇▆▇▆▅ Advertising 0 1 6.64 6.65 0 0.00 5.00 12.00 29.00 ▇▃▃▁▁ Population 0 1 264.84 147.38 10 139.00 272.00 398.50 509.00 ▇▇▇▇▇ Price 0 1 115.80 23.68 24 100.00 117.00 131.00 191.00 ▁▂▇▆▁ Age 0 1 53.32 16.20 25 39.75 54.50 66.00 80.00 ▇▆▇▇▇ Education 0 1 13.90 2.62 10 12.00 14.00 16.00 18.00 ▇▇▃▇▇ We create a new variable High to denote if Sales &lt;= 8, then the Sales predictor is removed as it is a perfect predictor of High. carseats &lt;- as_tibble(Carseats) %&gt;% mutate(High = factor(if_else(Sales &gt; 8, &quot;Yes&quot;, &quot;No&quot;))) %&gt;% select(-Sales) "],["exploratory-data-analysis-eda.html", "8.4 Exploratory Data Analysis (EDA)", " 8.4 Exploratory Data Analysis (EDA) Let’s count High carseats %&gt;% count(High) ## # A tibble: 2 × 2 ## High n ## &lt;fct&gt; &lt;int&gt; ## 1 No 236 ## 2 Yes 164 High plot carseats %&gt;% ggplot(aes(High, fill = High)) + geom_bar() + theme(legend.position = &#39;none&#39;) "],["correlation-analysis.html", "8.5 Correlation Analysis", " 8.5 Correlation Analysis Correlation heatmap (Pearson) # convert factor features to numeric for correlation analysis carseats_num &lt;- carseats %&gt;% mutate(High = ifelse(High == &quot;No&quot;, 0 , 1), Urban = ifelse(Urban == &quot;No&quot;, 0, 1), US = ifelse(US == &quot;No&quot;, 0, 1), ShelveLoc = case_when( ShelveLoc == &#39;Bad&#39; ~ 1, ShelveLoc == &quot;Medium&quot; ~ 2, TRUE ~ 3 )) carseats_num ## # A tibble: 400 × 11 ## CompPrice Income Advertising Population Price ShelveLoc Age Education Urban ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 138 73 11 276 120 1 42 17 1 ## 2 111 48 16 260 83 3 65 10 1 ## 3 113 35 10 269 80 2 59 12 1 ## 4 117 100 4 466 97 2 55 14 1 ## 5 141 64 3 340 128 1 38 13 1 ## 6 124 113 13 501 72 1 78 16 0 ## 7 115 105 0 45 108 2 71 15 1 ## 8 136 81 15 425 120 3 67 10 1 ## 9 132 110 0 108 124 2 76 10 0 ## 10 132 113 0 131 124 2 76 17 0 ## # … with 390 more rows, and 2 more variables: US &lt;dbl&gt;, High &lt;dbl&gt; library(dlookr) carseats_num %&gt;% correlate() %&gt;% plot() Correlation heatmap (Spearman) carseats_num %&gt;% correlate(method = &quot;spearman&quot;) %&gt;% plot() "],["build-a-model.html", "8.6 Build a model", " 8.6 Build a model Split dataset into train/test set.seed(1234) carseats_split &lt;- initial_split(carseats, prop = 0.75, strata = High) carseats_train &lt;- training(carseats_split) carseats_test &lt;- testing(carseats_split) Create decision tree classification spec class_tree_spec &lt;- decision_tree() %&gt;% set_engine(&quot;rpart&quot;) %&gt;% set_mode(&quot;classification&quot;) Fit the decision tree model class_tree_fit &lt;- fit(class_tree_spec, High ~ ., data = carseats_train) "],["visualize-our-decision-tree.html", "8.7 Visualize our decision tree", " 8.7 Visualize our decision tree class_tree_fit %&gt;% extract_fit_engine() %&gt;% rpart.plot(roundint = FALSE) "],["evaluate-the-model.html", "8.8 Evaluate the model", " 8.8 Evaluate the model Confusion matrix (train) augment(class_tree_fit, new_data = carseats_train) %&gt;% conf_mat(truth = High, estimate = .pred_class) ## Truth ## Prediction No Yes ## No 159 17 ## Yes 18 106 augment(class_tree_fit, new_data = carseats_train) %&gt;% accuracy(truth = High, estimate = .pred_class) ## # A tibble: 1 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 accuracy binary 0.883 Training accuracy: 88.3% Confusion matrix (test) augment(class_tree_fit, new_data = carseats_test) %&gt;% conf_mat(truth = High, estimate = .pred_class) ## Truth ## Prediction No Yes ## No 39 7 ## Yes 20 34 augment(class_tree_fit, new_data = carseats_test) %&gt;% accuracy(truth = High, estimate = .pred_class) ## # A tibble: 1 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 accuracy binary 0.73 Testing accuracy: 73% (overfit) "],["tuning-the-model.html", "8.9 Tuning the model", " 8.9 Tuning the model Let’s try to tune the cost_complexity of the decision tree to find a more optimal complexity. We use the class_tree_spec object and use the set_args() function to specify that we want to tune cost_complexity. This is then passed directly into the workflow object to avoid creating an intermediate object. Also, since the dataset has 400 observations (rows), we’ll apply boostrapping to increase the sample number in each fold. WARNING: Bootstraps resample number has a direct relationship with execution time. For academic purposes, a value of 100 is used. However, in a development stage, the greater the number of resamples, the greater the statistical significance of the model. set.seed(1234) bootstraps_samples &lt;- 100 carseats_boot &lt;- bootstraps(carseats_train, times = bootstraps_samples, apparent = TRUE, strata = High) carseats_boot ## # Bootstrap sampling using stratification with apparent sample ## # A tibble: 101 × 2 ## splits id ## &lt;list&gt; &lt;chr&gt; ## 1 &lt;split [300/117]&gt; Bootstrap001 ## 2 &lt;split [300/110]&gt; Bootstrap002 ## 3 &lt;split [300/111]&gt; Bootstrap003 ## 4 &lt;split [300/104]&gt; Bootstrap004 ## 5 &lt;split [300/107]&gt; Bootstrap005 ## 6 &lt;split [300/107]&gt; Bootstrap006 ## 7 &lt;split [300/112]&gt; Bootstrap007 ## 8 &lt;split [300/117]&gt; Bootstrap008 ## 9 &lt;split [300/107]&gt; Bootstrap009 ## 10 &lt;split [300/117]&gt; Bootstrap010 ## # … with 91 more rows To be able to tune the variable we need 2 more objects. With the resamples object, we will use a k-fold bootstrap data set, and a grid of values to try. Since we are only tuning 2 hyperparameters it is fine to stay with a regular grid. tree_spec &lt;- decision_tree( cost_complexity = tune(), tree_depth = tune(), min_n = tune() ) %&gt;% set_engine(&quot;rpart&quot;) %&gt;% set_mode(&quot;classification&quot;) Setup parallel processing —- ## [1] 9 tree_grid &lt;- grid_regular(cost_complexity(range = c(-4, -1)), tree_depth(range = c(3, 7)), min_n(range = c(10, 20)), levels = 5 ) # set.seed(2001) # tune_res &lt;- tune_grid( # tree_spec, # High ~ ., # resamples = carseats_boot, # grid = tree_grid, # metrics = metric_set(accuracy) # ) # save tune_res # write_rds(tune_res, &#39;data/08_tree_tune_grid_results.rds&#39;) tune_res &lt;- read_rds(&#39;data/08_tree_tune_grid_results.rds&#39;) tune_res ## # Tuning results ## # Bootstrap sampling using stratification with apparent sample ## # A tibble: 101 × 4 ## splits id .metrics .notes ## &lt;list&gt; &lt;chr&gt; &lt;list&gt; &lt;list&gt; ## 1 &lt;split [300/300]&gt; Apparent &lt;tibble [125 × 7]&gt; &lt;tibble [0 × 3]&gt; ## 2 &lt;split [300/117]&gt; Bootstrap001 &lt;tibble [125 × 7]&gt; &lt;tibble [0 × 3]&gt; ## 3 &lt;split [300/110]&gt; Bootstrap002 &lt;tibble [125 × 7]&gt; &lt;tibble [0 × 3]&gt; ## 4 &lt;split [300/111]&gt; Bootstrap003 &lt;tibble [125 × 7]&gt; &lt;tibble [0 × 3]&gt; ## 5 &lt;split [300/104]&gt; Bootstrap004 &lt;tibble [125 × 7]&gt; &lt;tibble [0 × 3]&gt; ## 6 &lt;split [300/107]&gt; Bootstrap005 &lt;tibble [125 × 7]&gt; &lt;tibble [0 × 3]&gt; ## 7 &lt;split [300/107]&gt; Bootstrap006 &lt;tibble [125 × 7]&gt; &lt;tibble [0 × 3]&gt; ## 8 &lt;split [300/112]&gt; Bootstrap007 &lt;tibble [125 × 7]&gt; &lt;tibble [0 × 3]&gt; ## 9 &lt;split [300/117]&gt; Bootstrap008 &lt;tibble [125 × 7]&gt; &lt;tibble [0 × 3]&gt; ## 10 &lt;split [300/107]&gt; Bootstrap009 &lt;tibble [125 × 7]&gt; &lt;tibble [0 × 3]&gt; ## # … with 91 more rows "],["evaluate-the-model-1.html", "8.10 Evaluate the model", " 8.10 Evaluate the model tune_res %&gt;% collect_metrics() ## # A tibble: 125 × 9 ## cost_complexity tree_depth min_n .metric .estimator mean n std_err ## &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 0.0001 3 10 accuracy binary 0.716 101 0.00374 ## 2 0.000562 3 10 accuracy binary 0.716 101 0.00374 ## 3 0.00316 3 10 accuracy binary 0.716 101 0.00374 ## 4 0.0178 3 10 accuracy binary 0.715 101 0.00365 ## 5 0.1 3 10 accuracy binary 0.697 101 0.00474 ## 6 0.0001 4 10 accuracy binary 0.718 101 0.00449 ## 7 0.000562 4 10 accuracy binary 0.718 101 0.00449 ## 8 0.00316 4 10 accuracy binary 0.718 101 0.00449 ## 9 0.0178 4 10 accuracy binary 0.724 101 0.00405 ## 10 0.1 4 10 accuracy binary 0.697 101 0.00471 ## # … with 115 more rows, and 1 more variable: .config &lt;chr&gt; Using autoplot() shows which values of cost_complexity appear to produce the highest accuracy. autoplot(tune_res) We can now select the best performing model with select_best(), finalize the workflow by updating the value of cost_complexity, tree_depth, and min_n and fit the model on the full training data set. # select best model best_model &lt;- select_best(tune_res) # fit model with best model hyperparameters class_tree_final &lt;- finalize_model(tree_spec, best_model) # refit training dataset with best model hyperparameters class_tree_final_fit &lt;- fit(class_tree_final, High ~ ., data = carseats_train) class_tree_final_fit ## parsnip model object ## ## n= 300 ## ## node), split, n, loss, yval, (yprob) ## * denotes terminal node ## ## 1) root 300 123 No (0.59000000 0.41000000) ## 2) ShelveLoc=Bad,Medium 237 73 No (0.69198312 0.30801688) ## 4) Advertising&lt; 13.5 203 50 No (0.75369458 0.24630542) ## 8) Price&gt;=109.5 120 14 No (0.88333333 0.11666667) * ## 9) Price&lt; 109.5 83 36 No (0.56626506 0.43373494) ## 18) CompPrice&lt; 124.5 65 20 No (0.69230769 0.30769231) ## 36) Price&gt;=76.5 55 12 No (0.78181818 0.21818182) ## 72) Age&gt;=61.5 30 2 No (0.93333333 0.06666667) * ## 73) Age&lt; 61.5 25 10 No (0.60000000 0.40000000) ## 146) CompPrice&lt; 110 12 1 No (0.91666667 0.08333333) * ## 147) CompPrice&gt;=110 13 4 Yes (0.30769231 0.69230769) * ## 37) Price&lt; 76.5 10 2 Yes (0.20000000 0.80000000) * ## 19) CompPrice&gt;=124.5 18 2 Yes (0.11111111 0.88888889) * ## 5) Advertising&gt;=13.5 34 11 Yes (0.32352941 0.67647059) ## 10) Age&gt;=70.5 7 1 No (0.85714286 0.14285714) * ## 11) Age&lt; 70.5 27 5 Yes (0.18518519 0.81481481) * ## 3) ShelveLoc=Good 63 13 Yes (0.20634921 0.79365079) ## 6) Price&gt;=135 15 6 No (0.60000000 0.40000000) * ## 7) Price&lt; 135 48 4 Yes (0.08333333 0.91666667) * "],["visualize-the-tuned-decision-tree-classification.html", "8.11 Visualize the tuned decision tree (classification)", " 8.11 Visualize the tuned decision tree (classification) At last, we can visualize the model, and we see that the better-performing model is less complex than the original model we fit. class_tree_final_fit %&gt;% extract_fit_engine() %&gt;% rpart.plot(roundint = FALSE) "],["variable-importance.html", "8.12 Variable importance", " 8.12 Variable importance The broomstick package (https://github.com/njtierney/broomstick/) enables the analyst to extract the decision tree variable importance from the fitted model. library(forcats) broomstick::tidy(class_tree_final_fit$fit) %&gt;% mutate(variable = variable %&gt;% as_factor() %&gt;% fct_rev()) %&gt;% ggplot(aes(y = variable, x = importance)) + geom_col(fill = &quot;steelblue&quot;) "],["final-evaluation.html", "8.13 Final evaluation", " 8.13 Final evaluation Confusion matrix (train, best model) augment(class_tree_final_fit, new_data = carseats_train) %&gt;% conf_mat(truth = High, estimate = .pred_class) ## Truth ## Prediction No Yes ## No 160 24 ## Yes 17 99 augment(class_tree_final_fit, new_data = carseats_train) %&gt;% accuracy(truth = High, estimate = .pred_class) ## # A tibble: 1 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 accuracy binary 0.863 Training accuracy: 86.3% Confusion matrix (test, best model) augment(class_tree_final_fit, new_data = carseats_test) %&gt;% conf_mat(truth = High, estimate = .pred_class) ## Truth ## Prediction No Yes ## No 39 9 ## Yes 20 32 augment(class_tree_final_fit, new_data = carseats_test) %&gt;% accuracy(truth = High, estimate = .pred_class) ## # A tibble: 1 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 accuracy binary 0.71 Testing accuracy: 71% "],["fitting-regression-trees.html", "8.14 8.2 - Fitting Regression Trees", " 8.14 8.2 - Fitting Regression Trees We will now show how we fit a regression tree. This is very similar to what we saw in the last section. The main difference here is that the response we are looking at will be continuous instead of categorical. "],["decision-trees-regression-explained-statquest.html", "8.15 Decision Trees (Regression) Explained (StatQuest)", " 8.15 Decision Trees (Regression) Explained (StatQuest) 8.15.1 EDA Let’s plot a histogram for Sales (target) Carseats %&gt;% ggplot(aes(Sales)) + geom_histogram(fill = &quot;steelblue&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. "],["correlation-analysis-1.html", "8.16 Correlation Analysis", " 8.16 Correlation Analysis Pearson correlation Carseats %&gt;% mutate(Urban = ifelse(Urban == &quot;No&quot;, 0, 1), US = ifelse(US == &quot;No&quot;, 0, 1), ShelveLoc = case_when( ShelveLoc == &#39;Bad&#39; ~ 1, ShelveLoc == &quot;Medium&quot; ~ 2, TRUE ~ 3) ) %&gt;% correlate() %&gt;% plot() "],["build-a-regression-tree.html", "8.17 Build a regression tree", " 8.17 Build a regression tree We can reuse class_tree_spec as a base for the regression decision tree specification. reg_tree_spec &lt;- class_tree_spec %&gt;% set_mode(&quot;regression&quot;) We are using the Carseats dataset. Let’s do the validation split. set.seed(1010) carseats_split &lt;- initial_split(Carseats) carseats_train &lt;- training(carseats_split) carseats_test &lt;- testing(carseats_split) Fit the decision tree regression model reg_tree_fit &lt;- fit(reg_tree_spec, Sales ~ ., data = carseats_train) reg_tree_fit ## parsnip model object ## ## n= 300 ## ## node), split, n, deviance, yval ## * denotes terminal node ## ## 1) root 300 2529.534000 7.469333 ## 2) ShelveLoc=Bad,Medium 235 1476.666000 6.695702 ## 4) Price&gt;=124.5 86 393.091600 5.283721 ## 8) CompPrice&lt; 147.5 72 303.087600 4.916528 ## 16) Price&gt;=137.5 27 99.462070 3.735556 * ## 17) Price&lt; 137.5 45 143.374700 5.625111 ## 34) Advertising&lt; 14.5 38 99.190880 5.249211 * ## 35) Advertising&gt;=14.5 7 9.665971 7.665714 * ## 9) CompPrice&gt;=147.5 14 30.370240 7.172143 * ## 5) Price&lt; 124.5 149 813.155300 7.510671 ## 10) Age&gt;=50.5 91 411.524700 6.751758 ## 20) Price&gt;=86.5 80 283.338300 6.353750 ## 40) CompPrice&lt; 123.5 52 157.669400 5.689231 ## 80) Price&gt;=102.5 24 52.750780 4.710833 ## 160) ShelveLoc=Bad 8 12.693000 3.120000 * ## 161) ShelveLoc=Medium 16 9.688775 5.506250 * ## 81) Price&lt; 102.5 28 62.252070 6.527857 * ## 41) CompPrice&gt;=123.5 28 60.061870 7.587857 * ## 21) Price&lt; 86.5 11 23.347450 9.646364 * ## 11) Age&lt; 50.5 58 266.987700 8.701379 ## 22) Income&lt; 59.5 18 65.389180 7.101111 * ## 23) Income&gt;=59.5 40 134.760100 9.421500 * ## 3) ShelveLoc=Good 65 403.719500 10.266310 ## 6) Price&gt;=109.5 42 197.649800 9.155238 ## 12) Price&gt;=142.5 12 36.647220 7.152500 * ## 13) Price&lt; 142.5 30 93.618500 9.956333 ## 26) Age&gt;=61.5 9 17.323960 8.537778 * ## 27) Age&lt; 61.5 21 50.422110 10.564290 ## 54) CompPrice&lt; 129.5 11 14.599670 9.515455 * ## 55) CompPrice&gt;=129.5 10 10.411360 11.718000 * ## 7) Price&lt; 109.5 23 59.542770 12.295220 * "],["visualize-our-decision-tree-1.html", "8.18 Visualize our decision tree", " 8.18 Visualize our decision tree reg_tree_fit %&gt;% extract_fit_engine() %&gt;% rpart.plot(roundint = FALSE) "],["evaluate-the-model-2.html", "8.19 Evaluate the model", " 8.19 Evaluate the model Collect metrics using augment augment(reg_tree_fit, new_data = carseats_train) %&gt;% rmse(truth = Sales, estimate = .pred) ## # A tibble: 1 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 1.58 augment(reg_tree_fit, new_data = carseats_test) %&gt;% rmse(truth = Sales, estimate = .pred) ## # A tibble: 1 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 2.18 Training RMSE: 1.58 Testing RMSE: 2.18 (overfit) "],["tuning-the-regression-model.html", "8.20 Tuning the regression model", " 8.20 Tuning the regression model Now let us again try to tune the cost_complexity to find the best performing model. reg_tree_wf &lt;- workflow() %&gt;% add_model(reg_tree_spec %&gt;% set_args(cost_complexity = tune())) %&gt;% add_formula(Sales ~ .) Create the bootstrap folds. set.seed(4321) # bootstraps_samples are defined in line 158 carseats_boot &lt;- bootstraps(carseats_train, times = bootstraps_samples, apparent = TRUE) carseats_boot ## # Bootstrap sampling with apparent sample ## # A tibble: 101 × 2 ## splits id ## &lt;list&gt; &lt;chr&gt; ## 1 &lt;split [300/111]&gt; Bootstrap001 ## 2 &lt;split [300/111]&gt; Bootstrap002 ## 3 &lt;split [300/102]&gt; Bootstrap003 ## 4 &lt;split [300/112]&gt; Bootstrap004 ## 5 &lt;split [300/109]&gt; Bootstrap005 ## 6 &lt;split [300/113]&gt; Bootstrap006 ## 7 &lt;split [300/128]&gt; Bootstrap007 ## 8 &lt;split [300/118]&gt; Bootstrap008 ## 9 &lt;split [300/107]&gt; Bootstrap009 ## 10 &lt;split [300/115]&gt; Bootstrap010 ## # … with 91 more rows Create the tuning grid. param_grid &lt;- grid_regular(cost_complexity(range = c(-5, -1)), levels = 10) tune_res &lt;- tune_grid( reg_tree_wf, resamples = carseats_boot, grid = param_grid ) tune_res ## # Tuning results ## # Bootstrap sampling with apparent sample ## # A tibble: 101 × 4 ## splits id .metrics .notes ## &lt;list&gt; &lt;chr&gt; &lt;list&gt; &lt;list&gt; ## 1 &lt;split [300/300]&gt; Apparent &lt;tibble [20 × 5]&gt; &lt;tibble [0 × 3]&gt; ## 2 &lt;split [300/111]&gt; Bootstrap001 &lt;tibble [20 × 5]&gt; &lt;tibble [0 × 3]&gt; ## 3 &lt;split [300/111]&gt; Bootstrap002 &lt;tibble [20 × 5]&gt; &lt;tibble [0 × 3]&gt; ## 4 &lt;split [300/102]&gt; Bootstrap003 &lt;tibble [20 × 5]&gt; &lt;tibble [0 × 3]&gt; ## 5 &lt;split [300/112]&gt; Bootstrap004 &lt;tibble [20 × 5]&gt; &lt;tibble [0 × 3]&gt; ## 6 &lt;split [300/109]&gt; Bootstrap005 &lt;tibble [20 × 5]&gt; &lt;tibble [0 × 3]&gt; ## 7 &lt;split [300/113]&gt; Bootstrap006 &lt;tibble [20 × 5]&gt; &lt;tibble [0 × 3]&gt; ## 8 &lt;split [300/128]&gt; Bootstrap007 &lt;tibble [20 × 5]&gt; &lt;tibble [0 × 3]&gt; ## 9 &lt;split [300/118]&gt; Bootstrap008 &lt;tibble [20 × 5]&gt; &lt;tibble [0 × 3]&gt; ## 10 &lt;split [300/107]&gt; Bootstrap009 &lt;tibble [20 × 5]&gt; &lt;tibble [0 × 3]&gt; ## # … with 91 more rows "],["evaluate-the-model-3.html", "8.21 Evaluate the model", " 8.21 Evaluate the model It appears that higher complexity works are to be preferred according to our cross-validation. autoplot(tune_res) We select the best-performing model according to \"rmse\" and fit the final model on the whole training data set. best_complexity &lt;- select_best(tune_res, metric = &quot;rmse&quot;) reg_tree_final &lt;- finalize_workflow(reg_tree_wf, best_complexity) reg_tree_final_fit &lt;- fit(reg_tree_final, data = carseats_train) reg_tree_final_fit ## ══ Workflow [trained] ══════════════════════════════════════════════════════════ ## Preprocessor: Formula ## Model: decision_tree() ## ## ── Preprocessor ──────────────────────────────────────────────────────────────── ## Sales ~ . ## ## ── Model ─────────────────────────────────────────────────────────────────────── ## n= 300 ## ## node), split, n, deviance, yval ## * denotes terminal node ## ## 1) root 300 2529.534000 7.469333 ## 2) ShelveLoc=Bad,Medium 235 1476.666000 6.695702 ## 4) Price&gt;=124.5 86 393.091600 5.283721 ## 8) CompPrice&lt; 147.5 72 303.087600 4.916528 ## 16) Price&gt;=137.5 27 99.462070 3.735556 ## 32) Income&lt; 90 20 74.584220 3.207000 ## 64) Population&lt; 223.5 9 33.593560 2.347778 * ## 65) Population&gt;=223.5 11 28.910000 3.910000 * ## 33) Income&gt;=90 7 3.326371 5.245714 * ## 17) Price&lt; 137.5 45 143.374700 5.625111 ## 34) Advertising&lt; 14.5 38 99.190880 5.249211 ## 68) Population&gt;=380 11 11.129290 4.260909 * ## 69) Population&lt; 380 27 72.940210 5.651852 ## 138) Age&gt;=43 17 36.912490 5.160588 * ## 139) Age&lt; 43 10 24.950210 6.487000 * ## 35) Advertising&gt;=14.5 7 9.665971 7.665714 * ## 9) CompPrice&gt;=147.5 14 30.370240 7.172143 * ## 5) Price&lt; 124.5 149 813.155300 7.510671 ## 10) Age&gt;=50.5 91 411.524700 6.751758 ## 20) Price&gt;=86.5 80 283.338300 6.353750 ## 40) CompPrice&lt; 123.5 52 157.669400 5.689231 ## 80) Price&gt;=102.5 24 52.750780 4.710833 ## 160) ShelveLoc=Bad 8 12.693000 3.120000 * ## 161) ShelveLoc=Medium 16 9.688775 5.506250 * ## 81) Price&lt; 102.5 28 62.252070 6.527857 ## 162) ShelveLoc=Bad 10 30.689160 5.652000 * ## 163) ShelveLoc=Medium 18 19.629840 7.014444 * ## 41) CompPrice&gt;=123.5 28 60.061870 7.587857 ## 82) Advertising&lt; 12.5 21 37.242320 7.114762 ## 164) Price&gt;=110 13 14.217510 6.506154 * ## 165) Price&lt; 110 8 10.384790 8.103750 * ## 83) Advertising&gt;=12.5 7 4.018743 9.007143 * ## 21) Price&lt; 86.5 11 23.347450 9.646364 * ## 11) Age&lt; 50.5 58 266.987700 8.701379 ## 22) Income&lt; 59.5 18 65.389180 7.101111 * ## 23) Income&gt;=59.5 40 134.760100 9.421500 ## 46) Price&gt;=105.5 19 36.968860 8.594211 * ## 47) Price&lt; 105.5 21 73.022200 10.170000 ## 94) ShelveLoc=Bad 7 15.859600 8.660000 * ## 95) ShelveLoc=Medium 14 33.221550 10.925000 * ## 3) ShelveLoc=Good 65 403.719500 10.266310 ## 6) Price&gt;=109.5 42 197.649800 9.155238 ## 12) Price&gt;=142.5 12 36.647220 7.152500 * ## 13) Price&lt; 142.5 30 93.618500 9.956333 ## 26) Age&gt;=61.5 9 17.323960 8.537778 * ## ## ... ## and 6 more lines. "],["visualize-the-tuned-decision-tree-regression.html", "8.22 Visualize the tuned decision tree (regression)", " 8.22 Visualize the tuned decision tree (regression) reg_tree_final_fit %&gt;% extract_fit_engine() %&gt;% rpart.plot(roundint = FALSE) "],["variable-importance-1.html", "8.23 Variable importance", " 8.23 Variable importance The broomstick package (https://github.com/njtierney/broomstick/) enables the analyst to extract the decision tree variable importance from the fitted model. broomstick::tidy(reg_tree_final_fit$fit$fit) %&gt;% mutate(variable = variable %&gt;% as_factor() %&gt;% fct_rev()) %&gt;% ggplot(aes(y = variable, x = importance)) + geom_col(fill = &quot;steelblue&quot;) "],["final-evaluation-1.html", "8.24 Final evaluation", " 8.24 Final evaluation Collect tuned metrics using augment augment(reg_tree_final_fit, new_data = carseats_train) %&gt;% rmse(truth = Sales, estimate = .pred) ## # A tibble: 1 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 1.38 augment(reg_tree_final_fit, new_data = carseats_test) %&gt;% rmse(truth = Sales, estimate = .pred) ## # A tibble: 1 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 2.04 Training RMSE: 1.38 Testing RMSE: 2.04 (still overfitting!) "],["bagging-and-random-forests.html", "8.25 8.3 - Bagging and Random Forests", " 8.25 8.3 - Bagging and Random Forests "],["random-forest-diagram.html", "8.26 Random Forest Diagram", " 8.26 Random Forest Diagram Source: https://en.wikipedia.org/wiki/Random_forest#/media/File:Random_forest_diagram_complete.png "],["example.html", "8.27 Example", " 8.27 Example Here, we apply bagging and random forests to the Carseats data set. We will be using the randomForest package as the engine. A bagging model is the same as a random forest where mtry is equal to the number of predictors. We can specify the mtry to be .cols() which means that the number of columns in the predictor matrix is used. This is useful if you want to make the specification more general and usable to many different data sets. .cols() is one of many descriptors in the parsnip package. We also set importance = TRUE in set_engine() to tell the engine to save the information regarding variable importance. This is needed for this engine if we want to use the vip package later. For a more detailed explanation of bagging and its counterpart boosting, read this link bagging_spec &lt;- rand_forest(mtry = .cols()) %&gt;% set_engine(&quot;randomForest&quot;, importance = TRUE) %&gt;% set_mode(&quot;regression&quot;) Fit the model. bagging_fit &lt;- fit(bagging_spec, Sales ~ ., data = carseats_train) "],["evaluate-the-model-4.html", "8.28 Evaluate the model", " 8.28 Evaluate the model … and we take a look at the testing performance (notice an improvement over the decision tree). augment(bagging_fit, new_data = carseats_train) %&gt;% rmse(truth = Sales, estimate = .pred) ## # A tibble: 1 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 0.671 augment(bagging_fit, new_data = carseats_test) %&gt;% rmse(truth = Sales, estimate = .pred) ## # A tibble: 1 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 1.35 Training RMSE: 0.671 Testing RMSE: 1.35 (overfit) We can also create a quick scatterplot between the true and predicted value to see if we can make any diagnostics. augment(bagging_fit, new_data = carseats_test) %&gt;% ggplot(aes(Sales, .pred)) + geom_abline() + geom_point(alpha = 0.5) "],["variable-importance-2.html", "8.29 Variable importance", " 8.29 Variable importance vip(bagging_fit) "],["random-forest-using-a-set-of-features-mtry.html", "8.30 Random Forest using a set of features (mtry)", " 8.30 Random Forest using a set of features (mtry) By default, randomForest() p / 3 variables when building a random forest of regression trees, and sqrt(p) variables when building a random forest of classification trees. Here we use mtry = 6, trees = 2000 and min_n = 10. rf_spec &lt;- rand_forest(mtry = 6, trees = 2000, min_n = 10) %&gt;% set_engine(&quot;randomForest&quot;, importance = TRUE) %&gt;% set_mode(&quot;regression&quot;) Fit the model rf_fit &lt;- fit(rf_spec, Sales ~ ., data = carseats_train) "],["evaluate-the-model-5.html", "8.31 Evaluate the model", " 8.31 Evaluate the model This model has similar performance compared to the bagging model. augment(rf_fit, new_data = carseats_train) %&gt;% rmse(truth = Sales, estimate = .pred) ## # A tibble: 1 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 0.858 augment(rf_fit, new_data = carseats_test) %&gt;% rmse(truth = Sales, estimate = .pred) ## # A tibble: 1 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 1.36 Training RMSE: 0.858 Testing RMSE: 1.36 (still overfitting) We can likewise plot the true value against the predicted value. augment(rf_fit, new_data = carseats_test) %&gt;% ggplot(aes(Sales, .pred)) + geom_abline() + geom_point(alpha = 0.5) "],["variable-importance-3.html", "8.32 Variable importance", " 8.32 Variable importance vip(rf_fit) "],["boosting.html", "8.33 Boosting", " 8.33 Boosting We will now fit a boosted tree model. The xgboost package has a good implementation of boosted trees. It has many parameters to tune and we know that setting trees too high can lead to overfitting. Nevertheless, let us try fitting a boosted tree. We set tree = 5000 to grow 5000 trees with a maximal depth of 4 by setting tree_depth = 4. boost_spec &lt;- boost_tree(trees = 5000, tree_depth = 4) %&gt;% set_engine(&quot;xgboost&quot;) %&gt;% set_mode(&quot;regression&quot;) Fit the model. boost_fit &lt;- fit(boost_spec, Sales ~ ., data = carseats_train) "],["evaluate-the-model-6.html", "8.34 Evaluate the model", " 8.34 Evaluate the model … and the rmse is a little high in this case which is properly because we didn’t tune any of the parameters. augment(boost_fit, new_data = carseats_train) %&gt;% rmse(truth = Sales, estimate = .pred) ## # A tibble: 1 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 0.00162 augment(boost_fit, new_data = carseats_test) %&gt;% rmse(truth = Sales, estimate = .pred) ## # A tibble: 1 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 1.38 Training RMSE: 0.00162 Testing RMSE: 1.38 (definitively overfitting) "],["tuning-the-xgboost-regression-model.html", "8.35 Tuning the xgboost regression model", " 8.35 Tuning the xgboost regression model We are using the Carseats dataset. Let’s do the validation split with a different seed. set.seed(1001) carseats_split &lt;- initial_split(Carseats) carseats_train &lt;- training(carseats_split) carseats_test &lt;- testing(carseats_split) Create the bootstrap folds. set.seed(2341) carseats_boot &lt;- bootstraps(carseats_train, times = bootstraps_samples, apparent = TRUE, strata = Sales) carseats_boot ## # Bootstrap sampling using stratification with apparent sample ## # A tibble: 101 × 2 ## splits id ## &lt;list&gt; &lt;chr&gt; ## 1 &lt;split [300/115]&gt; Bootstrap001 ## 2 &lt;split [300/107]&gt; Bootstrap002 ## 3 &lt;split [300/116]&gt; Bootstrap003 ## 4 &lt;split [300/109]&gt; Bootstrap004 ## 5 &lt;split [300/106]&gt; Bootstrap005 ## 6 &lt;split [300/97]&gt; Bootstrap006 ## 7 &lt;split [300/104]&gt; Bootstrap007 ## 8 &lt;split [300/99]&gt; Bootstrap008 ## 9 &lt;split [300/108]&gt; Bootstrap009 ## 10 &lt;split [300/103]&gt; Bootstrap010 ## # … with 91 more rows Model spec xgb_spec &lt;- boost_tree( trees = 2000, mtry = tune(), min_n = tune(), tree_depth = tune(), learn_rate = tune() ) %&gt;% set_engine(&quot;xgboost&quot;) %&gt;% set_mode(&quot;regression&quot;) Create the workflow() xgb_wf &lt;- workflow() %&gt;% add_model(xgb_spec) %&gt;% add_formula(Sales ~ .) "],["grid-tuning-with-finetunerace_anova.html", "8.36 Grid tuning with finetune::race_anova()", " 8.36 Grid tuning with finetune::race_anova() Tune the xgboost model with race_anova() to accelerate the tuning speed. # library(finetune) # # set.seed(4242) # # tictoc::tic() # xgb_rs &lt;- # tune_race_anova( # xgb_wf, # carseats_boot, # grid = 30, # control = control_race(verbose_elim = TRUE) # ) # tictoc::toc() # save xgb_rs # write_rds(xgb_rs, &#39;data/08_boost_tree_racing_grid_results.rds&#39;) xgb_rs &lt;- read_rds(&#39;data/08_boost_tree_racing_grid_results.rds&#39;) xgb_rs ## # Tuning results ## # Bootstrap sampling using stratification with apparent sample ## # A tibble: 101 × 5 ## splits id .order .metrics .notes ## &lt;list&gt; &lt;chr&gt; &lt;int&gt; &lt;list&gt; &lt;list&gt; ## 1 &lt;split [300/113]&gt; Bootstrap013 2 &lt;tibble [60 × 8]&gt; &lt;tibble [0 × 3]&gt; ## 2 &lt;split [300/110]&gt; Bootstrap057 3 &lt;tibble [60 × 8]&gt; &lt;tibble [0 × 3]&gt; ## 3 &lt;split [300/114]&gt; Bootstrap066 1 &lt;tibble [60 × 8]&gt; &lt;tibble [0 × 3]&gt; ## 4 &lt;split [300/116]&gt; Bootstrap042 4 &lt;tibble [4 × 8]&gt; &lt;tibble [0 × 3]&gt; ## 5 &lt;split [300/300]&gt; Apparent 79 &lt;tibble [2 × 8]&gt; &lt;tibble [0 × 3]&gt; ## 6 &lt;split [300/115]&gt; Bootstrap001 100 &lt;tibble [2 × 8]&gt; &lt;tibble [0 × 3]&gt; ## 7 &lt;split [300/107]&gt; Bootstrap002 41 &lt;tibble [2 × 8]&gt; &lt;tibble [0 × 3]&gt; ## 8 &lt;split [300/116]&gt; Bootstrap003 30 &lt;tibble [2 × 8]&gt; &lt;tibble [0 × 3]&gt; ## 9 &lt;split [300/109]&gt; Bootstrap004 66 &lt;tibble [2 × 8]&gt; &lt;tibble [0 × 3]&gt; ## 10 &lt;split [300/106]&gt; Bootstrap005 72 &lt;tibble [2 × 8]&gt; &lt;tibble [0 × 3]&gt; ## # … with 91 more rows "],["evaluate-the-model-7.html", "8.37 Evaluate the model", " 8.37 Evaluate the model autoplot(xgb_rs) Show best models show_best(xgb_rs) ## Warning: No value of `metric` was given; metric &#39;rmse&#39; will be used. ## # A tibble: 1 × 10 ## mtry min_n tree_depth learn_rate .metric .estimator mean n std_err ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 4 5 2 0.0394 rmse standard 1.35 101 0.0122 ## # … with 1 more variable: .config &lt;chr&gt; Select best model select_best(xgb_rs, &quot;rmse&quot;) ## # A tibble: 1 × 5 ## mtry min_n tree_depth learn_rate .config ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 4 5 2 0.0394 Preprocessor1_Model28 "],["final-evauation.html", "8.38 Final evauation", " 8.38 Final evauation Last fit xgb_last_fit &lt;- xgb_wf %&gt;% finalize_workflow(select_best(xgb_rs, &quot;rmse&quot;)) %&gt;% last_fit(carseats_split) xgb_last_fit ## # Resampling results ## # Manual resampling ## # A tibble: 1 × 6 ## splits id .metrics .notes .predictions .workflow ## &lt;list&gt; &lt;chr&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; ## 1 &lt;split [300/100]&gt; train/test split &lt;tibble&gt; &lt;tibble&gt; &lt;tibble&gt; &lt;workflow&gt; Collect metrics xgb_last_fit %&gt;% collect_metrics() ## # A tibble: 2 × 4 ## .metric .estimator .estimate .config ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 rmse standard 1.41 Preprocessor1_Model1 ## 2 rsq standard 0.712 Preprocessor1_Model1 RMSE: 1.42 R-squared: 0.706 "],["feature-importance.html", "8.39 Feature importance", " 8.39 Feature importance xgb_fit &lt;- extract_fit_parsnip(xgb_last_fit) vip(xgb_fit, geom = &quot;point&quot;, num_features = 12) Stop parallel cluster "],["meeting-videos-7.html", "8.40 Meeting Videos", " 8.40 Meeting Videos 8.40.1 Cohort 1 Meeting chat log 00:12:39 Ryan Metcalf: (I’m very self-conscious) 00:21:31 Mei Ling Soh: I didn’t have time to read this chapter, so sorry to ask, how did you decide on the internal nodes? 00:22:57 Jon Harmon (jonthegeek): I think we&#39;re about to go into that :) 00:23:08 shamsuddeen: I guess it is calculated based on the purity Meeting chat log 00:43:03 jonathan.bratt: Maybe “inversely correlated” is more accurate? Meeting chat log 00:09:47 Jon Harmon (jonthegeek): Yay for using anything other than Boston 😁 00:14:51 jonathan.bratt: BTW: 00:15:07 jonathan.bratt: node), split, n, loss, yval, (yprob) * denotes terminal node 00:15:40 jonathan.bratt: From `rpart` output 00:21:47 Federica Gazzelloni: I need to jump off in 15 minutes..apologies.. 00:34:36 Federica Gazzelloni: See you on Thursday @laura - thanks @jon 8.40.2 Cohort 2 Meeting chat log 01:08:47 Jim Gruman: thank you. see you all next week 01:11:56 Ricardo: https://youtu.be/_L39rN6gz7Y Meeting chat log 00:27:35 Jim Gruman: Jared Lander gives a talk on Finding the Tallest Tree and references ISLR as a great resource to learn more. He discusses the various types of hyperparameters of rpart and others. His deck: https://jaredlander.com/content/2020/08/TallestTree.html#1 8.40.3 Cohort 3 Meeting chat log ADD LOG HERE 8.40.4 Cohort 4 Meeting chat log ADD LOG HERE "],["support-vector-machines.html", "Chapter 9 Support Vector Machines", " Chapter 9 Support Vector Machines Learning objectives: Implement a binary classification model using a maximal margin classifier. Implement a binary classification model using a support vector classifier. Implement a binary classification model using a support vector machine (SVM). Generalize SVM models to multi-class cases. Support vector machine (SVM), an approach for classification developed in 1990. SVM is a generalizaion of classifiers methods, in particular: maximal margin classifier (it requires that the classes be separable by a linear boundary). support vector classifier support vector machine: binary classification setting with two classes "],["maximal-margin-classifier-hyperplanes.html", "9.1 Maximal Margin Classifier &amp; Hyperplanes", " 9.1 Maximal Margin Classifier &amp; Hyperplanes A hyperplane is a \\(p-1\\)-dimensional flat subspace of a \\(p\\)-dimensional space. For example, in a 2-dimensional space, a hyperplane is a flat one-dimensional space: a line. Mathematical definition of hyperplane (2D space): \\[\\beta_{0} + \\beta_{1}X_{1} + \\beta_{2}X_{2} = 0\\] Any \\(X\\) s.t. \\(X = (X_{1}, X_{2})^T\\) for which the equation above is satisfied is a point on the hyperplane. "],["using-a-separating-hyperplane-to-classify.html", "9.2 Using a Separating Hyperplane to Classify", " 9.2 Using a Separating Hyperplane to Classify Consider a matrix X of dimensions \\(n*p\\), and a \\(y_{i} \\in \\{-1, 1\\}\\). We have a new observation, \\(x^*\\), which is a vector \\(x^* = (x^*_{1}...x^*_{p})^T\\) which we wish to classify to one of two groups. We will use a separating hyperplane to classify the observation. "],["visual-example-of-using-a-hyperplane-to-classify.html", "9.3 Visual Example of Using a Hyperplane to Classify", " 9.3 Visual Example of Using a Hyperplane to Classify We can label the blue observations as \\(y_{i} = 1\\) and the pink observations as \\(y_{i} = -1\\). Thus, a separating hyperplane has the property s.t. \\(\\beta_{0} + \\beta_{1}X_{i1} + \\beta_{2}X_{i2} ... + \\beta_{p}X_{ip} &gt; 0\\) if \\(y_{i} =1\\) and \\(\\beta_{0} + \\beta_{1}X_{i1} + \\beta_{2}X_{i2} ... + \\beta_{p}X_{ip} &lt; 0\\) if \\(y_{i} = -1\\). In other words, a separating hyperplane has the property s.t. \\(y_{i}(\\beta_{0} + \\beta_{1}X_{i1} + \\beta_{2}X_{i2} ... + \\beta_{p}X_{ip}) &gt; 0\\) for all \\(i = 1...n\\). Consider also the magnitude of \\(f(x^*)\\). If it is far from zero, we are confident in its classification, whereas if it is close to 0, then \\(x^*\\) is located near the hyperplane, and we are less confident about its classification. "],["maximal-margin-classifier.html", "9.4 Maximal Margin Classifier", " 9.4 Maximal Margin Classifier Generally, if data can be perfectly separated using a hyperplane, an infinite amount of such hyperplanes exist. An intuitive choice is the maximal margin hyperplane, which is the hyperplane that is farthest from the training data. We compute the perpendicular distance from each training observation to the hyperplane. The smallest of these distances is known as the margin. The maximal margin hyperplane is the hyperplane for which the margin is maximized. We can classify a test observation based on which side of the maximal margin hyperplane it lies on, and this is known as the maximal margin classifier. The maximal margin classifier classifies \\(x^*\\) based on the sign of \\(f(x^*) = \\beta_{0} + \\beta_{1}x^*_{1} + ... + \\beta_{p}x^*_{p}\\). "],["visual-representation-of-maximal-margin-classifier.html", "9.5 Visual Representation of Maximal Margin Classifier", " 9.5 Visual Representation of Maximal Margin Classifier Note the 3 training observations that lie on the margin and are equidistant from the hyperplane. These are the support vectors (vectors in p-dimensional space; in this case \\(p=2\\)). They support the hyperplane because if their location was changed, the hyperplane would change. The maximal margin hyperplane depends on these observations, but not the others (unless the other observations were moved at or within the margin). "],["mathematics-of-the-mmc.html", "9.6 Mathematics of the MMC", " 9.6 Mathematics of the MMC Consider constructing an MMC based on the training observations \\(x_{1}...x_{n} \\in \\mathbb{R}^p\\). This is the solution to the optimization problem: \\[maximize_{\\beta_{0}...\\beta_{p}, M} \\space M\\] \\[subject \\space to \\space\\sum_{j=1}^{p}\\beta_{j}^2 = 1\\] \\[y_{i}(\\beta_{0} + \\beta_{1}X_{i1} + \\beta_{2}X_{i2} ... + \\beta_{p}X_{ip}) \\geq M \\space\\space \\forall \\space\\space i = 1...n\\] M is the margin, and the \\(\\beta s\\) are chosen to maximize M. The constraint (3rd equation) ensures that each observation will be correctly classified, as long as M is positive. The 2nd and 3rd equations ensure that each data point is on the correct side of the hyperplane and at least M-distance away from the hyperplane. The perpendicular distance to the hyperplane is given by \\(y_{i}(\\beta_{0} + \\beta_{1}x_{i1} + \\beta_{2}x_{i2} ... + \\beta_{p}x_{ip})\\). "],["support-vector-classifiers.html", "9.7 Support Vector Classifiers", " 9.7 Support Vector Classifiers We can’t always use a hyperplane to separate two classes. Even if such a classifier does exist, it’s not always desirable, due to overfitting or too much sensitivity to individual observations. Thus, it might be worthwhile to consider a classifier/hyperplane that misclassifies a few observations in order to improve classification of the remaining data points. The support vector classifier, a.k.a the soft margin classifier, allows some training data to be on the wrong side of the margin or even the hyperplane. "],["mathematics-of-the-svc.html", "9.8 Mathematics of the SVC", " 9.8 Mathematics of the SVC The SVC classifies a test observation based on which side of the hyperplane it lies. \\[maximize_{\\beta_{0}...\\beta_{p}, \\epsilon_{1}...\\epsilon_{n}, M} \\space M\\] \\[subject \\space to \\space\\sum_{j=1}^{p}\\beta_{j}^2 = 1\\] \\[y_{i}(\\beta_{0} + \\beta_{1}X_{i1} + \\beta_{2}X_{i2} ... + \\beta_{p}X_{ip}) \\geq M(1 - \\epsilon_{i})\\] \\[\\epsilon_{i} \\geq 0, \\sum_{i=1}^{n}\\epsilon_{i} \\leq C\\] C is a nonnegative tuning parameter, typically chosen through cross-validation, and can be thought of as the budget for margin violation by the observations. The \\(\\epsilon\\)s are slack variables that allow individual observations to be on the wrong side of the margin or hyperplane. The \\(\\epsilon_{i}\\) indicates where the ith observation is located with regards to the margin and hyperplane. If \\(\\epsilon_{i}\\) = 0, the observation is on the correct side of the margin. If greater than 0, it is on the wrong side of margin. If greater than 1, it is on the wrong side of the hyperplane. Since C constrains the sum of the \\(\\epsilon\\)s, it determines the number and magnitude of violations to the margin. If \\(C=0\\), there is no margin for violation, thus all the \\(\\epsilon_{1}...\\epsilon_{n} = 0\\). Note that if \\(C&gt;0\\), no more than C observations can be on wrong side of hyperplane, since in these cases \\(\\epsilon_{i} &gt; 1\\). "],["visual-illustration-of-svc.html", "9.9 Visual Illustration of SVC", " 9.9 Visual Illustration of SVC A property of the classifier is that only data points which lie on or violate the margin will affect the hyperplane. These data points are known as support vectors. C controls the bias-variance tradeoff of the classifier. When C is large, the budget for margin violation is large, and there are many observations involved in determining the hyperplane. See upper left pane of Figure 9.7 for an illustration of high bias-low variance. When C is small, we have a low bias-high variance classifier related to the low number of support vectors involved in determination of the SVC. See bottom right panel above. The property of the SVC solely being dependent on certain observations in classification differs from other classification methods such as LDA (depends on mean of all observations in each class, as well as each class’s covariance matrix using all observations). However, logistic regression is more similar to SVC in that it has low sensitivity to observations far from the decision boundary. "],["support-vector-machines-1.html", "9.10 Support Vector Machines", " 9.10 Support Vector Machines Many decision boundaries are not linear. We could fit an SVC to the data using \\(2p\\) features (in the case of p features and using a quadratic form). See page 380 for equations. Note that in the enlarged feature space (i.e., with the quadratic terms), the decision boundary is linear. But in the original feature space, it is quadratic \\(q(x) = 0\\) (in this example), and generally the solutions are not linear. One could also include interaction terms, higher degree polynomials, etc., and thus the feature space could enlarge quickly and entail unmanageable computations. "],["support-vector-machines-cont..html", "9.11 Support Vector Machines, cont.", " 9.11 Support Vector Machines, cont. The SVM is an extension of the SVC which results from using kernels to enlarge the feature space. A kernel is a function that quantifies the similarity of two data points. Essentially, we want to enlarge the feature space to make use of a nonlinear decision boundary, while avoiding getting bogged down in unmanageable calculations. The solution to the SVC problem in the SVM context involves only the inner products (a.k.a dot products) of the observations. \\[\\langle x_{i} \\; , x_{i&#39;} \\; \\rangle = \\sum_{j=1}^{p}x_{ij}x_{i&#39;j}\\] In the context of SVM, the linear support vector classifier is as follows: \\[f(x) = \\beta_{0} + \\sum_{i=1}^{n}\\alpha_{i}\\langle \\; x, x_i\\; \\rangle\\] To estimate the n \\(\\alpha\\)s, and \\(\\beta_{0}\\), we only need the \\({n \\choose 2}\\) (n choose 2) inner products between all pairs of training observations. Note that in the equation above, in order to compute \\(f(x)\\) for the new point \\(x\\), we need the inner product between the new point and all the training observations. However, \\(\\alpha_{i} = 0\\) for all points that are not on or within the margin (i.e., points that are not support vectors). So we can rewrite the equation as follows, where \\(S\\) is the set of support point indices: \\[f(x) = \\beta_{0} + \\sum_{i \\in S}\\alpha_{i}\\langle \\; x, x_{i} \\; \\rangle\\] Replace every inner product with \\(K(x_{i}, x_{i&#39;})\\), where \\(K\\) is a kernel function. \\(K(x_{i}, x_{i&#39;}) = \\sum_{j=1}^{p}x_{ij}x_{i&#39;j}\\) is the SVC and is known as a linear kernel since it is linear in the features. One could also have kernel functions of the following form, where \\(d\\) is a positive integer: \\[K(x_{i}, x_{i&#39;}) = (1 + \\sum_{j=1}^{p}x_{ij}x_{i&#39;j})^d\\] This will lead to a much more flexible decision boundary, and is basically fitting an SVC in a higher-dimensional space involving polynomials of degree \\(d\\), instead of the original feature space. When an SVC is combined with a nonlinear kernel as above, the result is a support vector machine. \\[f(x) = \\beta_{0} + \\sum_{i \\in S}\\alpha_{i}K(x, x_{i})\\] "],["radial-kernels.html", "9.12 Radial Kernels", " 9.12 Radial Kernels There are other options besides polynomial kernel functions, and a popular one is a radial kernel. \\(\\gamma\\) is a positive constant. \\[K(x, x_{i}) = exp(-\\gamma\\sum_{j=1}^p(x_{ij} - x_{i&#39;j})^2)\\] For a given test observations \\(x^*\\), if it is far from \\(x_{i}\\), then \\(K(x^*, x_{i})\\) will be small given the negative \\(\\gamma\\) and large \\(\\sum_{j=1}^p(x^*_{j} - x_{ij})^2)\\). Thus, \\(x_{i}\\) will play little role in \\(f(x^*)\\). The predicted class for \\(x^*\\) is based on the sign of \\(f(x^*)\\), so training observations far from a given test point play little part in determining the label for a test observation. The radial kernel therefore exhibits local behavior with respect to other observations. "],["radial-kernels-cont..html", "9.13 Radial Kernels, cont.", " 9.13 Radial Kernels, cont. The advantage of using a kernel rather than simply enlarging feature space is computational, since it is only necessary to compute \\(n \\choose 2\\) kernel functions. For radial kernels, the feature space is implicit and infinite dimensional, so we could not do the computations in such a space anyways. "],["svms-with-more-than-two-classes.html", "9.14 SVMs with More than Two Classes", " 9.14 SVMs with More than Two Classes The concept of separating hyperplanes does not extend naturally to more than two classes, but there are some ways around this. A one-versus-one approach constructs \\(K \\choose 2\\) SVMs, where \\(K\\) is the number of classes. An observation is classified to each of the \\(K \\choose 2\\) classes, and the number of times it appears in each class is counted. The \\(k\\)th class might be coded as +1 versus the \\(k\\)’th class is coded as -1. The data point is classified to the class for which it was most often assigned in the pairwise classifications. Another option is one-versus-all classification. This can be useful when there are a lot of classes. \\(K\\) SVMs are fitted, and one of the K classes to the remaining \\(K-1\\) classes. \\(\\beta_{0k}...\\beta_{pk}\\) denotes the parameters that results from constructing an SVM comparing the \\(k\\)th class (coded as +1) to the other classes (-1). Assign test observation \\(x^*\\) to the class \\(k\\) for which \\(\\beta_{0k} + ... + \\beta_{pk}x^*_{p}\\) is largest. "],["lab-support-vector-classifier.html", "9.15 Lab: Support Vector Classifier", " 9.15 Lab: Support Vector Classifier This is a hybrid of Emil’s tidymodels labs and the original lab. I make sure our data matches theirs so we can compare more directly. library(tidymodels) library(kernlab) # We&#39;ll use the plot method from this. set.seed(1) sim_data &lt;- matrix( rnorm (20 * 2), ncol = 2, dimnames = list(NULL, c(&quot;x1&quot;, &quot;x2&quot;)) ) %&gt;% as_tibble() %&gt;% mutate( y = factor(c(rep(-1, 10), rep(1, 10))) ) %&gt;% mutate( x1 = ifelse(y == 1, x1 + 1, x1), x2 = ifelse(y == 1, x2 + 1, x2) ) sim_data %&gt;% ggplot() + aes(x1, x2, color = y) + geom_point() # I generated this using their process then saved it to use here. test_data &lt;- readRDS(&quot;data/09-testdat.rds&quot;) %&gt;% rename(x1 = x.1, x2 = x.2) test_data %&gt;% ggplot() + aes(x1, x2, color = y) + geom_point() We create a spec for a model, which we’ll update throughout this lab with different costs. svm_linear_spec &lt;- svm_poly(degree = 1) %&gt;% set_mode(&quot;classification&quot;) %&gt;% set_engine(&quot;kernlab&quot;, scaled = FALSE) Then we do a couple fits with manual cost. svm_linear_fit_10 &lt;- svm_linear_spec %&gt;% set_args(cost = 10) %&gt;% fit(y ~ ., data = sim_data) svm_linear_fit_10 ## parsnip model object ## ## Support Vector Machine object of class &quot;ksvm&quot; ## ## SV type: C-svc (classification) ## parameter : cost C = 10 ## ## Polynomial kernel function. ## Hyperparameters : degree = 1 scale = 1 offset = 1 ## ## Number of Support Vectors : 7 ## ## Objective Function Value : -52.4483 ## Training error : 0.15 ## Probability model included. svm_linear_fit_10 %&gt;% extract_fit_engine() %&gt;% plot() svm_linear_fit_01 &lt;- svm_linear_spec %&gt;% set_args(cost = 0.1) %&gt;% fit(y ~ ., data = sim_data) svm_linear_fit_01 ## parsnip model object ## ## Support Vector Machine object of class &quot;ksvm&quot; ## ## SV type: C-svc (classification) ## parameter : cost C = 0.1 ## ## Polynomial kernel function. ## Hyperparameters : degree = 1 scale = 1 offset = 1 ## ## Number of Support Vectors : 16 ## ## Objective Function Value : -1.189 ## Training error : 0.05 ## Probability model included. svm_linear_fit_01 %&gt;% extract_fit_engine() %&gt;% plot() svm_linear_fit_001 &lt;- svm_linear_spec %&gt;% set_args(cost = 0.01) %&gt;% fit(y ~ ., data = sim_data) svm_linear_fit_001 ## parsnip model object ## ## Support Vector Machine object of class &quot;ksvm&quot; ## ## SV type: C-svc (classification) ## parameter : cost C = 0.01 ## ## Polynomial kernel function. ## Hyperparameters : degree = 1 scale = 1 offset = 1 ## ## Number of Support Vectors : 20 ## ## Objective Function Value : -0.1859 ## Training error : 0.25 ## Probability model included. svm_linear_fit_001 %&gt;% extract_fit_engine() %&gt;% plot() 9.15.1 Tuning Let’s find the best cost. svm_linear_wf &lt;- workflow() %&gt;% add_model( svm_linear_spec %&gt;% set_args(cost = tune()) ) %&gt;% add_formula(y ~ .) set.seed(1234) sim_data_fold &lt;- vfold_cv(sim_data, strata = y) param_grid &lt;- grid_regular(cost(), levels = 10) # Our grid isn&#39;t identical to the book, but it&#39;s close enough. param_grid ## # A tibble: 10 × 1 ## cost ## &lt;dbl&gt; ## 1 0.000977 ## 2 0.00310 ## 3 0.00984 ## 4 0.0312 ## 5 0.0992 ## 6 0.315 ## 7 1 ## 8 3.17 ## 9 10.1 ## 10 32 tune_res &lt;- tune_grid( svm_linear_wf, resamples = sim_data_fold, grid = param_grid ) # We ran this locally and then saved it so everyone doesn&#39;t need to wait for # this to process each time they build the book. # saveRDS(tune_res, &quot;data/09-tune_res.rds&quot;) autoplot(tune_res) Tune can pull out the best result for us. best_cost &lt;- select_best(tune_res, metric = &quot;accuracy&quot;) svm_linear_final &lt;- finalize_workflow(svm_linear_wf, best_cost) svm_linear_fit &lt;- svm_linear_final %&gt;% fit(sim_data) svm_linear_fit %&gt;% augment(new_data = test_data) %&gt;% conf_mat(truth = y, estimate = .pred_class) ## Truth ## Prediction -1 1 ## -1 9 1 ## 1 2 8 svm_linear_fit_001 %&gt;% augment(new_data = test_data) %&gt;% conf_mat(truth = y, estimate = .pred_class) ## Truth ## Prediction -1 1 ## -1 11 6 ## 1 0 3 9.15.2 Linearly separable data sim_data_sep &lt;- sim_data %&gt;% mutate( x1 = ifelse(y == 1, x1 + 0.5, x1), x2 = ifelse(y == 1, x2 + 0.5, x2) ) sim_data_sep %&gt;% ggplot() + aes(x1, x2, color = y) + geom_point() svm_fit_sep_1e5 &lt;- svm_linear_spec %&gt;% set_args(cost = 1e5) %&gt;% fit(y ~ ., data = sim_data_sep) svm_fit_sep_1e5 ## parsnip model object ## ## Support Vector Machine object of class &quot;ksvm&quot; ## ## SV type: C-svc (classification) ## parameter : cost C = 1e+05 ## ## Polynomial kernel function. ## Hyperparameters : degree = 1 scale = 1 offset = 1 ## ## Number of Support Vectors : 3 ## ## Objective Function Value : -24.3753 ## Training error : 0 ## Probability model included. svm_fit_sep_1e5 %&gt;% extract_fit_engine() %&gt;% plot() svm_fit_sep_1 &lt;- svm_linear_spec %&gt;% set_args(cost = 1) %&gt;% fit(y ~ ., data = sim_data_sep) svm_fit_sep_1 ## parsnip model object ## ## Support Vector Machine object of class &quot;ksvm&quot; ## ## SV type: C-svc (classification) ## parameter : cost C = 1 ## ## Polynomial kernel function. ## Hyperparameters : degree = 1 scale = 1 offset = 1 ## ## Number of Support Vectors : 7 ## ## Objective Function Value : -3.5451 ## Training error : 0.05 ## Probability model included. svm_fit_sep_1 %&gt;% extract_fit_engine() %&gt;% plot() test_data_sep &lt;- test_data %&gt;% mutate( x1 = ifelse(y == 1, x1 + 0.5, x1), x2 = ifelse(y == 1, x2 + 0.5, x2) ) svm_fit_sep_1e5 %&gt;% augment(new_data = test_data_sep) %&gt;% conf_mat(truth = y, estimate = .pred_class) ## Truth ## Prediction -1 1 ## -1 9 1 ## 1 2 8 svm_fit_sep_1 %&gt;% augment(new_data = test_data_sep) %&gt;% conf_mat(truth = y, estimate = .pred_class) ## Truth ## Prediction -1 1 ## -1 9 0 ## 1 2 9 "],["lab-support-vector-machine-non-linear-kernel.html", "9.16 Lab: Support Vector Machine (non-linear kernel)", " 9.16 Lab: Support Vector Machine (non-linear kernel) Now that we’ve seen one that’s comparable, I want to focus on the tidymodels version. For the rest of the meeting, see Emil’s version "],["meeting-videos-8.html", "9.17 Meeting Videos", " 9.17 Meeting Videos 9.17.1 Cohort 1 Meeting chat log 00:41:54 Jon Harmon (jonthegeek): 9.23 in the book *does* still have the alpha. 00:52:14 Federica Gazzelloni: dealing with grouping based on some indications 00:54:37 Ryan Metcalf: Laura, not sure if this is the same video series. https://www.youtube.com/channel/UCB2p-jaoolkv0h22m4I9l9Q/videos 00:55:02 Jon Harmon (jonthegeek): Here specifically: https://www.youtube.com/watch?v=ooYwHNvH-YU&amp;list=PLAOUn-KLSAVOf4Uk-WbLGPUDFjMSyytkw&amp;index=3 00:59:15 Federica Gazzelloni: let’s share the lab?? 9.17.2 Cohort 2 Meeting chat log 00:10:29 Federica Gazzelloni: https://emilhvitfeldt.github.io/ISLR-tidymodels-labs/support-vector-machines.html 01:04:02 Jenny Smith: https://www.youtube.com/c/TidyX_screencast 9.17.3 Cohort 3 Meeting chat log ADD LOG HERE 9.17.4 Cohort 4 Meeting chat log ADD LOG HERE "],["deep-learning.html", "Chapter 10 Deep Learning", " Chapter 10 Deep Learning Learning objectives: Describe the structure of a single-layer neural network. Describe the structure of a multilayer neural network. Describe the structure of a convolutional neural network. Describe the structure of a recurrent neural network. Compare deep learning to simpler models. Recognize the process by which neutral networks are fit. Explain the double descent phenomenon. "],["slide-1.html", "10.1 Slide 1", " 10.1 Slide 1 Try to follow a slide-like format. "],["slide-2.html", "10.2 Slide 2", " 10.2 Slide 2 Use ## to create new slides (sections). "],["meeting-videos-9.html", "10.3 Meeting Videos", " 10.3 Meeting Videos 10.3.1 Cohort 1 Meeting chat log 00:22:21 Mei Ling Soh: It&#39;s z &lt;0 in the book, pg 405 00:26:39 Jon Harmon (jonthegeek): For anyone who wants to watch that video after this: https://www.youtube.com/watch?v=CqOfi41LfDw 00:53:53 Federica Gazzelloni: part2 of the video: https://www.youtube.com/watch?v=IN2XmBhILt4&amp;list=PLblh5JKOoLUIxGDQs4LFFD--41Vzf-ME1&amp;index=4 00:57:40 Mei Ling Soh: Thanks! Meeting chat log 00:09:09 Jon Harmon (jonthegeek): https://www.statlearning.com/resources-second-edition 00:41:00 Jon Harmon (jonthegeek): (dataloader, dataset or list) A dataloader created with torch::dataloader() used for training the model, or a dataset created with torch::dataset() or a list. Dataloaders and datasets must return list with at most 2 items. The first item will be used as input for the module and the second will be used as target for the loss function. 10.3.2 Cohort 2 Meeting chat log 00:28:37 Jim Gruman: https://playground.tensorflow.org/ 00:29:06 Federica Gazzelloni: Thanks Jim Meeting chat log 00:14:41 Ricardo Serrano: Neural network course https://youtu.be/ob1yS9g-Zcs 10.3.3 Cohort 3 Meeting chat log ADD LOG HERE 10.3.4 Cohort 4 Meeting chat log ADD LOG HERE "],["survival-analysis-and-censored-data.html", "Chapter 11 Survival Analysis and Censored Data", " Chapter 11 Survival Analysis and Censored Data Learning objectives: Describe how censored data impacts survival analysis. Calculate a Kaplan-Meier survival curve. Compare the survival rates of two groups with the log-rank test. Model survival data using Cox’s proportional hazards Apply shrinkage methods to regularize the Cox model. Calculate the generalized AUC for a survival model. "],["slide-1-1.html", "11.1 Slide 1", " 11.1 Slide 1 Try to follow a slide-like format. "],["slide-2-1.html", "11.2 Slide 2", " 11.2 Slide 2 Use ## to create new slides (sections). "],["meeting-videos-10.html", "11.3 Meeting Videos", " 11.3 Meeting Videos 11.3.1 Cohort 1 Meeting chat log 00:30:47 Jon Harmon (jonthegeek): ggplot2 default red is &quot;salmon&quot; (according to this site: https://www.htmlcsscolor.com/hex/F8766D) the default blue is &quot;cornflower blue&quot; https://www.htmlcsscolor.com/hex/619CFF and while I&#39;m at it, the default green is &quot;dark pastel green&quot; https://www.htmlcsscolor.com/hex/00BA38 00:55:24 Jon Harmon (jonthegeek): summand == addend https://www.merriam-webster.com/dictionary/summand 00:55:54 Jon Harmon (jonthegeek): subtrahend 00:56:31 Jon Harmon (jonthegeek): https://www.merriam-webster.com/dictionary/subtrahend A subtrahend is subtracted from a minuend. https://www.merriam-webster.com/dictionary/minuend 00:57:29 Jonathan.Bratt: 😄 01:05:55 Federica Gazzelloni: workshop: https://bioconnector.github.io/workshops/r-survival.html 01:06:29 Jon Harmon (jonthegeek): There&#39;s an in-progress survival analysis package in tidymodels, btw: https://github.com/tidymodels/censored/ Meeting chat log 00:27:56 Jon Harmon (jonthegeek): https://github.com/tidymodels/censored/ not yet on CRAN 00:50:52 Jon Harmon (jonthegeek): https://twitter.com/justsaysrisks 11.3.2 Cohort 2 Meeting chat log ADD LOG HERE 11.3.3 Cohort 3 Meeting chat log ADD LOG HERE 11.3.4 Cohort 4 Meeting chat log ADD LOG HERE "],["unsupervised-learning.html", "Chapter 12 Unsupervised Learning", " Chapter 12 Unsupervised Learning Learning objectives: Compare and contrast supervised learning and unsupervised learning. Perform principal component analysis to analyze the sources of variance in a dataset. Impute missing values in a dataset via matrix completion. Perform K-means clustering to partition observations into a pre-specified number of clusters. Perform hierarchical clustering to partition observations into a tree-like structure. "],["unsupervised-learning-vs-supervised-learning.html", "Unsupervised Learning vs Supervised Learning", " Unsupervised Learning vs Supervised Learning Supervised learning has predictors (features) + outcome(s). Unsupervised learning only has features. Goal: Discover interesting things Visualization Subgroups "],["principal-component-analysis.html", "Principal Component Analysis", " Principal Component Analysis Also discussed in Chapter 6. Find axes with most variance. We’ll demonstrate with this 2-feature dataset. "],["principal-component-analysis-1.html", "Principal Component Analysis", " Principal Component Analysis Green line = line that spreads out the data the most. Map each point onto that line. "],["principal-component-analysis-2.html", "Principal Component Analysis", " Principal Component Analysis That line is the first principal component. Distance from that line is the 2nd (orthogonal to 1st). "],["pca-lab-recipe.html", "PCA Lab: Recipe", " PCA Lab: Recipe Based on Emil Hvitfeldt’s tidymodels implementation library(tidymodels) library(tidyverse) pca_rec &lt;- recipes::recipe(~., data = USArrests) %&gt;% recipes::step_normalize(recipes::all_numeric_predictors()) %&gt;% recipes::step_pca(recipes::all_numeric_predictors(), id = &quot;pca&quot;) %&gt;% recipes::prep() "],["pca-lab-loadings.html", "PCA Lab: Loadings", " PCA Lab: Loadings pca_rec %&gt;% broom::tidy(id = &quot;pca&quot;, type = &quot;coef&quot;) %&gt;% ggplot() + aes(x = value, y = terms) + facet_wrap(~ component) + geom_col() + theme_minimal() "],["pca-lab-variance.html", "PCA Lab: Variance", " PCA Lab: Variance pca_rec %&gt;% broom::tidy(id = &quot;pca&quot;, type = &quot;variance&quot;) %&gt;% dplyr::filter(terms == &quot;cumulative percent variance&quot;) %&gt;% ggplot() + aes(x = component, y = value/100) + geom_col() + scale_y_continuous(labels = scales::percent) + geom_hline(yintercept = 0.9) + ylab(&quot;cumulative percent variance&quot;) + theme_minimal() step_pca also has num_comp and threshold arguments. "],["matrix-completion.html", "Matrix Completion", " Matrix Completion They haven’t added this to their slides yet. Sometimes you want to fill in NAs intelligently. This can be the whole task, not just before modeling (Netflix, etc) Techniques based on PCA are sometimes good at this. "],["matrix-completion-their-technique.html", "Matrix Completion: Their Technique", " Matrix Completion: Their Technique Start with mean imputation per column. Use the computed PCA data to impute values. Recompute PCA and repeat. Technically they use svd() (singular-value decomposition) in the lab, which is called inside the prcomp() function, to more directly demonstrate what’s happening. "],["matrix-completion-lab-setup.html", "Matrix Completion Lab: Setup", " Matrix Completion Lab: Setup First we set up a matrix with missing values. The code for this is in the book and not particularly interesting, but I’ve made the names suck less. I also don’t scale, because their package does this internally. arrests &lt;- data.matrix(USArrests) n_omit &lt;- 20 set.seed(15) target_rows &lt;- sample(seq(50), n_omit) target_cols &lt;- sample(1:4, n_omit, replace = TRUE) targets &lt;- cbind(target_rows, target_cols) head(targets, 2) ## target_rows target_cols ## [1,] 37 3 ## [2,] 47 1 arrests_na &lt;- arrests arrests_na[targets] &lt;- NA head(arrests_na, 2) ## Murder Assault UrbanPop Rape ## Alabama 13.2 NA 58 21.2 ## Alaska 10.0 263 NA 44.5 is_missing &lt;- is.na(arrests_na) "],["matrix-completion-lab-softimpute.html", "Matrix Completion Lab: softImpute", " Matrix Completion Lab: softImpute They created the {softImpute} package to do this, let’s use it! fit_svd &lt;- softImpute::softImpute( arrests_na, type = &quot;svd&quot;, thresh = 1e-16, maxit = 3000 ) arrests_imputed &lt;- softImpute::complete(arrests_na, fit_svd, unscale = TRUE) cor(arrests_imputed[is_missing], arrests[is_missing]) ## [1] 0.8793766 "],["k-means-clustering.html", "K-Means Clustering", " K-Means Clustering Randomly assign each observation to a cluster. Compute each centroid. Assign each observation to the nearest centroid. Repeat until it stops changing. "],["k-means-clustering-data.html", "K-Means Clustering: Data", " K-Means Clustering: Data "],["k-means-clustering-randomly-assign.html", "K-Means Clustering: Randomly Assign", " K-Means Clustering: Randomly Assign "],["k-means-clustering-initial-centroids.html", "K-Means Clustering: Initial Centroids", " K-Means Clustering: Initial Centroids "],["k-means-clustering-reassign.html", "K-Means Clustering: Reassign", " K-Means Clustering: Reassign "],["k-means-clustering-new-centroids.html", "K-Means Clustering: New Centroids", " K-Means Clustering: New Centroids "],["k-means-clustering-final-result.html", "K-Means Clustering: Final Result", " K-Means Clustering: Final Result "],["k-means-clustering-warning.html", "K-Means Clustering: Warning", " K-Means Clustering: Warning "],["k-means-lab-setup.html", "12.1 K-Means Lab: Setup", " 12.1 K-Means Lab: Setup Based on Emil Hvitfeldt’s tidymodels implementation set.seed(2) x_df_labeled &lt;- tibble( x = rnorm(n = 75, mean = rep(c(0, 3, 5), each = 25)), y = rnorm(n = 75, mean = rep(c(0, -4, 2), each = 25)), true_cluster = rep(c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;), each = 25) ) x_df_labeled %&gt;% ggplot(aes(x, y, color = true_cluster)) + geom_point() + scale_color_viridis_d() + theme_minimal() "],["k-means-lab-3-clusters.html", "12.2 K-Means Lab: 3 Clusters", " 12.2 K-Means Lab: 3 Clusters x_df &lt;- select(x_df_labeled, -true_cluster) set.seed(1234) res_kmeans &lt;- kmeans( x_df, centers = 3, nstart = 20 ) augment(res_kmeans, data = x_df) %&gt;% rename(pred_cluster = &quot;.cluster&quot;) %&gt;% ggplot(aes(x, y, color = pred_cluster)) + geom_point() + scale_color_viridis_d() + theme_minimal() "],["k-means-lab-multi.html", "12.3 K-Means Lab: Multi", " 12.3 K-Means Lab: Multi set.seed(1234) multi_kmeans &lt;- tibble(k = 1:10) %&gt;% mutate( model = purrr::map(k, ~ kmeans(x_df, centers = .x, nstart = 20)), tot.withinss = purrr::map_dbl(model, ~ glance(.x)$tot.withinss) ) multi_kmeans %&gt;% ggplot(aes(k, tot.withinss)) + geom_point() + geom_line() + theme_minimal() "],["k-means-lab-finalize.html", "12.4 K-Means Lab: Finalize", " 12.4 K-Means Lab: Finalize multi_kmeans %&gt;% filter(k == 3) %&gt;% pull(model) %&gt;% pluck(1) %&gt;% augment(data = x_df) %&gt;% rename(pred_cluster = &quot;.cluster&quot;) %&gt;% ggplot(aes(x, y, color = pred_cluster)) + geom_point() + scale_color_viridis_d() + theme_minimal() "],["k-means-lab-bad-choices.html", "12.5 K-Means Lab: Bad Choices", " 12.5 K-Means Lab: Bad Choices multi_kmeans %&gt;% filter(k == 2) %&gt;% pull(model) %&gt;% pluck(1) %&gt;% augment(data = x_df) %&gt;% rename(pred_cluster = &quot;.cluster&quot;) %&gt;% ggplot(aes(x, y, color = pred_cluster)) + geom_point() + scale_color_viridis_d() + theme_minimal() multi_kmeans %&gt;% filter(k == 5) %&gt;% pull(model) %&gt;% pluck(1) %&gt;% augment(data = x_df) %&gt;% rename(pred_cluster = &quot;.cluster&quot;) %&gt;% ggplot(aes(x, y, color = pred_cluster)) + geom_point() + scale_color_viridis_d() + theme_minimal() "],["hierarchical-clustering.html", "Hierarchical Clustering", " Hierarchical Clustering Clusters of clusters of clusters. Start: Each point = cluster. At each step, assign the 2 closest clusters to a shared cluster. Repeat. Ends when all observations are in a single cluster. "],["hierarchical-clustering-types-of-linkage.html", "Hierarchical Clustering: Types of Linkage", " Hierarchical Clustering: Types of Linkage Linkage Description Complete Largest pairwise distances between A &amp; B. Single Smallest pairwise distances between A &amp; B. Average Average pairwise distances between A &amp; B. Centroid Distance between centroids of A &amp; B. “Distance” can be different measures: Euclidean distance. Correlation. Presumably other distance measures. "],["hierarchical-clustering-lab.html", "12.6 Hierarchical Clustering Lab", " 12.6 Hierarchical Clustering Lab I don’t have any changes, let’s go through Emil’s lab as-is "],["meeting-videos-11.html", "Meeting Videos", " Meeting Videos 12.6.1 Cohort 1 12.6.2 Cohort 2 Meeting chat log ADD LOG HERE 12.6.3 Cohort 3 Meeting chat log ADD LOG HERE 12.6.4 Cohort 4 Meeting chat log ADD LOG HERE "],["multiple-testing.html", "Chapter 13 Multiple Testing", " Chapter 13 Multiple Testing Learning objectives: Identify the challenges of performing multiple hypothesis tests. Reduce Type I errors by controlling the family-wise error rate (FWER). Balance Type I and Type II errors by controlling the false discovery rate (FDR). Calculate p-values using resampling. "],["slide-1-2.html", "13.1 Slide 1", " 13.1 Slide 1 Try to follow a slide-like format. "],["slide-2-2.html", "13.2 Slide 2", " 13.2 Slide 2 Use ## to create new slides (sections). "],["meeting-videos-12.html", "13.3 Meeting Videos", " 13.3 Meeting Videos 13.3.1 Cohort 1 Meeting chat log ADD LOG HERE 13.3.2 Cohort 2 Meeting chat log ADD LOG HERE 13.3.3 Cohort 3 Meeting chat log ADD LOG HERE 13.3.4 Cohort 4 Meeting chat log ADD LOG HERE "],["abbreviations.html", "Abbreviations", " Abbreviations Abbreviation Term RSE residual standard error RSS residual sum of squares TSS total sum of squares "],["latex.html", "Appendix: Bookdown and LaTeX Notes", " Appendix: Bookdown and LaTeX Notes Ray’s Formatting Notes: version 2021-09-22 https://gist.github.com/RaymondBalise/ee4b7da0a70087317dc52bf479a4e2b6 .col2 { columns: 2 150px; /* number of columns and width in pixels*/ -webkit-columns: 2 150px; /* chrome, safari */ -moz-columns: 2 150px; /* firefox */ } .col3 { columns: 3 100px; -webkit-columns: 3 100px; -moz-columns: 3 100px; } "],["markdown-highlighting.html", "Markdown highlighting", " Markdown highlighting Formatting Code bold **bold** bold __bold__ italic *italic* italic _italic_ "],["text-coloring.html", "Text coloring", " Text coloring To add color you can use CSS or R code like this: color_text &lt;- function(x, color){ if(knitr::is_latex_output()) paste(&quot;\\\\textcolor{&quot;,color,&quot;}{&quot;,x,&quot;}&quot;,sep=&quot;&quot;) else if(knitr::is_html_output()) paste(&quot;&lt;font color=&#39;&quot;,color,&quot;&#39;&gt;&quot;,x,&quot;&lt;/font&gt;&quot;,sep=&quot;&quot;) else x } red &lt;- function(x){ if(knitr::is_latex_output()) paste(&quot;\\\\textcolor{&quot;,&#39;red&#39;,&quot;}{&quot;,x,&quot;}&quot;,sep=&quot;&quot;) else if(knitr::is_html_output()) paste(&quot;&lt;font color=&#39;red&#39;&gt;&quot;,x,&quot;&lt;/font&gt;&quot;,sep=&quot;&quot;) else x } The strng This is colored with the red function comes from: `r red(&quot;This is colored with the red function&quot;)` The string This is colored with the color_text function set to mauve comes from: `r color_text(&quot;This is colored with the color_text function set to mauve&quot;, &quot;#E0B0FF&quot;)` "],["x99-4.html", "Section references", " Section references Section 99.4 comes from Section [99.4](#x99-4) "],["footnotes.html", "Footnotes", " Footnotes 1 comes from ^[A footnote] A footnote↩︎ "],["formatting-text.html", "Formatting Text", " Formatting Text Appearance Code Three xfun::n2w(3, cap = TRUE) 14.5% scales::percent(0.1447, accuracy= .1) p=0.145 scales::pvalue(0.1447, accuracy= .001, add_p =TRUE) "],["figures.html", "Figures", " Figures Name a figure chunk with a name like figure99-1 and include fig.cap=“something” then reference it like this: \\@ref(fig:figure99-1) "],["displaying-formula.html", "Displaying Formula", " Displaying Formula Formatting To tweak the appearance of words use these formats: Formatting Code Looks like plain text \\text{text Pr} \\(\\text{text Pr}\\) bold Greek symbol \\boldsymbol{\\epsilon} \\(\\boldsymbol{\\epsilon}\\) typewriter \\tt{blah} \\(\\tt{blah}\\) slide font \\sf{blah} \\(\\sf{blah}\\) bold \\mathbf{x} \\(\\mathbf{x}\\) plain \\mathrm{text Pr} \\(\\mathrm{text Pr}\\) cursive \\mathcal{S} \\(\\mathcal{S}\\) Blackboard bold \\mathbb{R} \\(\\mathbb{R}\\) Symbols Symbols Code \\(\\stackrel{\\text{def}}{=}\\) \\stackrel{\\text{def}}{=} Notation Based on: https://www.calvin.edu/~rpruim/courses/s341/S17/from-class/MathinRmd.html Math Code \\(x = y\\) $x = y$ \\(x \\approx y\\) $x \\approx y$ \\(x &lt; y\\) $x &lt; y$ \\(x &gt; y\\) $x &gt; y$ \\(x \\le y\\) $x \\le y$ \\(x \\ge y\\) $x \\ge y$ \\(x \\ge y\\) $x \\ge y$ \\(x \\times y\\) $x \\times y$ \\(x^{n}\\) $x^{n}$ \\(x_{n}\\) $x_{n}$ \\(\\overline{x}\\) $\\overline{x}$ \\(\\hat{x}\\) $\\hat{x}$ \\(\\widehat{SE}\\) $\\widehat{SE}$ \\(\\tilde{x}\\) $\\tilde{x}$ \\(\\frac{a}{b}\\) $\\frac{a}{b}$ \\(\\displaystyle \\frac{a}{b}\\) $\\displaystyle \\frac{a}{b}$ \\(\\binom{n}{k}\\) $\\binom{n}{k}$ \\(x_{1} + x_{2} + \\cdots + x_{n}\\) $x_{1} + x_{2} + \\cdots + x_{n}$ \\(x_{1}, x_{2}, \\dots, x_{n}\\) $x_{1}, x_{2}, \\dots, x_{n}$ \\(\\mathbf{x} = \\langle x_{1}, x_{2}, \\dots, x_{n}\\rangle\\) $\\mathbf{x} = \\langle x_{1}, x_{2}, \\dots, x_{n}\\rangle$ \\(x \\in A\\) $x \\in A$ \\(|A|\\) $|A|$ \\(x \\in A\\) $x \\in A$ \\(x \\subset B\\) $x \\subset B$ \\(x \\subseteq B\\) $x \\subseteq B$ \\(A \\cup B\\) $A \\cup B$ \\(A \\cap B\\) $A \\cap B$ \\(X \\sim {\\sf Binom}(n, \\pi)\\) X \\sim {\\sf Binom}(n, \\pi)$ \\(\\mathrm{P}(X \\le x) = {\\tt pbinom}(x, n, \\pi)\\) $\\mathrm{P}(X \\le x) = {\\tt pbinom}(x, n, \\pi)$ \\(P(A \\mid B)\\) $P(A \\mid B)$ \\(\\mathrm{P}(A \\mid B)\\) $\\mathrm{P}(A \\mid B)$ \\(\\{1, 2, 3\\}\\) $\\{1, 2, 3\\}$ \\(\\sin(x)\\) $\\sin(x)$ \\(\\log(x)\\) $\\log(x)$ \\(\\int_{a}^{b}\\) $\\int_{a}^{b}$ \\(\\left(\\int_{a}^{b} f(x) \\; dx\\right)\\) $\\left(\\int_{a}^{b} f(x) \\; dx\\right)$ \\(\\left[\\int_{-\\infty}^{\\infty} f(x) \\; dx\\right]\\) $\\left[\\int_{\\-infty}^{\\infty} f(x) \\; dx\\right]$ \\(\\left. F(x) \\right|_{a}^{b}\\) $\\left. F(x) \\right|_{a}^{b}$ \\(\\sum_{x = a}^{b} f(x)\\) $\\sum_{x = a}^{b} f(x)$ \\(\\prod_{x = a}^{b} f(x)\\) $\\prod_{x = a}^{b} f(x)$ \\(\\lim_{x \\to \\infty} f(x)\\) $\\lim_{x \\to \\infty} f(x)$ \\(\\displaystyle \\lim_{x \\to \\infty} f(x)\\) $\\displaystyle \\lim_{x \\to \\infty} f(x)$ ` \\(RMSE = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n} (Y_n - \\hat{Y}_i)^2}\\) $RMSE = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n} (Y_n - \\hat{Y}_i)^2}$ ` "],["equations.html", "Equations", " Equations These are formulas that appear with an equation number. Basic Equation The names of equations can not include . or _ but it can include - \\begin{equation} 1 + 1 = 2 (\\#eq:eq99-1) \\end{equation} Which appears as: \\[\\begin{equation} 1 + 1 = 2 \\tag{13.1} \\end{equation}\\] The reference to the equation is (13.1) which comes from this code \\@ref(eq:eq99-1) Case-When Equation (Large Curly Brace) Based on: https://tex.stackexchange.com/questions/9065/large-braces-for-specifying-values-of-variables-by-condition Case when formula: \\[\\begin{equation} y = \\begin{cases} 0, &amp; \\text{if}\\ a=1 \\\\ 1, &amp; \\text{otherwise} \\end{cases} \\tag{13.2} \\end{equation} \\] Which comes from this code: \\begin{equation} y = \\begin{cases} 0, &amp; \\text{if}\\ a=1 \\\\ 1, &amp; \\text{otherwise} \\end{cases} (\\#eq:eq99-2) \\begin{equation} The reference to equation is (13.2) which comes from this code \\@ref(eq:eq99-2) Alligned with Underbars \\[\\begin{equation} \\begin{aligned} \\mathrm{E}(Y-\\hat{Y})^2 &amp; = \\mathrm{E}[f(X) + \\epsilon -\\hat{f}(X)]^2 \\\\ &amp; = \\underbrace{[f(X) -\\hat{f}(X)]^2}_{\\mathrm{Reducible}} + \\underbrace{\\mathrm{var}(\\epsilon)}_{\\mathrm{Irreducible}} \\\\ \\end{aligned} \\tag{13.3} \\end{equation}\\] Comes from this code: \\begin{equation} \\begin{aligned} \\mathrm{E}(Y-\\hat{Y})^2 &amp; = \\mathrm{E}[f(X) + \\epsilon -\\hat{f}(X)]^2 \\\\ &amp; = \\underbrace{[f(X) -\\hat{f}(X)]^2}_{\\mathrm{Reducible}} + \\underbrace{\\mathrm{var}(\\epsilon)}_{\\mathrm{Irreducible}} \\\\ \\end{aligned} (\\#eq:eq99-3) \\end{equation} "],["greek-letters.html", "Greek letters", " Greek letters Based on: https://www.calvin.edu/~rpruim/courses/s341/S17/from-class/MathinRmd.html letters code \\(\\alpha A\\) $\\alpha A$ \\(\\beta B\\) $\\beta B$ \\(\\gamma \\Gamma\\) $\\gamma \\Gamma$ \\(\\delta \\Delta\\) $\\delta \\Delta$ \\(\\epsilon \\varepsilon E\\) $\\epsilon \\varepsilon E$ \\(\\zeta Z \\sigma\\) $\\zeta Z \\sigma \\(\\eta H\\) $\\eta H$ \\(\\theta \\vartheta \\Theta\\) $\\theta \\vartheta \\Theta$ \\(\\iota I\\) $\\iota I$ \\(\\kappa K\\) $\\kappa K$ \\(\\lambda \\Lambda\\) $\\lambda \\Lambda$ \\(\\mu M\\) $\\mu M$ \\(\\nu N\\) $\\nu N$ \\(\\xi\\Xi\\) $\\xi\\Xi$ \\(o O\\) $o O$ (omicron) \\(\\pi \\Pi\\) $\\pi \\Pi$ \\(\\rho\\varrho P\\) $\\rho\\varrho P$ \\(\\sigma \\Sigma\\) \\sigma \\Sigma$ \\(\\tau T\\) $\\tau T$ \\(\\upsilon \\Upsilon\\) $\\upsilon \\Upsilon$ \\(\\phi \\varphi \\Phi\\) $\\phi \\varphi \\Phi$ \\(\\chi X\\) $\\chi X$ \\(\\psi \\Psi\\) $\\psi \\Psi$ \\(\\omega \\Omega\\) $\\omega \\Omega$ "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
